[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical analysis and experimental design",
    "section": "",
    "text": "Overview\nThis week-long course is aimed at people with little or no experience using statistical analyses in research. It introduces participants to core concepts in statistics and experimental design, aimed at ensuring that the resulting data is able to address the research question using appropriate statistical methods.\nThe interactive course gives participants a hands-on, applied foundation in statistical data analysis and experimental design. Group exercises and discussions are combined with short lectures that introduce key theoretical concepts. Computational methods are used throughout the course, using the R programming language. Formative assessment exercises allow participants to test their understanding throughout the course and encourage questions and critical thinking.\nBy the end of the course participants will be able to critically evaluate and design effective research questions, linking experimental design concepts to subsequent statistical analyses. It will allow participants to make informed decisions on which statistical tests are most appropriate to their research questions. The course will provide a solid grounding for further development of applied statistical competencies.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Statistical analysis and experimental design",
    "section": "",
    "text": "Learning Objectives\n\n\n\nDuring this course you will learn about:\n\nOne and two sample hypothesis tests\nANOVA\nSimple linear Regression\nANCOVA\nLinear Models\nModel selection techniques\nPower Analyses\nPractices in experimental design that lead to high quality research\nWhat to do with more advanced analysis techniques for experiments with unusual or complex designs\nHow to take power analysis into consideration in your experimental design\nHow to implement piloting in your experiments\n\nAfter this course you should be able to:\n\nAnalyse datasets using standard statistical techniques\nKnow when each test is and is not appropriate\nLink experimental design to your statistical analysis strategy\nFormulate good research questions\nIdentify common design pitfalls, and how to avoid or mitigate them\nOperationalise variables effectively\nIdentify and deal with confounding variables and pseudoreplication\n\n\n\n\nTarget Audience\nThe course is aimed at people at postgraduate level who are involved in research.\nApplicants are expected to have a working knowledge of R and must complete a prerequisite quiz as part of the registration process.\nThe course is open to Postdocs and Staff members from the University of Cambridge, Affiliated Institutions and other external Institutions or individuals.\n\n\nPrerequisites\nWorking knowledge of R and the tidyverse package (assessed through a short quiz provided before acceptance on the course).\nThis course is not suitable for people who have completed either the Core Statistics or Experimental Design for Statistical Analysis courses, since significant portions of the course borrow from these stand-alone courses.\n\n\n\nExercises\nExercises in these materials are labelled according to their level of difficulty:\n\n\n\n\n\n\n\nLevel\nDescription\n\n\n\n\n  \nExercises in level 1 are simpler and designed to get you familiar with the concepts and syntax covered in the course.\n\n\n  \nExercises in level 2 combine different concepts together and apply it to a given task.\n\n\n  \nExercises in level 3 require going beyond the concepts and syntax introduced to solve new problems.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#authors",
    "href": "index.html#authors",
    "title": "Statistical analysis and experimental design",
    "section": "Authors",
    "text": "Authors\n\nAbout the authors:\n\nVicki Hodgson\nAffiliation: Bioinformatics Training Facility, University of Cambridge\nRoles: writing - original draft; conceptualisation; coding; creation of synthetic datasets\nMartin van Rongen \nAffiliation: Bioinformatics Training Facility, University of Cambridge\nRoles: writing - review & editing; conceptualisation; coding",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Data & Setup",
    "section": "",
    "text": "Data\nThe data used in these materials is provided as a zip file. Download and unzip the folder to your Desktop to follow along with the materials.\nDownload",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data & Setup</span>"
    ]
  },
  {
    "objectID": "setup.html#setup",
    "href": "setup.html#setup",
    "title": "Data & Setup",
    "section": "Setup",
    "text": "Setup\n\nR and RStudio\n\n\nWindows\nDownload and install all these using default options:\n\nR\nRTools\nRStudio\n\n\n\nMac OS\nDownload and install all these using default options:\n\nR\nRStudio\n\n\n\nLinux\n\nGo to the R installation folder and look at the instructions for your distribution.\nDownload the RStudio installer for your distribution and install it using your package manager.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data & Setup</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "3  References",
    "section": "",
    "text": "Glen, Stephanie. 2021. “Link Function.” Statistics How\nTo: Elementary Statistics for the Rest of Us! https://www.statisticshowto.com/link-function/.\n\n\nLamichhaney, Sangeet, Fan Han, Matthew T. Webster, B. Rosemary Grant,\nPeter R. Grant, and Leif Andersson. 2020. “Female-Biased Gene Flow\nBetween Two Species of Darwin’s Finches.” Nature Ecology\n& Evolution 4 (7): 979–86. https://doi.org/10.1038/s41559-020-1183-9.\n\n\nStevens, S. S. 1946. “On the Theory of Scales of\nMeasurement.” Science 103 (2684): 677–80. https://doi.org/10.1126/science.103.2684.677.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>References</span>"
    ]
  },
  {
    "objectID": "materials/research-questions.html",
    "href": "materials/research-questions.html",
    "title": "4  Research questions",
    "section": "",
    "text": "4.1 Criteria for a good research question\nFocused on a single topic\nThe scope of the question is important. If your research question is too broad, or tries to tackle multiple topics at once, you’ll struggle to design an experiment that can actually answer it. Even if you succeed, your experiment will necessarily be more complicated and take more time and resources to conduct, and may require compromising on some of your\nUses specific, well-defined concepts\nOr, alternatively, be prepared with your own definition of the concepts/variables that you’re studying! It should be clear to you in advance of collecting data precisely what your variables of interest are and how you’re going to measure them. This should also be clear to other researchers who may be reading your work or using your data, since your experiment should also be repeatable by others. This is all particularly true if you’re studying something that is a little abstract or can be defined in multiple ways. (See the section on operationalising variables for more on this.)\nRelevant and addressing a current research need\nYour research question should be focused on something that is of use to the scientific community or the public more broadly, either by increasing our understanding of basic science or by promoting translation to clinical or industry settings. It should, ideally, be motivated by the existing literature, or more specifically, by gaps or outstanding questions in the existing literature. We could get very philosophical here about the role of researchers in society, but the focus of this course is much more practical than that, and from a practical perspective (and a cynical one): these are the studies that get funded and published!\nResearchable\nThis perhaps seems obvious, but let’s unpack it a bit. It should be possible to answer your research question either by collecting original data, or using credible existing sources (e.g., a meta-analysis), and crucially, your research question shouldn’t depend on any subjective opinions or value judgements. For instance, it’s a good idea to avoid words like “best” or “worst” in your questions.\nOriginal\nIn short: what you’re trying to find out, shouldn’t be possible to find out elsewhere already. An important caveat here is that this does not mean that attempts to replicate prior experiments aren’t valid - they absolutely are. The research question for a replication study is different to the earlier study, because it is specifically asking whether a previously observed effect or phenomenon can be replicated.\nComplex and insightful\nBy “complex”, we don’t necessarily mean that the research question has to be difficult or convoluted for the sake of it - but it should have more than a simple yes or no answer. There are situations where a yes or no answer forms part of your overall conclusion - e.g., can tartigrades survive in the vacuum of space? Will compound X react with compound Y? Can humans remember their dreams? - but in all of these situations, a well-designed experiment should also give additional information, like the degree to which a particular phenomenon is observed, or details about the conditions under which it occurs. If you’re going to the effort of designing and conducting an experiment, you may as well be getting some detailed insight!",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Research questions</span>"
    ]
  },
  {
    "objectID": "materials/research-questions.html#some-examples",
    "href": "materials/research-questions.html#some-examples",
    "title": "4  Research questions",
    "section": "4.2 Some examples",
    "text": "4.2 Some examples\nBelow are a list of research questions that don’t quite meet all of the above criteria. Have a read through them, and see if you can identify the issues, and how you might refine the question to improve it.\n\nDoes owning a dog make you more likeable?\nWhy do programmers make typos in their code?\nHow does nuclear radiation affect humans?\nDo human beings have a soul that continues to exist after death?\nIs bureaucracy in the University bad or good?\nDoes spending a lot of time on social media affect children’s development?\nCan pet parrots live more than 50 years?",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Research questions</span>"
    ]
  },
  {
    "objectID": "materials/research-questions.html#summary",
    "href": "materials/research-questions.html#summary",
    "title": "4  Research questions",
    "section": "4.3 Summary",
    "text": "4.3 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nWithin the statistical inference and hypothesis testing framework, it is essential to set a good research question\nSetting the question is the first step to designing the experiment that will answer it\nA good research question should be focused, researchable, relevant, feasible, original and complex",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Research questions</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_one-sample.html",
    "href": "materials/cs1_practical_one-sample.html",
    "title": "5  One-sample data",
    "section": "",
    "text": "5.1 Libraries and functions",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>One-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_one-sample.html#libraries-and-functions",
    "href": "materials/cs1_practical_one-sample.html#libraries-and-functions",
    "title": "5  One-sample data",
    "section": "",
    "text": "Click to expand\n\n\n\n\n\n\nRPython\n\n\n\n5.1.1 Libraries\n\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\n\n\n5.1.2 Functions\n\n# Performs a one-sample t-test, Student's t-test and Welch's t-test in later sections\nstats::t.test()\n\n# Performs a Shapiro-Wilk test for normality\nstats::shapiro.test()\n\n# Performs one and two sample Wilcoxon tests\nstats::wilcox.test()\n\n# Plots a Q-Q plot for comparison with a normal distribution\nggplot2::stat_qq()\n\n# Adds a comparison line to the Q-Q plot\nggplot2::stat_qq_line()\n\n\n\n\n\n\n\n\n\n\n\nLibraries\nDescription\n\n\n\n\nplotnine\nThe Python equivalent of ggplot2.\n\n\npandas\nA Python data analysis and manipulation tool.\n\n\npingouin\nA Python module developed to have simple yet exhaustive stats functions.\n\n\n\n\n\n\nFunctions\nDescription\n\n\n\n\npingouin.normality()\nPerforms the Shapiro-Wilk test for normality.\n\n\npingouin.ttest()\nPerforms a t-test\n\n\npingouin.wilcoxon()\nWilcoxon signed rank test.\n\n\nplotnine.stats.stat_qq()\nPlots a Q-Q plot for comparison with a normal distribution.\n\n\nplotnine.stats.stat_qq_line()\nAdds a comparison line to the Q-Q plot.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>One-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_one-sample.html#purpose-and-aim",
    "href": "materials/cs1_practical_one-sample.html#purpose-and-aim",
    "title": "5  One-sample data",
    "section": "5.2 Purpose and aim",
    "text": "5.2 Purpose and aim\nOne sample tests are used when we have a single sample of continuous data. It is used to find out if the sample came from a parent distribution with a given mean (or median). This essentially boils down to finding out if the sample mean (or median) is “close enough” to our hypothesised parent population mean (or median). So, in the figure below, we could use these tests to see what the probability is that the sample of ten points comes from the distribution plotted above it i.e. a population with a mean of 20 mm.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>One-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_one-sample.html#choosing-a-test",
    "href": "materials/cs1_practical_one-sample.html#choosing-a-test",
    "title": "5  One-sample data",
    "section": "5.3 Choosing a test",
    "text": "5.3 Choosing a test\nThere are two tests that we are going to look at in this situation; the one-sample t-test, and the one-sample Wilcoxon signed rank-sum test. Both tests work on the sort of data that we’re considering here, but they both have different assumptions.\nIf your data is normally distributed, then a one-sample t-test is appropriate. If your data aren’t normally distributed, but their distribution is symmetric, and the sample size is small then a one-sample Wilcoxon signed rank-sum test is more appropriate.\nFor each statistical test we consider there will be five tasks. These will come back again and again, so pay extra close attention.\n\n\n\n\n\n\nImportant\n\n\n\n\nSetting out of the hypothesis\nSummarise and visualisation of the data\nAssessment of assumptions\nImplementation of the statistical test\nInterpreting the output and presentation of results\n\n\n\nWe won’t always carry these out in exactly the same order, but we will always consider each of the five tasks for every test.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>One-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_one-sample.html#data-and-hypotheses",
    "href": "materials/cs1_practical_one-sample.html#data-and-hypotheses",
    "title": "5  One-sample data",
    "section": "5.4 Data and hypotheses",
    "text": "5.4 Data and hypotheses\nFor example, suppose we measure the body lengths of male guppies (in mm) collected from the Guanapo River in Trinidad. We want to test whether the data support the hypothesis that the mean body is actually 20 mm. We form the following null and alternative hypotheses:\n\n\\(H_0\\): The mean body length is equal to 20mm (\\(\\mu =\\) 20).\n\\(H_1\\): The mean body length is not equal to 20mm (\\(\\mu \\neq\\) 20).\n\nWe will use a one-sample, two-tailed t-test to see if we should reject the null hypothesis or not.\n\nWe use a one-sample test because we only have one sample.\nWe use a two-tailed t-test because we want to know if our data suggest that the true (population) mean is different from 20 mm in either direction rather than just to see if it is greater than or less than 20 mm (in which case we would use a one-tailed test).\nWe’re using a t-test because we don’t know any better yet and because I’m telling you to. We’ll look at what the precise assumptions/requirements need to be in a moment.\n\nMake sure you have downloaded the data (see: Data section) and placed it within your working directory.\n\nRPython\n\n\nFirst we load the relevant libraries:\n\n# load tidyverse\nlibrary(tidyverse)\n\nWe then read in the data and create a table containing the data.\n\n# import the data\nfishlengthDF &lt;- read_csv(\"data/CS1-onesample.csv\")\n\nfishlengthDF\n\n# A tibble: 29 × 3\n      id river   length\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n 1     1 Guanapo   19.1\n 2     2 Guanapo   23.3\n 3     3 Guanapo   18.2\n 4     4 Guanapo   16.4\n 5     5 Guanapo   19.7\n 6     6 Guanapo   16.6\n 7     7 Guanapo   17.5\n 8     8 Guanapo   19.9\n 9     9 Guanapo   19.1\n10    10 Guanapo   18.8\n# ℹ 19 more rows\n\n\nThe first line reads the data into R and creates an object called a tibble, which is a type of data frame. This data frame contains 3 columns: a unique id, river encoding the river and length with the measured guppy length.\n\n\nWe then read the data in:\n\n# load the data\nfishlength_py = pd.read_csv('data/CS1-onesample.csv')\n\n# inspect the data\nfishlength_py.head()\n\n   id    river  length\n0   1  Guanapo    19.1\n1   2  Guanapo    23.3\n2   3  Guanapo    18.2\n3   4  Guanapo    16.4\n4   5  Guanapo    19.7",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>One-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_one-sample.html#summarise-and-visualise",
    "href": "materials/cs1_practical_one-sample.html#summarise-and-visualise",
    "title": "5  One-sample data",
    "section": "5.5 Summarise and visualise",
    "text": "5.5 Summarise and visualise\nSummarise the data and visualise it:\n\nRPython\n\n\n\nsummary(fishlengthDF)\n\n       id        river               length    \n Min.   : 1   Length:29          Min.   :11.2  \n 1st Qu.: 8   Class :character   1st Qu.:17.5  \n Median :15   Mode  :character   Median :18.8  \n Mean   :15                      Mean   :18.3  \n 3rd Qu.:22                      3rd Qu.:19.7  \n Max.   :29                      Max.   :23.3  \n\n\nFrom the summary() output we can see that the mean and median of the length variable are quite close together. The id column also has minimum, maximum, mean etc. values - these are not useful! The numbers in the id column have no numerical value, but are just to ensure each observation can be traced back, if needed.\n\nggplot(fishlengthDF,\n       aes(x = river,\n           y = length)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\nFirst we have a look at a numerical summary of the data:\n\nfishlength_py.describe()\n\n              id     length\ncount  29.000000  29.000000\nmean   15.000000  18.296552\nstd     8.514693   2.584636\nmin     1.000000  11.200000\n25%     8.000000  17.500000\n50%    15.000000  18.800000\n75%    22.000000  19.700000\nmax    29.000000  23.300000\n\n\nFrom the describe() output we can see that the mean and median of the length variable are quite close together. The id column also has minimum, maximum, mean etc. values - these are not useful! The numbers in the id column have no numerical value, but are just to ensure each observation can be traced back, if needed.\n\n(ggplot(fishlength_py,\n        aes(x = 'river',\n            y = 'length')) +\n     geom_boxplot())\n\n\n\n\n\n\n\n\n\n\n\nThe data do not appear to contain any obvious errors, and whilst both the mean and median are less than 20 (18.3 and 18.8 respectively) it is not absolutely certain that the sample mean is sufficiently different from this value to be “statistically significant”, although we may anticipate such a result.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>One-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_one-sample.html#assumptions",
    "href": "materials/cs1_practical_one-sample.html#assumptions",
    "title": "5  One-sample data",
    "section": "5.6 Assumptions",
    "text": "5.6 Assumptions\nWhen it comes to one-sample tests, we have two options:\n\nt-test\nWilcoxon signed-rank test\n\nFor us to use a t-test for this analysis (and for the results to be valid) we have to make two assumptions:\n\nThe parent distribution from which the sample is taken is normally distributed (and as such the sample data are normally distributed themselves).\n\n\n\n\n\n\n\nNote\n\n\n\nIt is worth noting though that the t-test is actually pretty robust in situations where the sample data are not normal. For sufficiently large sample sizes (your guess is as good as mine, but conventionally this means about 30 data points), you can use a t-test without worrying about whether the underlying population is normally distributed or not.\n\n\n\nEach data point in the sample is independent of the others. This is in general not something that can be tested for and instead has to be considered from the sampling procedure. For example, taking repeated measurements from the same individual would generate data that are not independent.\n\nThe second point we know nothing about and so we ignore it here (this is an issue that needs to be considered from the experimental design), whereas the first assumption can be checked. There are three ways of checking for normality:\nIn increasing order of rigour, we have\n\nHistogram\nQuantile-quantile plot\nShapiro-Wilk test\n\n\n5.6.1 Histogram of the data\nPlot a histogram of the data, which gives:\n\nRPython\n\n\n\nggplot(fishlengthDF,\n       aes(x = length)) +\n  geom_histogram(bins = 15)\n\n\n\n\n\n\n\n\n\n\n\n(ggplot(fishlength_py,\n        aes(x = \"length\")) +\n     geom_histogram(bins = 15))\n\n\n\n\n\n\n\n\n\n\n\nThe distribution appears to be uni-modal and symmetric, and so it isn’t obviously non-normal. However, there are a lot of distributions that have these simple properties but which aren’t normal, so this isn’t exactly rigorous. Thankfully there are other, more rigorous tests.\nNB. By even looking at this distribution to assess the assumption of normality we are already going far beyond what anyone else ever does. Nevertheless, we will continue.\n\n\n5.6.2 Q-Q plot of the data\nQ-Q plot is the short for quantile-quantile plot. This diagnostic plot (as it is sometimes called) is a way of comparing two distributions. How Q-Q plots work won’t be explained here but will be addressed in the next session.\nConstruct a Q-Q Plot of the quantiles of the data against the quantiles of a normal distribution:\n\nRPython\n\n\n\nggplot(fishlengthDF,\n       aes(sample = length)) +\n  stat_qq() +\n  stat_qq_line(colour = \"blue\")\n\n\n\n\n\n\n\n\n\n\n\n(ggplot(fishlength_py,\n        aes(sample = \"length\")) +\n     stat_qq() +\n     stat_qq_line(colour = \"blue\"))\n\n\n\n\n\n\n\n\n\n\n\nWhat is important to know is that if the data were normally distributed then all of the points should lie on (or close to) the diagonal line in this graph.\nIn this case, the points lie quite close to the line for the most part but the sample quantiles (points) from either end of the sample distribution are either smaller (below the line on the left) or larger (above the line on the right) than expected if they were supposed to be normally distributed. This suggests that the sample distribution is a bit more spread out than would be expected if it came from a normal distribution.\nIt is important to recognise that there isn’t a simple unambiguous answer when interpreting these types of graph, in terms of whether the assumption of normality has been well met or not and instead it often boils down to a matter of experience.\nIt is a very rare situation indeed where the assumptions necessary for a test will be met unequivocally and a certain degree of personal interpretation is always needed. Here you have to ask yourself whether the data are normal “enough” for you to be confident in the validity of the test.\nBelow are four examples of QQ plots for different types of distributions:\n\n\n\n\n\n\n\n\n\nThese two graphs relate to 200 data points that have been drawn from a normal distribution. Even here you can see that the points do not all lie perfectly on the diagonal line in the QQ plot, and a certain amount of deviation at the top and bottom of the graph can happen just by chance (if I were to draw a different set of point then the graph would look slightly different).\n\n\n\n\n\n\n\n\n\n\nThese two graphs relate to 200 data points that have been drawn from a uniform distribution. Uniform distributions are more condensed than normal distributions, and this is reflected in the QQ plot having a very pronounced S-shaped pattern to it (this is colloquially known as snaking).\n\n\n\n\n\n\n\n\n\n\nThese two graphs relate to 200 data points that have been drawn from a t distribution. t distributions are more spread out than normal distributions, and this is reflected in the QQ plot again having a very pronounced S-shaped pattern to it, but this time the snaking is a reflection of that observed for the uniform distribution.\n\n\n\n\n\n\n\n\n\n\nThese two graphs relate to 200 data points that have been drawn from an exponential distribution. Exponential distributions are not symmetric and are very skewed compared with normal distributions. The significant right-skew in this distribution is reflected in the QQ plot again having points that curve away above the diagonal line at both ends (a left-skew would have the points being below the line at both ends).\nIn all four cases it is worth noting that the deviations are only at the ends of the plot.\n\n\n5.6.3 Shapiro-Wilk test\nThis is one of a number of formal statistical test that assess whether a given sample of numbers come from a normal distribution. It calculates the probability of getting the sample data if the underlying distribution is in fact normal. It is very easy to carry out in R.\nPerform a Shapiro-Wilk test on the data:\n\nRPython\n\n\nThe shapiro.test() function needs a numerical vector as input. We get this by extracting the length column.\n\nshapiro.test(fishlengthDF$length)\n\n\n    Shapiro-Wilk normality test\n\ndata:  fishlengthDF$length\nW = 0.94938, p-value = 0.1764\n\n\n\nThe 1st line gives the name of the test and the data: tells you which data are used.\nThe 3rd line contains the two key outputs from the test:\nThe calculated W-statistic is 0.9494 (we don’t need to know this)\nThe p-value is 0.1764\n\n\n\nWe take the length values from the fishlength_py data frame and pass that to the normality() function in pingouin:\n\npg.normality(fishlength_py.length)\n\n               W      pval  normal\nlength  0.949384  0.176418    True\n\n\n\nthe W column gives us the W-statistic\nthe pval column gives us the p-value\nthe normal column gives us the outcome of the test in True/False\n\n\n\n\nAs the p-value is bigger than 0.05 (say) then we can say that there is insufficient evidence to reject the null hypothesis that the sample came from a normal distribution.\nIt is important to recognise that the Shapiro-Wilk test is not without limitations. It is rather sensitive to the sample size being considered. In general, for small sample sizes, the test is very relaxed about normality (and nearly all data sets are considered normal), whereas for large sample sizes the test can be overly strict, and it can fail to recognise data sets that are very nearly normal indeed.\n\n\n5.6.4 Assumptions overview\n\n\n\n\n\n\nImportant\n\n\n\nIn terms of assessing the assumptions of a test it is always worth considering several methods, both graphical and analytic, and not just relying on a single method.\n\n\nIn the fishlength example, the graphical Q-Q plot analysis was not especially conclusive as there was some suggestion of snaking in the plots, but the Shapiro-Wilk test gave a non-significant p-value (0.1764). Putting these two together, along with the original histogram and the recognition that there were only 30 data points in the data set I personally would be happy that the assumptions of the t-test were met well enough to trust the result of the t-test, but you may not be…\nIn which case we would consider an alternative test that has less stringent assumptions (but is less powerful): the one-sample Wilcoxon signed-rank test.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>One-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_one-sample.html#implement-and-interpret-the-test",
    "href": "materials/cs1_practical_one-sample.html#implement-and-interpret-the-test",
    "title": "5  One-sample data",
    "section": "5.7 Implement and interpret the test",
    "text": "5.7 Implement and interpret the test\nPerform a one-sample, two-tailed t-test:\n\nRPython\n\n\n\nt.test(fishlengthDF$length,\n       mu = 20, alternative = \"two.sided\")\n\n\n    One Sample t-test\n\ndata:  fishlengthDF$length\nt = -3.5492, df = 28, p-value = 0.001387\nalternative hypothesis: true mean is not equal to 20\n95 percent confidence interval:\n 17.31341 19.27969\nsample estimates:\nmean of x \n 18.29655 \n\n\n\nThe first argument must be a numerical vector of data values. In our case it’s the length values.\nThe second argument must be a number and is the mean to be tested under the null hypothesis.\nThe third argument gives the type of alternative hypothesis and must be one of two.sided, greater or less. We have no prior assumptions on whether the alternative fish length would be greater or less than 20, so we choose two.sided.\n\nIn the output:\n\nThe 1st line gives the name of the test and the 2nd line reminds you what the dataset was called\nThe 3rd line contains the three key outputs from the test:\n\nThe calculated t-value is -3.5492 (we’ll need this for reporting)\nThere are 28 degrees of freedom (again we’ll need this for reporting)\nThe p-value is 0.001387.\n\nThe 4th line simply states the alternative hypothesis\nThe 5th and 6th lines give the 95th confidence interval (we don’t need to know this)\nThe 7th, 8th and 9th lines give the sample mean again (18.29655).\n\n\n\n\npg.ttest(x = fishlength_py.length,\n         y = 20,\n         alternative = \"two-sided\").round(3)\n\n            T  dof alternative  p-val           CI95%  cohen-d    BF10  power\nT-test -3.549   28   two-sided  0.001  [17.31, 19.28]    0.659  25.071  0.929\n\n\n\nthe x argument must be a numerical series of data values\nthe y argument must be a number and is the mean to be tested under the null hypothesis\nthe alternative argument defines the alternative hypothesis (we have no expectation that the fish length is smaller or larger than 20, if the null hypothesis does not hold up)\nwith .round(3) we’re rounding the outcome to 3 digits\n\nIn the output:\nWe’re not focussing on all of the output just yet, but\n\nT gives us the value of the t-statistic\ndof gives us the degrees of freedom (we’ll need this for reporting)\npval gives us the p-value\n\n\n\n\nThe p-value is what we’re mostly interested in. It gives the probability of us getting a sample such as ours if the null hypothesis were actually true.\n\n\n\n\n\n\np-value interpretation\n\n\n\n\na high p-value means that there is a high probability of observing a sample such as ours and the null hypothesis is probably true whereas\na low p-value means that there is a low probability of observing a sample such as ours and the null hypothesis is probably not true.\n\nIt is important to realise that the p-value is just an indication and there is no absolute certainty here in this interpretation.\nPeople, however like more definite answers and so we pick an artificial probability threshold (called a significance level) in order to be able to say something more decisive. The standard significance level is 0.05 and since our p-value is smaller than this we choose to say that “it is very unlikely that we would have this particular sample if the null hypothesis were true”.\n\n\nIn this case the p-value is much smaller than 0.05, so we reject our null hypothesis and state that:\n\nA one-sample t-test indicated that the mean body length of male guppies (\\(\\bar{x}\\) = 18.29mm) differs significantly from 20 mm (p = 0.0014).\n\nThe above sentence is an adequate concluding statement for this test and is what we would write in any paper or report. Note that we have included (in brackets) information on the actual mean value of our group(\\(\\bar{x}\\) = 18.29mm) and the p-value (p = 0.0014). In some journals you are only required to report whether the p-value is less than the critical value (e.g. p &lt; 0.05) but I would always recommend reporting the actual p-value obtained.\n\n\n\n\n\n\nImportant\n\n\n\nAdditional information, such as the test statistic and degrees of freedom, are sometimes also reported. This is more of a legacy from the time where people did the calculations by hand and used tables. I personally find it much more useful to report as above and supply the data and analysis, so other people can see what I’ve done and why!",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>One-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_one-sample.html#dealing-with-non-normal-data",
    "href": "materials/cs1_practical_one-sample.html#dealing-with-non-normal-data",
    "title": "5  One-sample data",
    "section": "5.8 Dealing with non-normal data",
    "text": "5.8 Dealing with non-normal data\nYour data might not always be normally distributed. That’s not a huge issue and there are statistical tests that can deal with this. For a one-sample data set there is the Wilcoxon signed rank test. This test, in contrast to the one-sample t-test does not assume that the parent distribution is normally distributed. We do still need the parent distribution (and consequently the sample) to be the same shape and scale.\nThe Wilcoxon signed rank test checks if the rank-transformed values are symmetric around the median. As such, using this test we look to see if the median of the parent distributions differs significantly from a given hypothesised value (in contrast with the t-test that looks at the mean).\n\n5.8.1 Data and hypotheses\nAgain, we use the fishlength data set. The one-sample Wilcoxon signed rank test allows to see if the median body length is different from a specified value. Here we want to test whether the data support the hypothesis that the median body is actually 20 mm. The following null and alternative hypotheses are very similar to those used for the one sample t-test:\n\n\\(H_0\\): The median body length is equal to 20 mm (\\(\\mu =\\) 20).\n\\(H_1\\): The median body length is not equal to 20 mm (\\(\\mu \\neq\\) 20).\n\nWe will use a one-sample, two-tailed Wilcoxon signed rank test to see if we should reject the null hypothesis or not.\n\n\n5.8.2 Summarise and visualise\nWe did this before in the previous section, nothing really should have changed between now and then (if it has then you’re not off to a good start on this practical!)\n\n\n5.8.3 Assumptions\nIn order to use a one-sample Wilcoxon signed rank test for this analysis (and for the results to be strictly valid) we have to make two assumptions:\n\nThe data are distributed symmetrically around the median\nEach data point in the sample is independent of the others. This is the same as for the t-test and is a common feature of nearly all statistical tests. Lack of independence in your data is really tough to deal with (if not impossible) and a large part of proper experimental design is ensuring this.\n\nWhilst there are formal statistical tests for symmetry we will opt for a simple visual inspection using both a box plot and a histogram.\nPlot a histogram and a box plot of the data:\n\nRPython\n\n\nLet’s first determine the median, so we can use that to compare our data to. We’ll also store the value in an object called median_fishlength.\n\n# determine the median\nmedian_fishlength &lt;- median(fishlengthDF$length)\n\n\n# create a histogram\nfishlengthDF %&gt;% \n  ggplot(aes(x = length)) +\n  geom_histogram(bins = 10) +\n  geom_vline(xintercept = median_fishlength,\n             colour = \"red\")\n\n\n\n\n\n\n\n# create box plot\nfishlengthDF %&gt;% \n  ggplot(aes(y = length)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\nLet’s first determine the median, so we can use that to compare our data to.\n\nmedian_fishlength = fishlength_py.length.median()\n\n\n# create a histogram\n(ggplot(fishlength_py,\n        aes(x = \"length\")) +\n     geom_histogram(bins = 10) +\n     geom_vline(xintercept = median_fishlength,\n                colour = \"red\"))\n\n\n\n\n\n\n\n# create box plot\n(ggplot(fishlength_py,\n        aes(x = 1,\n            y = \"length\")) +\n     geom_boxplot())\n\n\n\n\n\n\n\n\n\n\n\nHere we can see that whilst the distribution isn’t perfectly symmetric, neither is it heavily skewed to the left or right and we can make the call that the distribution is symmetric enough for us to be happy with the results of the test.\n\n\n5.8.4 Implement and interpret the test\nPerform a one-sample, two-tailed Wilcoxon signed rank test:\n\nRPython\n\n\n\nwilcox.test(fishlengthDF$length,\n            mu = 20, alternative = \"two.sided\")\n\nWarning in wilcox.test.default(fishlengthDF$length, mu = 20, alternative =\n\"two.sided\"): cannot compute exact p-value with ties\n\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  fishlengthDF$length\nV = 67.5, p-value = 0.001222\nalternative hypothesis: true location is not equal to 20\n\n\nThe syntax is identical to the one-sample t-test we carried out earlier.\n\nThe first argument must be a numerical vector of data values.\nThe second argument must be a number and is the median to be tested under the null hypothesis.\nThe third argument gives the type of alternative hypothesis and must be one of two.sided, greater or less.\nThe first line gives a warning (not an error) message regarding the implementation of this test. This can be safely ignored in this case as the p-value is so small, but essentially, it’s letting you know that some of the data values are identical to each other. This is not supposed to happen as we should be dealing with continuous data for this test, but in practice it’s not something that we need to worry ourselves with.\nIt then gives you the name of the test and data: reminds you what the data set was called (here it’s the numerical length values coming from the pipe)\nNext are the two key outputs from the test:\n\nThe calculated V statistic is 67.5\nThe p-value is 0.001222.\n\nThe last line simply states the alternative hypothesis\n\n\n\n\npg.wilcoxon(fishlength_py.length - 20,\n            alternative = \"two-sided\")\n\n          W-val alternative     p-val       RBC  CLES\nWilcoxon   67.5   two-sided  0.000669 -0.689655   NaN\n\n\nThe syntax is similar to what we did earlier:\n\nThe 1st argument we give to the wilcoxon() function is an array of the differences between our data points and the median to be tested under the null hypothesis, i.e. our data points (fishlength_py.length) minus the test median (20, in this case).\nThe 2nd argument gives us the type of alternative hypothesis and must be one of “two-sided”, “larger”, or “smaller”.\n\n\n\n\nAgain, the p-value is what we’re most interested in. It gives the probability of us getting a sample such as ours if the null hypothesis were actually true. So, in this case since our p-value is less than 0.05 we can reject our null hypothesis and state that:\n\nA one-sample Wilcoxon signed rank test indicated that the median body length of male guppies (\\(\\tilde{x}\\) = 18.8 mm) differs significantly from 20 mm (p = 0.0012).\n\nThe above sentence is an adequate concluding statement for this test and is what we would write in any paper or report. Note that we have included (in brackets) information on the median value of the group (\\(\\tilde{x}\\) = 18.8 mm) and the p-value (p = 0.0012). Keep in mind that, when publishing, you’d also submit your data and scripts, so people can follow your analysis.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>One-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_one-sample.html#exercises",
    "href": "materials/cs1_practical_one-sample.html#exercises",
    "title": "5  One-sample data",
    "section": "5.9 Exercises",
    "text": "5.9 Exercises\n\n5.9.1 Gastric juices\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLevel: \nThe following data are the dissolving times (in seconds) of a drug in agitated gastric juice:\n42.7, 43.4, 44.6, 45.1, 45.6, 45.9, 46.8, 47.6\nThese data are stored in data/CS1-gastric_juices.csv.\nDo these results provide any evidence to suggest that dissolving time for this drug is different from 45 seconds?\n\nHere the data are already formatted for your convenience into a tidy format\nLoad the data from data/CS1-gastric_juices.csv\nWrite down the null and alternative hypotheses.\nSummarise and visualise the data and perform an appropriate one-sample t-test.\n\nWhat can you say about the dissolving time? (what sentence would you use to report this)\n\nCheck the assumptions for the test.\n\nWas the test valid?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n5.10 Answer\n\nHypotheses\n\\(H_0\\) : mean \\(=\\) 45s\n\\(H_1\\) : mean \\(\\neq\\) 45s\n\n\nData, summarise & visualise\nWe read in the data from CS1-gastric_juices.csv. It contains two columns, an id column and a dissolving_time column with the measured values.\n\nRPython\n\n\n\n# load the data\ndissolving &lt;- read_csv(\"data/CS1-gastric_juices.csv\")\n\n# have a look at the data\ndissolving\n\n# A tibble: 8 × 2\n     id dissolving_time\n  &lt;dbl&gt;           &lt;dbl&gt;\n1     1            42.7\n2     2            43.4\n3     3            44.6\n4     4            45.1\n5     5            45.6\n6     6            45.9\n7     7            46.8\n8     8            47.6\n\n# summarise the data\nsummary(dissolving)\n\n       id       dissolving_time\n Min.   :1.00   Min.   :42.70  \n 1st Qu.:2.75   1st Qu.:44.30  \n Median :4.50   Median :45.35  \n Mean   :4.50   Mean   :45.21  \n 3rd Qu.:6.25   3rd Qu.:46.12  \n Max.   :8.00   Max.   :47.60  \n\n\nWe can look at the histogram and box plot of the data:\n\n# create a histogram\nggplot(dissolving,\n       aes(x = dissolving_time)) +\n  geom_histogram(bins = 4)\n\n\n\n\n\n\n\n# create a boxplot\nggplot(dissolving,\n       aes(y = dissolving_time)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\n# load the data\ndissolving_py = pd.read_csv(\"data/CS1-gastric_juices.csv\")\n\n# have a look at the data\ndissolving_py.head()\n\n   id  dissolving_time\n0   1             42.7\n1   2             43.4\n2   3             44.6\n3   4             45.1\n4   5             45.6\n\n# summarise the data\ndissolving_py.describe()\n\n            id  dissolving_time\ncount  8.00000         8.000000\nmean   4.50000        45.212500\nstd    2.44949         1.640068\nmin    1.00000        42.700000\n25%    2.75000        44.300000\n50%    4.50000        45.350000\n75%    6.25000        46.125000\nmax    8.00000        47.600000\n\n\nWe can look at the histogram and box plot of the data:\n\n# create a histogram\n(ggplot(dissolving_py,\n        aes(x = \"dissolving_time\")) +\n     geom_histogram(bins = 4))\n\n\n\n\n\n\n\n\n\n# create a box plot\n(ggplot(dissolving_py,\n        aes(x = 1,\n            y = \"dissolving_time\")) +\n     geom_boxplot())\n\n\n\n\n\n\n\n\nPython (or plotnine in particular) gets a bit cranky if you try to create a geom_boxplot but do not define the x aesthetic. Hence us putting it as 1. The value has no numerical meaning, however.\n\n\n\nThere are only 8 data points, so the histogram is rather uninformative. Thankfully the box plot is a bit more useful here. We can see:\n\nThere don’t appear to be any major errors in data entry and there aren’t any huge outliers\nThe median value in the box-plot (the thick black line) is pretty close to 45 and so I wouldn’t be surprised if the mean of the data isn’t significantly different from 45. We can confirm that by looking at the mean and median values that we calculated using the summary command from earlier.\nThe data appear to be symmetric, and so whilst we can’t tell if they’re normal they’re a least not massively skewed.\n\n\n\nAssumptions\nNormality:\n\nRPython\n\n\n\n# perform Shapiro-Wilk test\nshapiro.test(dissolving$dissolving_time)\n\n\n    Shapiro-Wilk normality test\n\ndata:  dissolving$dissolving_time\nW = 0.98023, p-value = 0.9641\n\n\n\n# create a Q-Q plot\nggplot(dissolving,\n       aes(sample = dissolving_time)) +\n  stat_qq() +\n  stat_qq_line(colour = \"blue\")\n\n\n\n\n\n\n\n\n\n\n\n# Perform Shapiro-Wilk test to check normality\npg.normality(dissolving_py.dissolving_time)\n\n                        W      pval  normal\ndissolving_time  0.980234  0.964054    True\n\n\n\n# Create a Q-Q plot\n(ggplot(dissolving_py,\n        aes(sample = \"dissolving_time\")) +\n     stat_qq() +\n     stat_qq_line(colour = \"blue\"))\n\n\n\n\n\n\n\n\n\n\n\n\nThe Shapiro test has a p-value of 0.964 which (given that it is bigger than 0.05) suggests that the data are normal enough.\nThe Q-Q plot isn’t perfect, with some deviation of the points away from the line but since the points aren’t accelerating away from the line and, since we only have 8 points, we can claim, with some slight reservations, that the assumption of normality appears to be adequately well met.\n\nOverall, we are somewhat confident that the assumption of normality is well-enough met for the t-test to be an appropriate method for analysing the data. Note the ridiculous number of caveats here and the slightly political/slippery language I’m using. This is intentional and reflects the ambiguous nature of assumption checking. This is an important approach to doing statistics that you need to embrace.\nIn reality, if I found myself in this situation I would also try doing a non-parametric test on the data (Wilcoxon signed-rank test) and see whether I get the same conclusion about whether the median dissolving time differs from 45s. Technically, you don’t know about the Wilcoxon test yet as you haven’t done that section of the materials. Anyway, if I get the same conclusion then my confidence in the result of the test goes up considerably; it doesn’t matter how well an assumption has been met, I get the same result. If on the other hand I get a completely different conclusion from carrying out the non-parametric test then all bets are off; I now have very little confidence in my test result as I don’t know which one to believe (in the case that the assumptions of the test are a bit unclear). In this example a Wilcoxon test also gives us a non-significant result and so all is good.\n\n\nImplement test\n\nRPython\n\n\n\n# perform one-sample t-test\nt.test(dissolving$dissolving_time,\n       mu = 45, alternative = \"two.sided\")\n\n\n    One Sample t-test\n\ndata:  dissolving$dissolving_time\nt = 0.36647, df = 7, p-value = 0.7248\nalternative hypothesis: true mean is not equal to 45\n95 percent confidence interval:\n 43.84137 46.58363\nsample estimates:\nmean of x \n  45.2125 \n\n\n\n\n\npg.ttest(x = dissolving_py.dissolving_time,\n         y = 45,\n         alternative = \"two-sided\").round(3)\n\n            T  dof alternative  p-val           CI95%  cohen-d   BF10  power\nT-test  0.366    7   two-sided  0.725  [43.84, 46.58]     0.13  0.356  0.062\n\n\n\n\n\n\nA one-sample t-test indicated that the mean dissolving time of the drug is not significantly different from 45s (\\(\\bar{x}\\) = 45.2, p = 0.725).\n\nAnd that, is that.\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.10.1 Gastric juices (revisited)\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLevel: \nWhat if we were unsure if we could assume normality here? In that case we’d have to perform a Wilcoxon signed rank test.\n\nAnalyse the drug data set from before using a one-sample Wilcoxon signed rank test\nDiscuss with a (virtual) neighbour which of the two tests you feel is best suited to the data.\nDoes it matter in this case?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nHypotheses\n\\(H_0\\) : median \\(=\\) 45s\n\\(H_1\\) : median \\(\\neq\\) 45s\n\n\nAssumptions\nFrom the box plot from the previous exercise we already know that the data are symmetric enough for the test to be valid.\n\n\nWilcoxon signed rank test\nCalculating the median and performing the test:\n\nRPython\n\n\n\nmedian(dissolving$dissolving_time)\n\n[1] 45.35\n\n\n\nwilcox.test(dissolving$dissolving_time,\n            mu = 45, alternative = \"two.sided\")\n\n\n    Wilcoxon signed rank exact test\n\ndata:  dissolving$dissolving_time\nV = 22, p-value = 0.6406\nalternative hypothesis: true location is not equal to 45\n\n\n\n\n\ndissolving_py.dissolving_time.median()\n\n45.35\n\n\n\npg.wilcoxon(dissolving_py.dissolving_time - 45,\n            alternative = \"two-sided\")\n\n          W-val alternative     p-val       RBC  CLES\nWilcoxon   14.0   two-sided  0.640625  0.222222   NaN\n\n\n\n\n\n\nA one-sample Wilcoxon-signed rank test indicated that the median dissolving time of the drug is not significantly different from 45 s (\\(\\tilde{x}\\) = 45.35 , p = 0.64)\n\n\n\nDiscussion\nIn terms of choosing between the two test we can see that both meet their respective assumptions and so both tests are valid. In this case both tests also agree in terms of their conclusions i.e. that the average dissolving time (either mean or median) doesn’t differ significantly from the proposed value of 45 s.\n\nSo one answer would be that it doesn’t matter which test you use.\nAnother answer would be that you should pick the test that measures the quantity you’re interested in i.e. if you care about medians then use the Wilcoxon test, whereas if you care about means then use the t-test.\nA final answer would be that, since both test are valid we would prefer to use the test with greater power. t-tests always have more power than Wilcoxon tests (as long as they’re valid) and so we could report that one. (We’ll talk about this in the last session but power is effectively the capacity of a test to detect a significant difference - so more power is better).",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>One-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_one-sample.html#answer",
    "href": "materials/cs1_practical_one-sample.html#answer",
    "title": "5  One-sample data",
    "section": "5.10 Answer",
    "text": "5.10 Answer\n\nHypotheses\n\\(H_0\\) : mean \\(=\\) 45s\n\\(H_1\\) : mean \\(\\neq\\) 45s\n\n\nData, summarise & visualise\nWe read in the data from CS1-gastric_juices.csv. It contains two columns, an id column and a dissolving_time column with the measured values.\n\nRPython\n\n\n\n# load the data\ndissolving &lt;- read_csv(\"data/CS1-gastric_juices.csv\")\n\n# have a look at the data\ndissolving\n\n# A tibble: 8 × 2\n     id dissolving_time\n  &lt;dbl&gt;           &lt;dbl&gt;\n1     1            42.7\n2     2            43.4\n3     3            44.6\n4     4            45.1\n5     5            45.6\n6     6            45.9\n7     7            46.8\n8     8            47.6\n\n# summarise the data\nsummary(dissolving)\n\n       id       dissolving_time\n Min.   :1.00   Min.   :42.70  \n 1st Qu.:2.75   1st Qu.:44.30  \n Median :4.50   Median :45.35  \n Mean   :4.50   Mean   :45.21  \n 3rd Qu.:6.25   3rd Qu.:46.12  \n Max.   :8.00   Max.   :47.60  \n\n\nWe can look at the histogram and box plot of the data:\n\n# create a histogram\nggplot(dissolving,\n       aes(x = dissolving_time)) +\n  geom_histogram(bins = 4)\n\n\n\n\n\n\n\n# create a boxplot\nggplot(dissolving,\n       aes(y = dissolving_time)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\n# load the data\ndissolving_py = pd.read_csv(\"data/CS1-gastric_juices.csv\")\n\n# have a look at the data\ndissolving_py.head()\n\n   id  dissolving_time\n0   1             42.7\n1   2             43.4\n2   3             44.6\n3   4             45.1\n4   5             45.6\n\n# summarise the data\ndissolving_py.describe()\n\n            id  dissolving_time\ncount  8.00000         8.000000\nmean   4.50000        45.212500\nstd    2.44949         1.640068\nmin    1.00000        42.700000\n25%    2.75000        44.300000\n50%    4.50000        45.350000\n75%    6.25000        46.125000\nmax    8.00000        47.600000\n\n\nWe can look at the histogram and box plot of the data:\n\n# create a histogram\n(ggplot(dissolving_py,\n        aes(x = \"dissolving_time\")) +\n     geom_histogram(bins = 4))\n\n\n\n\n\n\n\n\n\n# create a box plot\n(ggplot(dissolving_py,\n        aes(x = 1,\n            y = \"dissolving_time\")) +\n     geom_boxplot())\n\n\n\n\n\n\n\n\nPython (or plotnine in particular) gets a bit cranky if you try to create a geom_boxplot but do not define the x aesthetic. Hence us putting it as 1. The value has no numerical meaning, however.\n\n\n\nThere are only 8 data points, so the histogram is rather uninformative. Thankfully the box plot is a bit more useful here. We can see:\n\nThere don’t appear to be any major errors in data entry and there aren’t any huge outliers\nThe median value in the box-plot (the thick black line) is pretty close to 45 and so I wouldn’t be surprised if the mean of the data isn’t significantly different from 45. We can confirm that by looking at the mean and median values that we calculated using the summary command from earlier.\nThe data appear to be symmetric, and so whilst we can’t tell if they’re normal they’re a least not massively skewed.\n\n\n\nAssumptions\nNormality:\n\nRPython\n\n\n\n# perform Shapiro-Wilk test\nshapiro.test(dissolving$dissolving_time)\n\n\n    Shapiro-Wilk normality test\n\ndata:  dissolving$dissolving_time\nW = 0.98023, p-value = 0.9641\n\n\n\n# create a Q-Q plot\nggplot(dissolving,\n       aes(sample = dissolving_time)) +\n  stat_qq() +\n  stat_qq_line(colour = \"blue\")\n\n\n\n\n\n\n\n\n\n\n\n# Perform Shapiro-Wilk test to check normality\npg.normality(dissolving_py.dissolving_time)\n\n                        W      pval  normal\ndissolving_time  0.980234  0.964054    True\n\n\n\n# Create a Q-Q plot\n(ggplot(dissolving_py,\n        aes(sample = \"dissolving_time\")) +\n     stat_qq() +\n     stat_qq_line(colour = \"blue\"))\n\n\n\n\n\n\n\n\n\n\n\n\nThe Shapiro test has a p-value of 0.964 which (given that it is bigger than 0.05) suggests that the data are normal enough.\nThe Q-Q plot isn’t perfect, with some deviation of the points away from the line but since the points aren’t accelerating away from the line and, since we only have 8 points, we can claim, with some slight reservations, that the assumption of normality appears to be adequately well met.\n\nOverall, we are somewhat confident that the assumption of normality is well-enough met for the t-test to be an appropriate method for analysing the data. Note the ridiculous number of caveats here and the slightly political/slippery language I’m using. This is intentional and reflects the ambiguous nature of assumption checking. This is an important approach to doing statistics that you need to embrace.\nIn reality, if I found myself in this situation I would also try doing a non-parametric test on the data (Wilcoxon signed-rank test) and see whether I get the same conclusion about whether the median dissolving time differs from 45s. Technically, you don’t know about the Wilcoxon test yet as you haven’t done that section of the materials. Anyway, if I get the same conclusion then my confidence in the result of the test goes up considerably; it doesn’t matter how well an assumption has been met, I get the same result. If on the other hand I get a completely different conclusion from carrying out the non-parametric test then all bets are off; I now have very little confidence in my test result as I don’t know which one to believe (in the case that the assumptions of the test are a bit unclear). In this example a Wilcoxon test also gives us a non-significant result and so all is good.\n\n\nImplement test\n\nRPython\n\n\n\n# perform one-sample t-test\nt.test(dissolving$dissolving_time,\n       mu = 45, alternative = \"two.sided\")\n\n\n    One Sample t-test\n\ndata:  dissolving$dissolving_time\nt = 0.36647, df = 7, p-value = 0.7248\nalternative hypothesis: true mean is not equal to 45\n95 percent confidence interval:\n 43.84137 46.58363\nsample estimates:\nmean of x \n  45.2125 \n\n\n\n\n\npg.ttest(x = dissolving_py.dissolving_time,\n         y = 45,\n         alternative = \"two-sided\").round(3)\n\n            T  dof alternative  p-val           CI95%  cohen-d   BF10  power\nT-test  0.366    7   two-sided  0.725  [43.84, 46.58]     0.13  0.356  0.062\n\n\n\n\n\n\nA one-sample t-test indicated that the mean dissolving time of the drug is not significantly different from 45s (\\(\\bar{x}\\) = 45.2, p = 0.725).\n\nAnd that, is that.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>One-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_one-sample.html#summary",
    "href": "materials/cs1_practical_one-sample.html#summary",
    "title": "5  One-sample data",
    "section": "5.11 Summary",
    "text": "5.11 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nOne-sample tests are used when you have a single sample of continuous data\nThe t-test assumes that the data are normally distributed and independent of each other\nA good way of assessing the assumption of normality is by checking the data against a Q-Q plot\nThe Wilcoxon signed rank test is used when you have a single sample of continuous data, which is not normally distributed",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>One-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_two-samples.html",
    "href": "materials/cs1_practical_two-samples.html",
    "title": "6  Two-sample data",
    "section": "",
    "text": "6.1 Libraries and functions",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Two-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_two-samples.html#libraries-and-functions",
    "href": "materials/cs1_practical_two-samples.html#libraries-and-functions",
    "title": "6  Two-sample data",
    "section": "",
    "text": "Click to expand\n\n\n\n\n\n\nRPython\n\n\n\n6.1.1 Libraries\n\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\n# Converts stats functions to a tidyverse-friendly format\nlibrary(rstatix)\n\n\n\n6.1.2 Functions\n\n# Computes summary statistics                         \nrstatix::get_summary_stats() \n\n# Performs Levene's test for equality of variance\n# (non-normally distributed data)\nrstatix::levene_test()\n\n# Performs Bartlett's test for equality of variance\n# (normally distributed data)\nstats::bartlett.test()\n\n# Performs Shapiro Wilk test\nstats::shapiro.test()\n\n# Performs one- and two-sample Wilcoxon tests\n# the latter is also known as 'Mann-Whitney U' test\nstats::wilcox.test()\n\n# Plots a Q-Q plot for comparison with a normal distribution\nggplot2::stat_qq()\n\n# Adds a comparison line to the Q-Q plot\nggplot2::stat_qq_line()\n\n\n\n\n\n\n\n\n\n\n\nLibraries\nDescription\n\n\n\n\nplotnine\nThe Python equivalent of ggplot2.\n\n\npandas\nA Python data analysis and manipulation tool.\n\n\npingouin\nA Python module developed to have simple yet exhaustive stats functions.\n\n\n\n\n\n\nFunctions\nDescription\n\n\n\n\npandas.DataFrame.read_csv\nReads in a .csv file\n\n\npandas.DataFrame.head()\nPlots the first few rows\n\n\npandas.DataFrame.describe()\nGives summary statistics\n\n\npandas.DataFrame.groupby()\nGroup DataFrame using a mapper or by a Series of columns\n\n\npandas.DataFrame.pivot()\nReturn reshaped DataFrame organised by given index / column values.\n\n\npandas.DataFrame.query()\nQuery the columns of a DataFrame with a boolean expression\n\n\npingouin.normality()\nPerforms the Shapiro-Wilk test for normality.\n\n\npingouin.homoscedasticity()\nChecks for equality of variance.\n\n\npingouin.ttest()\nPerforms a t-test\n\n\npingouin.mwu()\nPerforms the Mann-Whitney U test.\n\n\nplotnine.stats.stat_qq()\nPlots a Q-Q plot for comparison with a normal distribution.\n\n\nplotnine.stats.stat_qq_line()\nAdds a comparison line to the Q-Q plot.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Two-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_two-samples.html#purpose-and-aim",
    "href": "materials/cs1_practical_two-samples.html#purpose-and-aim",
    "title": "6  Two-sample data",
    "section": "6.2 Purpose and aim",
    "text": "6.2 Purpose and aim\nThese two-sample Student’s t-test is used when we have two samples of continuous data where we are trying to find out if the samples came from the same parent distribution or not. This essentially boils down to finding out if there is a difference in the means of the two samples.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Two-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_two-samples.html#data-and-hypotheses",
    "href": "materials/cs1_practical_two-samples.html#data-and-hypotheses",
    "title": "6  Two-sample data",
    "section": "6.3 Data and hypotheses",
    "text": "6.3 Data and hypotheses\nFor example, suppose we now measure the body lengths of male guppies (in mm) collected from two rivers in Trinidad; the Aripo and the Guanapo. We want to test whether the mean body length differs between samples. We form the following null and alternative hypotheses:\n\n\\(H_0\\): The mean body length does not differ between the two groups \\((\\mu A = \\mu G)\\)\n\\(H_1\\): The mean body length does differ between the two groups \\((\\mu A \\neq \\mu G)\\)\n\nWe use a two-sample, two-tailed t-test to see if we can reject the null hypothesis.\n\nWe use a two-sample test because we now have two samples.\nWe use a two-tailed t-test because we want to know if our data suggest that the true (population) means are different from one another rather than that one mean is specifically bigger or smaller than the other.\nWe’re using Student’s t-test because the sample sizes are big and because we’re assuming that the parent populations have equal variance (We can check this later).\n\nThe data are stored in the file data/CS1-twosample.csv.\nLet’s read in the data and have a quick look at the first rows to see how the data is structured.\nMake sure you have downloaded the data and placed it within your working directory.\n\nRPython\n\n\nFirst we load the relevant libraries:\n\n# load tidyverse\nlibrary(tidyverse)\n\n# load rstatix, a tidyverse-friendly stats package\nlibrary(rstatix)\n\nWe then read in the data and create a table containing the data.\n\nrivers &lt;- read_csv(\"data/CS1-twosample.csv\")\n\nrivers\n\n# A tibble: 68 × 2\n   river   length\n   &lt;chr&gt;    &lt;dbl&gt;\n 1 Guanapo   19.1\n 2 Guanapo   23.3\n 3 Guanapo   18.2\n 4 Guanapo   16.4\n 5 Guanapo   19.7\n 6 Guanapo   16.6\n 7 Guanapo   17.5\n 8 Guanapo   19.9\n 9 Guanapo   19.1\n10 Guanapo   18.8\n# ℹ 58 more rows\n\n\n\n\n\nrivers_py = pd.read_csv(\"data/CS1-twosample.csv\")\n\nrivers_py.head()\n\n     river  length\n0  Guanapo    19.1\n1  Guanapo    23.3\n2  Guanapo    18.2\n3  Guanapo    16.4\n4  Guanapo    19.7",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Two-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_two-samples.html#cs1-students-sumvisual",
    "href": "materials/cs1_practical_two-samples.html#cs1-students-sumvisual",
    "title": "6  Two-sample data",
    "section": "6.4 Summarise and visualise",
    "text": "6.4 Summarise and visualise\nLet’s first summarise the data.\n\nRPython\n\n\n\nsummary(rivers)\n\n    river               length     \n Length:68          Min.   :11.20  \n Class :character   1st Qu.:18.40  \n Mode  :character   Median :19.30  \n                    Mean   :19.46  \n                    3rd Qu.:20.93  \n                    Max.   :26.40  \n\n\nThis gives us the standard summary statistics, but in this case we have more than one group (Aripo and Guanapo), so it might be helpful to get summary statistics per group. One way of doing this is by using the get_summary_stats() function from the rstatix library.\n\n# get common summary stats for the length column\nrivers %&gt;% \n  group_by(river) %&gt;% \n  get_summary_stats(type = \"common\")\n\n# A tibble: 2 × 11\n  river   variable     n   min   max median   iqr  mean    sd    se    ci\n  &lt;chr&gt;   &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Aripo   length      39  17.5  26.4   20.1   2.2  20.3  1.78 0.285 0.577\n2 Guanapo length      29  11.2  23.3   18.8   2.2  18.3  2.58 0.48  0.983\n\n\nNumbers might not always give you the best insight into your data, so we also visualise our data:\n\nggplot(rivers,\n       aes(x = river, y = length)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\nrivers_py.describe()\n\n          length\ncount  68.000000\nmean   19.463235\nstd     2.370081\nmin    11.200000\n25%    18.400000\n50%    19.300000\n75%    20.925000\nmax    26.400000\n\n\nThis gives us the standard summary statistics, but in this case we have more than one group (Aripo and Guanapo), so it might be helpful to get summary statistics per group. Here we use the pd.groupby() function to group by river. We only want to have summary statistics for the length variable, so we specify that as well:\n\nrivers_py.groupby(\"river\")[\"length\"].describe()\n\n         count       mean       std   min   25%   50%   75%   max\nriver                                                            \nAripo     39.0  20.330769  1.780620  17.5  19.1  20.1  21.3  26.4\nGuanapo   29.0  18.296552  2.584636  11.2  17.5  18.8  19.7  23.3\n\n\nNumbers might not always give you the best insight into your data, so we also visualise our data:\n\n(ggplot(rivers_py,\n        aes(x = \"river\", y = \"length\")) + \n     geom_boxplot())\n\n\n\n\n\n\n\n\n\n\n\nThe box plot does appear to suggest that the two samples have different means, and moreover that the guppies in Guanapo may be smaller than the guppies in Aripo. It isn’t immediately obvious that the two populations don’t have equal variances though (box plots are not quite the right tool for this), so we plough on. Who ever said statistics would be glamorous?",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Two-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_two-samples.html#assumptions",
    "href": "materials/cs1_practical_two-samples.html#assumptions",
    "title": "6  Two-sample data",
    "section": "6.5 Assumptions",
    "text": "6.5 Assumptions\nIn order to use a Student’s t-test (and for the results to be strictly valid) we have to make three assumptions:\n\nThe parent distributions from which the samples are taken are both normally distributed (which would lead to the sample data being normally distributed too).\nEach data point in the samples is independent of the others.\nThe parent distributions should have the same variance.\n\nIn this example the first assumption can be ignored as the sample sizes are large enough (because of maths, with Aripo containing 39 and Guanapo 29 samples). If the samples were smaller then we would use the tests from the previous section.\nThe second point we can do nothing about unless we know how the data were collected, so again we ignore it.\nThe third point regarding equality of variance can be tested using either Bartlett’s test (if the samples are normally distributed) or Levene’s test (if the samples are not normally distributed).\nThis is where it gets a bit trickier. Although we don’t care if the samples are normally distributed for the t-test to be valid (because the sample size is big enough to compensate), we do need to know if they are normally distributed in order to decide which variance test to use.\nSo we perform a Shapiro-Wilk test on both samples separately.\n\nRPython\n\n\nWe can use the filter() function to filter the data by river, then we perform the Shapiro-Wilk test on the length measurement. The shapiro.test() function needs the data in a vector format. We get these by using the pull() function.\n\n\n\n\n\n\nTip\n\n\n\nIt’s good practice to check what kind of data is going into these functions. Run the code line-by-line to see what data is passed on from the filter() and pull() functions.\n\n\n\n# filter data by river and perform test\nrivers %&gt;% \n    filter(river == \"Aripo\") %&gt;% \n    pull(length) %&gt;% \n    shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.93596, p-value = 0.02802\n\nrivers %&gt;% \n    filter(river == \"Guanapo\") %&gt;% \n    pull(length) %&gt;% \n    shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.94938, p-value = 0.1764\n\n\n\n\nTo perform a Shapiro-Wilk test we can use the normality() function from pingouin. We can give it the data in the original ‘long’ format, where we specify:\n\ndv = dependent variable, length\ngroup = grouping variable, river\ndata = data frame\n\n\npg.normality(dv = \"length\",\n             group = \"river\",\n             data = rivers_py)\n\n                W      pval  normal\nGuanapo  0.949384  0.176418    True\nAripo    0.935958  0.028022   False\n\n\n\n\n\nWe can see that whilst the Guanapo data is probably normally distributed (p = 0.1764 &gt; 0.05), the Aripo data is unlikely to be normally distributed (p = 0.02802 &lt; 0.05). Remember that the p-value gives the probability of observing each sample if the parent population is actually normally distributed.\nThe Shapiro-Wilk test is quite sensitive to sample size. This means that if you have a large sample then even small deviations from normality will cause the sample to fail the test, whereas smaller samples are allowed to pass with much larger deviations. Here the Aripo data has nearly 40 points in it compared with the Guanapo data and so it is much easier for the Aripo sample to fail compared with the Guanapo data.\n\n\n\n\n\n\nImportant\n\n\n\nComplete Exercise 6.9.1.\n\n\nThe Q-Q plots show the opposite of what we found with the Shapiro-Wilk tests: the data for Aripo look pretty normally distributed apart from one data point, whereas the assumption of normality for the Guanapo data is less certain.\nWhat to do? Well, you could be conservative and state that you are not confident that the data in either group are normally distributed. That would be a perfectly reasonable conclusion.\nI would personally not have issues with stating that the Aripo data are probably normally distributed enough.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Two-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_two-samples.html#equality-of-variance",
    "href": "materials/cs1_practical_two-samples.html#equality-of-variance",
    "title": "6  Two-sample data",
    "section": "6.6 Equality of variance",
    "text": "6.6 Equality of variance\n\n\n\n\n\n\nTip\n\n\n\nRemember that statistical tests do not provide answers, they merely suggest patterns. Human interpretation is still a crucial aspect to what we do.\n\n\nThe reason why we’re checking for equality of variance (also referred to as homogeneity of variance) is because many statistical tests assume that the spread of the data within different parental populations (in this case, two) is the same.\nIf that is indeed the case, then the data themselves should have equal spread as well.\nThe Shapiro-Wilk test and the Q-Q plots have shown that some of the data might not be normal enough (although in opposite directions!) and so in order to test for equality of variance we will use Levene’s test.\n\nRPython\n\n\nThe function we use is levene_test() from the rstatix library.\nIt takes the data in the form of a formula as follows:\n\nlevene_test(data = rivers,\n            formula = length ~ river)\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     1    66      1.77 0.188\n\n\nOr shortened:\n\nlevene_test(rivers,\n            length ~ river)\n\nThe key bit of information is the p column. This is the p-value 0.1876 for this test.\n\n\nTo test for equality of variance, we can use the homoscedasticity() function from pingouin.\nNote that, contrary to R, we specify the type of test in the method argument. The default is \"levene\", assuming that data are not normally distributed.\n\npg.homoscedasticity(dv = \"length\",\n                    group = \"river\",\n                    method = \"levene\",\n                    data = rivers_py)\n\n               W      pval  equal_var\nlevene  1.773184  0.187569       True\n\n\n\n\n\nThe p-value tells us the probability of observing these two samples if they come from distributions with the same variance. As this probability is greater than our arbitrary significance level of 0.05 then we can be somewhat confident that the necessary assumptions for carrying out Student’s t-test on these two samples was valid. (Once again woohoo!)\n\n6.6.1 Bartlett’s test\nIf we had wanted to carry out Bartlett’s test (i.e. if the data had been sufficiently normally distributed) then we would have done:\n\nRPython\n\n\nHere we use bartlett.test() function.\n\nbartlett.test(length ~ river, data = rivers)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  length by river\nBartlett's K-squared = 4.4734, df = 1, p-value = 0.03443\n\n\nThe relevant p-value is given on the 3rd line.\n\n\n\npg.homoscedasticity(dv = \"length\",\n                    group = \"river\",\n                    method = \"bartlett\",\n                    data = rivers_py)\n\n                 T      pval  equal_var\nbartlett  4.473437  0.034426      False",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Two-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_two-samples.html#implement-and-interpret-the-test",
    "href": "materials/cs1_practical_two-samples.html#implement-and-interpret-the-test",
    "title": "6  Two-sample data",
    "section": "6.7 Implement and interpret the test",
    "text": "6.7 Implement and interpret the test\nIn this case we’re ignoring the fact that the data are not normal enough, according to the Shapiro-Wilk test. However, this is not entirely naughty, because the sample sizes are pretty large and the t-test is also pretty robust in this case, we can perform a t-test. Remember, this is only allowed because the variances of the two groups (Aripo and Guanapo) are equal.\nPerform a two-sample, two-tailed, t-test:\n\nRPython\n\n\n\n# two-sample, two-tailed t-test\nt.test(length ~ river,\n       alternative = \"two.sided\",\n       var.equal = TRUE,\n       data = rivers)\n\n\n    Two Sample t-test\n\ndata:  length by river\nt = 3.8433, df = 66, p-value = 0.0002754\nalternative hypothesis: true difference in means between group Aripo and group Guanapo is not equal to 0\n95 percent confidence interval:\n 0.9774482 3.0909868\nsample estimates:\n  mean in group Aripo mean in group Guanapo \n             20.33077              18.29655 \n\n\nHere we do the following:\n\nThe first argument must be in the formula format: variables ~ category\nThe second argument gives the type of alternative hypothesis and must be one of two.sided, greater or less\nThe third argument says whether the variance of the two samples can be assumed to be equal (Student’s t-test) or unequal (Welch’s t-test)\n\nSo, how do we interpret these results?\n\nThe 1st line gives the name of the test and the 2nd line reminds you what the data set was called, and what variables were used.\nThe 3rd line contains the three key outputs from the test:\n\nThe calculated t-value is 3.8433\nThere are 66 degrees of freedom\nThe p-value is 0.0002754.\n\nThe 4th line simply states the alternative hypothesis in terms of the difference between the two sample means (testing if the two sample means are different is equivalent to testing whether the difference in the means is equal to zero).\nThe 5th and 6th lines give the 95th confidence interval (we don’t need to know this here).\nThe 7th, 8th and 9th lines give the sample means for each group (20.33077 in Aripo and 18.29655 in Guanapo) which we found earlier.\n\n\n\nThe ttest() function in pingouin needs two vectors as input, so we split the data as follows:\n\naripo = rivers_py.query('river == \"Aripo\"')[\"length\"]\nguanapo = rivers_py.query('river == \"Guanapo\"')[\"length\"]\n\nNext, we perform the t-test. We specify that the variance are equal by setting correction = False. We also transpose() the data, so we can actually see the entire output.\n\npg.ttest(aripo, guanapo,\n         correction = False).transpose()\n\n                   T-test\nT                3.843267\ndof                    66\nalternative     two-sided\np-val            0.000275\nCI95%        [0.98, 3.09]\ncohen-d          0.942375\nBF10               92.191\npower            0.966135\n\n\n\n\n\nAgain, the p-value is what we’re most interested in. Since the p-value is very small (much smaller than the standard significance level) we choose to say “that it is very unlikely that these two samples came from the same parent distribution and as such we can reject our null hypothesis” and state that:\n\nA Student’s t-test indicated that the mean body length of male guppies in the Guanapo river (\\(\\bar{x}\\) = 18.29 mm) differs significantly from the mean body length of male guppies in the Aripo river (\\(\\bar{x}\\) = 20.33 mm, p = 0.0003).\n\nNow there’s a conversation starter.\n\n\n\n\n\n\nImportant\n\n\n\nComplete Exercise 6.9.2.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Two-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_two-samples.html#dealing-with-non-normal-data",
    "href": "materials/cs1_practical_two-samples.html#dealing-with-non-normal-data",
    "title": "6  Two-sample data",
    "section": "6.8 Dealing with non-normal data",
    "text": "6.8 Dealing with non-normal data\nIf we’re not sure that the data we are dealing with may come from a parent distribution that is normal, then we can’t use a Student’s t-test. Instead we use the Mann-Whitney U test. This test does not assume that the parent distributions are normally distributed. It does however assume that both have the same shape and variance. With this test we check if the medians of the two parent distributions differ significantly from each other.\n\n6.8.1 Data and hypotheses\nAgain, we use the rivers data set. We want to test whether the median body length of male guppies differs between samples. We form the following null and alternative hypotheses:\n\n\\(H_0\\): The difference in median body length between the two groups is 0 \\((\\mu A - \\mu G = 0)\\)\n\\(H_1\\): The difference in median body length between the two groups is not 0 \\((\\mu A - \\mu G \\neq 0)\\)\n\nWe use a two-tailed Mann-Whitney U test to see if we can reject the null hypothesis.\n\n\n6.8.2 Summarise and visualise\nWe did this in the previous section.\n\n\n6.8.3 Assumptions\nWe have checked these previously.\n\n\n6.8.4 Implement and interpret the test\nCalculate the median for each group (for reference) and perform a two-tailed, Mann-Whitney U test:\n\nRPython\n\n\nWe group the data using group_by() for each river and then use the summarise() the data.\n\nrivers %&gt;% \n    group_by(river) %&gt;% \n    summarise(median_length = median(length))\n\n# A tibble: 2 × 2\n  river   median_length\n  &lt;chr&gt;           &lt;dbl&gt;\n1 Aripo            20.1\n2 Guanapo          18.8\n\n\nPerform the Mann-Whitney U test:\n\nwilcox.test(length ~ river,\n            alternative = \"two.sided\",\n            data = rivers)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  length by river\nW = 841, p-value = 0.0006464\nalternative hypothesis: true location shift is not equal to 0\n\n\n\nThe first argument must be in the formula format: variable ~ category\nThe second argument gives the type of alternative hypothesis and must be one of two.sided, greater or less\n\nYou may get a warning message in the console stating cannot compute exact p-value with ties. This just means that some of the data points have exactly the same value which affects the internal mathematics slightly. However, given that the p-value is so very small, this is not something that we need to worry about.\nAfter the warning message:\n\nThe 1st line gives the name of the test and the 2nd line reminds you what the dataset was called, and what variables were used\nThe 3rd line contains the two key outputs from the test:\n\nThe calculated W-value is 841 (we’ll use this in reporting)\nThe p-value is 0.0006464.\n\nThe 4th line simply states the alternative hypothesis in terms of the difference between the two sample medians in that if there were a difference then one distribution would be shifted relative to the other.\n\n\n\nBefore we can implement the Mann-Whitney U test, we need to reformat our data a bit.\nThe pg.mwu() function requires the numerical input for the two groups it needs to compare.\nThe easiest way is to reformat our data from the long format where all the data are stacked on top of one another to the wide format, where the length values are in separate columns for the two rivers.\nWe can do this with the pd.pivot() function. We save the output in a new object and then access the values as required. It keeps all the data separate, meaning that there will be missing values NaN in this format. The pg.mwu() function ignores missing values by default.\n\n# reformat the data into a 'wide' format\nrivers_py_wide = pd.pivot(rivers_py,\n                          columns = 'river',\n                          values = 'length')\n      \n# have a look at the format\nrivers_py_wide.head()\n\nriver  Aripo  Guanapo\n0        NaN     19.1\n1        NaN     23.3\n2        NaN     18.2\n3        NaN     16.4\n4        NaN     19.7\n\n\nNext, we can calculate the median values for each river:\n\nrivers_py_wide['Aripo'].median()\n\n20.1\n\nrivers_py_wide['Guanapo'].median()\n\n18.8\n\n\nFinally, we can perform the Mann-Whitney U test:\n\n# perform the Mann-Whitney U test\n# ignoring the missing values\npg.mwu(rivers_py_wide['Aripo'],\n       rivers_py_wide['Guanapo'])\n\n     U-val alternative     p-val       RBC     CLES\nMWU  841.0   two-sided  0.000646 -0.487179  0.74359\n\n\n\n\n\nGiven that the p-value is less than 0.05 we can reject the null hypothesis at this confidence level. Again, the p-value on the 3rd line is what we’re most interested in. Since the p-value is very small (much smaller than the standard significance level) we choose to say “that it is very unlikely that these two samples came from the same parent distribution and as such we can reject our null hypothesis”.\nTo put it more completely, we can state that:\n\nA Mann-Whitney test indicated that the median body length of male guppies in the Guanapo river (\\(\\tilde{x}\\) = 18.8 mm) differs significantly from the median body length of male guppies in the Aripo river (\\(\\tilde{x}\\) = 20.1 mm, p = 0.0006).",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Two-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_two-samples.html#exercises",
    "href": "materials/cs1_practical_two-samples.html#exercises",
    "title": "6  Two-sample data",
    "section": "6.9 Exercises",
    "text": "6.9 Exercises\n\n6.9.1 Q-Q plots rivers\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLevel: \nCreate the Q-Q plots for the two rivers in the data/CS1-twosample.csv file and discuss with your neighbour what you see in light of the results from the above Shapiro-Wilk test.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\nRPython\n\n\n\n# we group the data by river\n# then create a panel per river\n# containing the Q-Q plot for that river\nggplot(rivers,\n       aes(sample = length)) +\n  stat_qq() +\n  stat_qq_line(colour = \"blue\") +\n  facet_wrap(facets = vars(river))\n\n\n\n\n\n\n\n\n\n\n\n(ggplot(rivers_py,\n        aes(sample = \"length\")) +\n     stat_qq() +\n     stat_qq_line(colour = \"red\") +\n     facet_wrap(\"river\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.9.2 Turtles\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLevel: \nThis exercise explores serum cholesterol concentrations in turtles.\nUsing the data in data/CS1-turtle.csv, test the null hypothesis that male and female turtles have the same mean serum cholesterol concentrations.\n\nLoad the data\nWrite down the null and alternative hypotheses\nImport the data\nSummarise and visualise the data\nCheck your assumptions (normality and variance) using appropriate tests and plots\nPerform a two-sample t-test\nWrite down a sentence that summarises the results that you have found\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n6.10 Answer\n\nData\nOnce you’ve loaded the data, have a look at the structure. The data is in a tidy data format, where each variable (the thing that you measure) is in its own column. Each observation has its own row.\n\n\nHypotheses\n\\(H_0\\) : male mean \\(=\\) female mean\n\\(H_1\\) : male mean \\(\\neq\\) female mean\n\n\nLoad, summarise and visualise data\nLet’s load the data and explore our data a bit more before we dive into the statistics.\n\nRPython\n\n\n\n# load the data\nturtle &lt;- read_csv(\"data/CS1-turtle.csv\")\n\n# and have a look\nturtle\n\n# A tibble: 13 × 2\n   serum sex   \n   &lt;dbl&gt; &lt;chr&gt; \n 1  220. Male  \n 2  219. Male  \n 3  230. Male  \n 4  229. Male  \n 5  222  Male  \n 6  224. Male  \n 7  226. Male  \n 8  223. Female\n 9  222. Female\n10  230. Female\n11  224. Female\n12  224. Female\n13  231. Female\n\n\nLet’s summarise the data (although a visualisation is probably much easier to work with):\n\n# create summary statistics for each group\nturtle %&gt;% \n  group_by(sex) %&gt;% \n  get_summary_stats(type = \"common\")\n\n# A tibble: 2 × 11\n  sex    variable     n   min   max median   iqr  mean    sd    se    ci\n  &lt;chr&gt;  &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Female serum        6  222.  231.   224.  5.22  226.  3.87  1.58  4.06\n2 Male   serum        7  219.  230.   224.  6.6   224.  4.26  1.61  3.94\n\n\nand visualise the data:\n\n# visualise the data\nggplot(turtle,\n       aes(x = sex, y = serum)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\nturtle_py = pd.read_csv(\"data/CS1-turtle.csv\")\n\nturtle_py.describe()\n\n            serum\ncount   13.000000\nmean   224.900000\nstd      3.978274\nmin    218.600000\n25%    222.000000\n50%    224.100000\n75%    228.800000\nmax    230.800000\n\n\nand visualise the data:\n\n(ggplot(turtle_py,\n        aes(x = \"sex\", y = \"serum\")) +\n     geom_boxplot())\n\n\n\n\n\n\n\n\n\n\n\nAs always we use the plot and summary to assess three things:\n\nDoes it look like we’ve loaded the data in correctly?\n\nWe have two groups and the extreme values of our plots seem to match with our data set, so I’m happy that we haven’t done anything massively wrong here.\n\nDo we think that there is a difference between the two groups?\n\nWe need the result of the formal test to make sense given the data, so it’s important to develop a sense of what we think is going to happen here. Whilst the ranges of the two groups suggests that the Female serum levels might be higher than the males when we look at things more closely we realise that isn’t the case. The box plot shows that the median values of the two groups is virtually identical and this is backed up by the summary statistics we calculated: the medians are both about 224.1, and the means are fairly close too (225.7 vs 224.2). Based on this, and the fact that there are only 13 observations in total I would be very surprised if any test came back showing that there was a difference between the groups.\n\nWhat do we think about assumptions?\n\nNormality looks a bit worrying: whilst the Male group appears nice and symmetric (and so might be normal), the Female group appears to be quite skewed (since the median is much closer to the bottom than the top). We’ll have to look carefully at the more formal checks to decided whether we think the data are normal enough for us to use a t-test.\nHomogeneity of variance. At this stage the spread of the data within each group looks similar, but because of the potential skew in the Female group we’ll again want to check the assumptions carefully.\n\n\n\n\nAssumptions\nNormality\nLet’s look at the normality of each of the groups separately. There are several ways of getting at the serum values for Male and Female groups separately. All of them come down to splitting the data. Afterwards we use the Shapiro-Wilk (‘formal’ test), followed by Q-Q plots (much more informative).\n\nRPython\n\n\n\n# perform Shapiro-Wilk test on each group\nturtle %&gt;% \n    filter(sex == \"Female\") %&gt;% \n    pull(serum) %&gt;% \n    shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.84178, p-value = 0.1349\n\nturtle %&gt;% \n    filter(sex == \"Male\") %&gt;% \n    pull(serum) %&gt;% \n    shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.94392, p-value = 0.6743\n\n\n\n\n\npg.normality(dv = \"serum\",\n             group = \"sex\",\n             data = turtle_py)\n\n               W      pval  normal\nMale    0.943924  0.674277    True\nFemale  0.841784  0.134869    True\n\n\n\n\n\nThe p-values for both Shapiro-Wilk tests are non-significant which suggests that the data are normal enough. This is a bit surprising given what we saw in the box plot but there are two bits of information that we can use to reassure us.\n\nThe p-value for the Female group is smaller than for the Male group (suggesting that the Female group is closer to being non-normal than the Male group) which makes sense based on our visual observations.\nThe Shapiro-Wilk test is generally quite relaxed about normality for small sample sizes (and notoriously strict for very large sample sizes). For a group with only 6 data points in it, the data would actually have to have a really, really skewed distribution. Given that the Female group only has 6 data points in it, it’s not too surprising that the Shapiro-Wilk test came back saying everything is OK.\n\nGiven these caveats of the Shapiro-Wilk test (I’ll stop mentioning them now, I think I’ve made my opinion clear ;)), let’s look at the Q-Q plots.\n\nRPython\n\n\n\n# create Q-Q plots for both groups\nggplot(turtle,\n       aes(sample = serum)) +\n  stat_qq() +\n  stat_qq_line(colour = \"blue\") +\n  facet_wrap(facets = vars(sex))\n\n\n\n\n\n\n\n\n\n\n\n# create Q-Q plots for both groups\n(ggplot(turtle_py,\n        aes(sample = \"serum\")) +\n     stat_qq() +\n     stat_qq_line(colour = \"red\") +\n     facet_wrap(\"sex\"))\n\n\n\n\n\n\n\n\n\n\n\nThe results from the Q-Q plots echo what we’ve already seen from the Shapiro-Wilk analyses. The normality of the data in the Male group doesn’t look too bad whereas the those in the Female group looks somewhat dodgy.\nOverall, the assumption of normality of the data doesn’t appear to be very well met at all, but we do have to bear in mind that there are only a few data points in each group and we might just be seeing this pattern in the data due to random chance rather than because the underlying populations are actually not normally distributed. Personally, though I’d edge towards non-normal here.\nHomogeneity of Variance\nIt’s not clear whether the data are normal or not, so it isn’t clear which test to use here. The sensible approach is to do both and hope that they agree (fingers crossed!). Or err on the side of caution and assume they are not normal, but potentially throwing away statistical power (more on that later).\n\nRPython\n\n\nBartlett’s test gives us:\n\n# perform Bartlett's test\nbartlett.test(serum ~ sex,\n              data = turtle)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  serum by sex\nBartlett's K-squared = 0.045377, df = 1, p-value = 0.8313\n\n\nand Levene’s test gives us:\n\n# perform Levene's test\nlevene_test(serum ~ sex,\n              data = turtle)\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     1    11     0.243 0.631\n\n\n\n\nBartlett’s test gives us:\n\npg.homoscedasticity(dv = \"serum\",\n                    group = \"sex\",\n                    method = \"bartlett\",\n                    data = turtle_py)\n\n                 T      pval  equal_var\nbartlett  0.045377  0.831312       True\n\n\nand Levene’s test gives us:\n\npg.homoscedasticity(dv = \"serum\",\n                    group = \"sex\",\n                    method = \"levene\",\n                    data = turtle_py)\n\n               W     pval  equal_var\nlevene  0.243418  0.63145       True\n\n\n\n\n\nThe good news is that both Levene and Bartlett agree that there is homogeneity of variance between the two groups (thank goodness, that’s one less thing to worry about!).\nOverall, what this means is that we’re not too sure about normality, but that homogeneity of variance is pretty good.\n\n\nImplement two-sample t-test\nBecause of the result of the Bartlett test I know that I can carry out a two-sample Student’s t-test. If the variances between the two groups were not equal, then we’d have to perform Welch’s t-test.\n\nRPython\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn the t.test() function the default option for dealing with equality of variance is to assume there there isn’t any. If you look at the help page with ?t.test() then you’ll see that the default for var.equal = FALSE. Here we do assume equality of variance, so we set it to var.equal = TRUE.\n\n\n\n# perform two-sample t-test\nt.test(serum ~ sex,\n       alternative = \"two.sided\",\n       var.equal = TRUE,\n       data = turtle)\n\n\n    Two Sample t-test\n\ndata:  serum by sex\nt = 0.62681, df = 11, p-value = 0.5436\nalternative hypothesis: true difference in means between group Female and group Male is not equal to 0\n95 percent confidence interval:\n -3.575759  6.423378\nsample estimates:\nmean in group Female   mean in group Male \n            225.6667             224.2429 \n\n\n\n\nTo test if the two groups are different from one another, we can use the pg.ttest() function.\nThis function requires the data to be separated into individual groups, so we need to split the serum data by sex. One of the ways we can do this is:\n\nturtle_male = turtle_py.query('sex == \"Male\"')[\"serum\"]\nturtle_female = turtle_py.query('sex == \"Female\"')[\"serum\"]\n\nNext, we use these data to test for differences:\n\npg.ttest(turtle_female, turtle_male,\n                alternative = \"two-sided\",\n                correction = False).transpose()\n\n                    T-test\nT                 0.626811\ndof                     11\nalternative      two-sided\np-val             0.543573\nCI95%        [-3.58, 6.42]\ncohen-d           0.348725\nBF10                 0.519\npower             0.088495\n\n\n\n\n\nWith a p-value of 0.544, this test tells us that there is insufficient evidence to suggest that the means of the two groups are different. A suitable summary sentence would be:\n\nA Student’s two-sample t-test indicated that the mean serum cholesterol level did not differ significantly between male and female turtles (p = 0.544).\n\n\n\nDiscussion\nIn reality, because of the ambiguous normality assumption assessment, for this data set I would actually carry out two different tests; the two-sample t-test with equal variance and the Mann-Whitney U test. If both of them agreed then it wouldn’t matter too much which one I reported (I’d personally report both with a short sentence to say that I’m doing that because it wasn’t clear whether the assumption of normality had or had not been met), but it would be acceptable to report just one.\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.10.1 Turtles (revisited)\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLevel: \nAnalyse the turtle data set from before using a Mann-Whitney U test.\nWe follow the same process as with Student’s t-test.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nHypotheses\n\\(H_0\\) : male median \\(=\\) female median\n\\(H_1\\) : male median \\(\\neq\\) female median\n\n\nSummarise and visualise\nThis is the same as before.\n\n\nAssumptions\nWe’ve already checked that the variances of the two groups are similar, so we’re OK there. Whilst the Mann-Whitney U test doesn’t require normality or symmetry of distributions it does require that the distributions have the same shape. In this example, with just a handful of data points in each group, it’s quite hard to make this call one way or another. My advice in this case would be say that unless it’s obvious that the distributions are very different we can just allow this assumption to pass, and you’re only going see obvious differences in distribution shape when you have considerably more data points than we have here.\n\n\nCarry out a Mann-Whitney U test\n\nRPython\n\n\n\nwilcox.test(serum ~ sex,\n            alternative = \"two.sided\",\n            data = turtle)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  serum by sex\nW = 26, p-value = 0.5338\nalternative hypothesis: true location shift is not equal to 0\n\n\n\n\n\n# reformat the data into a 'wide' format\nturtle_py_wide = pd.pivot(turtle_py,\n                          columns = 'sex',\n                          values = 'serum')\n      \n# have a look at the format\nturtle_py_wide.head()\n\nsex  Female   Male\n0       NaN  220.1\n1       NaN  218.6\n2       NaN  229.6\n3       NaN  228.8\n4       NaN  222.0\n\n\n\n# perform the Mann-Whitney U test\n# ignoring the missing values\npg.mwu(turtle_py_wide['Male'],\n       turtle_py_wide['Female'])\n\n     U-val alternative   p-val       RBC      CLES\nMWU   16.0   two-sided  0.5338  0.238095  0.380952\n\n\n\n\n\nThis gives us exactly the same conclusion that we got from the two-sample t-test i.e. that there isn’t any significant difference between the two groups.\n\nA Mann-Whitney U test indicated that there wasn’t a significant difference in the median serum cholesterol levels between male and female turtles (p = 0.534)",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Two-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_two-samples.html#answer",
    "href": "materials/cs1_practical_two-samples.html#answer",
    "title": "6  Two-sample data",
    "section": "6.10 Answer",
    "text": "6.10 Answer\n\nData\nOnce you’ve loaded the data, have a look at the structure. The data is in a tidy data format, where each variable (the thing that you measure) is in its own column. Each observation has its own row.\n\n\nHypotheses\n\\(H_0\\) : male mean \\(=\\) female mean\n\\(H_1\\) : male mean \\(\\neq\\) female mean\n\n\nLoad, summarise and visualise data\nLet’s load the data and explore our data a bit more before we dive into the statistics.\n\nRPython\n\n\n\n# load the data\nturtle &lt;- read_csv(\"data/CS1-turtle.csv\")\n\n# and have a look\nturtle\n\n# A tibble: 13 × 2\n   serum sex   \n   &lt;dbl&gt; &lt;chr&gt; \n 1  220. Male  \n 2  219. Male  \n 3  230. Male  \n 4  229. Male  \n 5  222  Male  \n 6  224. Male  \n 7  226. Male  \n 8  223. Female\n 9  222. Female\n10  230. Female\n11  224. Female\n12  224. Female\n13  231. Female\n\n\nLet’s summarise the data (although a visualisation is probably much easier to work with):\n\n# create summary statistics for each group\nturtle %&gt;% \n  group_by(sex) %&gt;% \n  get_summary_stats(type = \"common\")\n\n# A tibble: 2 × 11\n  sex    variable     n   min   max median   iqr  mean    sd    se    ci\n  &lt;chr&gt;  &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Female serum        6  222.  231.   224.  5.22  226.  3.87  1.58  4.06\n2 Male   serum        7  219.  230.   224.  6.6   224.  4.26  1.61  3.94\n\n\nand visualise the data:\n\n# visualise the data\nggplot(turtle,\n       aes(x = sex, y = serum)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\nturtle_py = pd.read_csv(\"data/CS1-turtle.csv\")\n\nturtle_py.describe()\n\n            serum\ncount   13.000000\nmean   224.900000\nstd      3.978274\nmin    218.600000\n25%    222.000000\n50%    224.100000\n75%    228.800000\nmax    230.800000\n\n\nand visualise the data:\n\n(ggplot(turtle_py,\n        aes(x = \"sex\", y = \"serum\")) +\n     geom_boxplot())\n\n\n\n\n\n\n\n\n\n\n\nAs always we use the plot and summary to assess three things:\n\nDoes it look like we’ve loaded the data in correctly?\n\nWe have two groups and the extreme values of our plots seem to match with our data set, so I’m happy that we haven’t done anything massively wrong here.\n\nDo we think that there is a difference between the two groups?\n\nWe need the result of the formal test to make sense given the data, so it’s important to develop a sense of what we think is going to happen here. Whilst the ranges of the two groups suggests that the Female serum levels might be higher than the males when we look at things more closely we realise that isn’t the case. The box plot shows that the median values of the two groups is virtually identical and this is backed up by the summary statistics we calculated: the medians are both about 224.1, and the means are fairly close too (225.7 vs 224.2). Based on this, and the fact that there are only 13 observations in total I would be very surprised if any test came back showing that there was a difference between the groups.\n\nWhat do we think about assumptions?\n\nNormality looks a bit worrying: whilst the Male group appears nice and symmetric (and so might be normal), the Female group appears to be quite skewed (since the median is much closer to the bottom than the top). We’ll have to look carefully at the more formal checks to decided whether we think the data are normal enough for us to use a t-test.\nHomogeneity of variance. At this stage the spread of the data within each group looks similar, but because of the potential skew in the Female group we’ll again want to check the assumptions carefully.\n\n\n\n\nAssumptions\nNormality\nLet’s look at the normality of each of the groups separately. There are several ways of getting at the serum values for Male and Female groups separately. All of them come down to splitting the data. Afterwards we use the Shapiro-Wilk (‘formal’ test), followed by Q-Q plots (much more informative).\n\nRPython\n\n\n\n# perform Shapiro-Wilk test on each group\nturtle %&gt;% \n    filter(sex == \"Female\") %&gt;% \n    pull(serum) %&gt;% \n    shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.84178, p-value = 0.1349\n\nturtle %&gt;% \n    filter(sex == \"Male\") %&gt;% \n    pull(serum) %&gt;% \n    shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.94392, p-value = 0.6743\n\n\n\n\n\npg.normality(dv = \"serum\",\n             group = \"sex\",\n             data = turtle_py)\n\n               W      pval  normal\nMale    0.943924  0.674277    True\nFemale  0.841784  0.134869    True\n\n\n\n\n\nThe p-values for both Shapiro-Wilk tests are non-significant which suggests that the data are normal enough. This is a bit surprising given what we saw in the box plot but there are two bits of information that we can use to reassure us.\n\nThe p-value for the Female group is smaller than for the Male group (suggesting that the Female group is closer to being non-normal than the Male group) which makes sense based on our visual observations.\nThe Shapiro-Wilk test is generally quite relaxed about normality for small sample sizes (and notoriously strict for very large sample sizes). For a group with only 6 data points in it, the data would actually have to have a really, really skewed distribution. Given that the Female group only has 6 data points in it, it’s not too surprising that the Shapiro-Wilk test came back saying everything is OK.\n\nGiven these caveats of the Shapiro-Wilk test (I’ll stop mentioning them now, I think I’ve made my opinion clear ;)), let’s look at the Q-Q plots.\n\nRPython\n\n\n\n# create Q-Q plots for both groups\nggplot(turtle,\n       aes(sample = serum)) +\n  stat_qq() +\n  stat_qq_line(colour = \"blue\") +\n  facet_wrap(facets = vars(sex))\n\n\n\n\n\n\n\n\n\n\n\n# create Q-Q plots for both groups\n(ggplot(turtle_py,\n        aes(sample = \"serum\")) +\n     stat_qq() +\n     stat_qq_line(colour = \"red\") +\n     facet_wrap(\"sex\"))\n\n\n\n\n\n\n\n\n\n\n\nThe results from the Q-Q plots echo what we’ve already seen from the Shapiro-Wilk analyses. The normality of the data in the Male group doesn’t look too bad whereas the those in the Female group looks somewhat dodgy.\nOverall, the assumption of normality of the data doesn’t appear to be very well met at all, but we do have to bear in mind that there are only a few data points in each group and we might just be seeing this pattern in the data due to random chance rather than because the underlying populations are actually not normally distributed. Personally, though I’d edge towards non-normal here.\nHomogeneity of Variance\nIt’s not clear whether the data are normal or not, so it isn’t clear which test to use here. The sensible approach is to do both and hope that they agree (fingers crossed!). Or err on the side of caution and assume they are not normal, but potentially throwing away statistical power (more on that later).\n\nRPython\n\n\nBartlett’s test gives us:\n\n# perform Bartlett's test\nbartlett.test(serum ~ sex,\n              data = turtle)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  serum by sex\nBartlett's K-squared = 0.045377, df = 1, p-value = 0.8313\n\n\nand Levene’s test gives us:\n\n# perform Levene's test\nlevene_test(serum ~ sex,\n              data = turtle)\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     1    11     0.243 0.631\n\n\n\n\nBartlett’s test gives us:\n\npg.homoscedasticity(dv = \"serum\",\n                    group = \"sex\",\n                    method = \"bartlett\",\n                    data = turtle_py)\n\n                 T      pval  equal_var\nbartlett  0.045377  0.831312       True\n\n\nand Levene’s test gives us:\n\npg.homoscedasticity(dv = \"serum\",\n                    group = \"sex\",\n                    method = \"levene\",\n                    data = turtle_py)\n\n               W     pval  equal_var\nlevene  0.243418  0.63145       True\n\n\n\n\n\nThe good news is that both Levene and Bartlett agree that there is homogeneity of variance between the two groups (thank goodness, that’s one less thing to worry about!).\nOverall, what this means is that we’re not too sure about normality, but that homogeneity of variance is pretty good.\n\n\nImplement two-sample t-test\nBecause of the result of the Bartlett test I know that I can carry out a two-sample Student’s t-test. If the variances between the two groups were not equal, then we’d have to perform Welch’s t-test.\n\nRPython\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn the t.test() function the default option for dealing with equality of variance is to assume there there isn’t any. If you look at the help page with ?t.test() then you’ll see that the default for var.equal = FALSE. Here we do assume equality of variance, so we set it to var.equal = TRUE.\n\n\n\n# perform two-sample t-test\nt.test(serum ~ sex,\n       alternative = \"two.sided\",\n       var.equal = TRUE,\n       data = turtle)\n\n\n    Two Sample t-test\n\ndata:  serum by sex\nt = 0.62681, df = 11, p-value = 0.5436\nalternative hypothesis: true difference in means between group Female and group Male is not equal to 0\n95 percent confidence interval:\n -3.575759  6.423378\nsample estimates:\nmean in group Female   mean in group Male \n            225.6667             224.2429 \n\n\n\n\nTo test if the two groups are different from one another, we can use the pg.ttest() function.\nThis function requires the data to be separated into individual groups, so we need to split the serum data by sex. One of the ways we can do this is:\n\nturtle_male = turtle_py.query('sex == \"Male\"')[\"serum\"]\nturtle_female = turtle_py.query('sex == \"Female\"')[\"serum\"]\n\nNext, we use these data to test for differences:\n\npg.ttest(turtle_female, turtle_male,\n                alternative = \"two-sided\",\n                correction = False).transpose()\n\n                    T-test\nT                 0.626811\ndof                     11\nalternative      two-sided\np-val             0.543573\nCI95%        [-3.58, 6.42]\ncohen-d           0.348725\nBF10                 0.519\npower             0.088495\n\n\n\n\n\nWith a p-value of 0.544, this test tells us that there is insufficient evidence to suggest that the means of the two groups are different. A suitable summary sentence would be:\n\nA Student’s two-sample t-test indicated that the mean serum cholesterol level did not differ significantly between male and female turtles (p = 0.544).\n\n\n\nDiscussion\nIn reality, because of the ambiguous normality assumption assessment, for this data set I would actually carry out two different tests; the two-sample t-test with equal variance and the Mann-Whitney U test. If both of them agreed then it wouldn’t matter too much which one I reported (I’d personally report both with a short sentence to say that I’m doing that because it wasn’t clear whether the assumption of normality had or had not been met), but it would be acceptable to report just one.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Two-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_two-samples.html#summary",
    "href": "materials/cs1_practical_two-samples.html#summary",
    "title": "6  Two-sample data",
    "section": "6.11 Summary",
    "text": "6.11 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nStudent’s t tests are used when you have two samples of continuous data, which are normally distributed, independent of each other and have equal variance\nA good way of assessing the assumption of normality is by checking the data against a Q-Q plot\nWe can check equality of variance (homoscedasticity) with Bartlett’s (normal data) or Levene’s (non-normal data) test\nThe Mann-Whitney U test is used when you have two samples of continuous data, which are not normally distributed, but are independent of each other, have equal variance and similar distributional shape",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Two-sample data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_two-samples-paired.html",
    "href": "materials/cs1_practical_two-samples-paired.html",
    "title": "7  Paired data",
    "section": "",
    "text": "7.1 Libraries and functions",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Paired data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_two-samples-paired.html#libraries-and-functions",
    "href": "materials/cs1_practical_two-samples-paired.html#libraries-and-functions",
    "title": "7  Paired data",
    "section": "",
    "text": "Click to expand\n\n\n\n\n\n\nRPython\n\n\n\n7.1.1 Libraries\n\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\n\n\n7.1.2 Functions\n\n# Performs a one-sample t-test, Student's t-test\n# and Welch's t-test in later sections\nstats::t.test()\n\n# Performs a Shapiro-Wilk test for normality\nstats::shapiro.test()\n\n# Performs one and two sample Wilcoxon tests\nstats::wilcox.test()\n\n# Plots a Q-Q plot for comparison with a normal distribution\nggplot2::stat_qq()\n\n# Adds a comparison line to the Q-Q plot\nggplot2::stat_qq_line()\n\n# Plots jittered points by adding a small amount of random\n# variation to each point, to handle overplotting\nggplot2::geom_jitter()\n\n# Computes summary statistics                         \nrstatix::get_summary_stats() \n\n# \"Widens\" the data, increasing the number of columns\ntidyr::pivot_wider()\n\n\n\n\n\n\n\n\n\n\n\nLibraries\nDescription\n\n\n\n\nplotnine\nThe Python equivalent of ggplot2.\n\n\npandas\nA Python data analysis and manipulation tool.\n\n\npingouin\nA Python module developed to have simple yet exhaustive stats functions\n\n\n\n\n\n\nFunctions\nDescription\n\n\n\n\npandas.DataFrame.read_csv\nReads in a .csv file\n\n\npandas.DataFrame.pivot()\nReturn reshaped DataFrame organised by given index / column values.\n\n\npingouin.normality()\nPerforms the Shapiro-Wilk test for normality.\n\n\npingouin.ttest()\nPerforms a t-test\n\n\nplotnine.stats.stat_qq()\nPlots a Q-Q plot for comparison with a normal distribution.\n\n\nplotnine.stats.stat_qq_line()\nAdds a comparison line to the Q-Q plot.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Paired data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_two-samples-paired.html#data-and-hypotheses",
    "href": "materials/cs1_practical_two-samples-paired.html#data-and-hypotheses",
    "title": "7  Paired data",
    "section": "7.2 Data and hypotheses",
    "text": "7.2 Data and hypotheses\nFor example, suppose we measure the cortisol levels in 20 adult females (nmol/l) first thing in the morning and again in the evening. We want to test whether the cortisol levels differs between the two measurement times. We will initially form the following null and alternative hypotheses:\n\n\\(H_0\\): There is no difference in cortisol level between times (\\(\\mu M = \\mu E\\))\n\\(H_1\\): There is a difference in cortisol levels between times (\\(\\mu M \\neq \\mu E\\))\n\nWe use a two-sample, two-tailed paired t-test to see if we can reject the null hypothesis.\n\nWe use a two-sample test because we now have two samples\nWe use a two-tailed t-test because we want to know if our data suggest that the true (population) means are different from one another rather than that one mean is specifically bigger or smaller than the other\nWe use a paired test because each data point in the first sample can be linked to another data point in the second sample by a connecting factor\nWe’re using a t-test because we’re assuming that the parent populations are normal and have equal variance (We’ll check this in a bit)\n\nThe data are stored in a tidy format in the file data/CS1-twopaired.csv.\n\nRPython\n\n\n\n# load the data\ncortisol &lt;- read_csv(\"data/CS1-twopaired.csv\")\n\nRows: 40 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): time\ndbl (2): patient_id, cortisol\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# have a look at the data\ncortisol\n\n# A tibble: 40 × 3\n   patient_id time    cortisol\n        &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1          1 morning     311.\n 2          2 morning     146.\n 3          3 morning     297 \n 4          4 morning     271.\n 5          5 morning     268.\n 6          6 morning     264.\n 7          7 morning     358.\n 8          8 morning     316.\n 9          9 morning     336.\n10         10 morning     221.\n# ℹ 30 more rows\n\n\n\n\n\n# load the data\ncortisol_py = pd.read_csv('data/CS1-twopaired.csv')\n\n# inspect the data\ncortisol_py.head()\n\n   patient_id     time  cortisol\n0           1  morning     310.6\n1           2  morning     146.1\n2           3  morning     297.0\n3           4  morning     270.9\n4           5  morning     267.5\n\n\n\n\n\nWe can see that the data frame consists of three columns:\n\npatient_id, a unique ID for each patient\ntime when the cortisol level was measured\ncortisol, which contains the measured value.\n\nFor each patient_id there are two measurements: one in the morning and one in the afternoon.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Paired data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_two-samples-paired.html#summarise-and-visualise",
    "href": "materials/cs1_practical_two-samples-paired.html#summarise-and-visualise",
    "title": "7  Paired data",
    "section": "7.3 Summarise and visualise",
    "text": "7.3 Summarise and visualise\nIt’s always a good idea to visualise your data, so let’s do that.\n\nRPython\n\n\n\n# create a boxplot\nggplot(cortisol,\n       aes(x = time, y = cortisol)) +\n  geom_boxplot() +\n  geom_jitter(width = 0.05) +\n  ylab(\"Cortisol level (nmol/l)\")\n\n\n\n\n\n\n\n\nHere we use also visualise the actual data points, to get a sense of how these data are spread out. To avoid overlapping the data points (try using geom_point() instead of geom_jitter()), we jitter the data points. What geom_jitter() does is add a small amount of variation to each point.\n\n\n\n(ggplot(cortisol_py,\n        aes(x = \"time\",\n            y = \"cortisol\")) +\n     geom_boxplot() +\n     geom_jitter(width = 0.05) +\n     ylab(\"Cortisol level (nmol/l)\"))\n\n\n\n\n\n\n\n\n\n\n\nHowever, this plot does not capture how the cortisol level of each individual subject has changed though. We can explore the individual changes between morning and evening by looking at the differences between the two times of measurement for each patient.\nTo do this, we need to put our data into a wide format, so we can calculate the change in cortisol level for each patient.\n\nRPython\n\n\nIn tidyverse we can use the pivot_wider() function.\n\n# calculate the difference between evening and morning values\ncortisol_diff &lt;- cortisol %&gt;%\n  pivot_wider(id_cols = patient_id,\n              names_from = time,\n              values_from = cortisol) %&gt;% \n  mutate(cortisol_change = evening - morning)\n\ncortisol_diff\n\n# A tibble: 20 × 4\n   patient_id morning evening cortisol_change\n        &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;           &lt;dbl&gt;\n 1          1    311.   273.            -37.4\n 2          2    146.    65.7           -80.4\n 3          3    297    257.            -40.4\n 4          4    271.   321              50.1\n 5          5    268.    80.3          -187. \n 6          6    264.   379.            116. \n 7          7    358.   163.           -195. \n 8          8    316.   294.            -22  \n 9          9    336.   140.           -196. \n10         10    221.   231.             10.4\n11         11    366    131.           -235. \n12         12    256.   114.           -142. \n13         13    432.   217.           -215. \n14         14    208.    60.1          -148. \n15         15    324.   199.           -125. \n16         16    388.   170.           -218. \n17         17    332    160.           -172. \n18         18    414.   179.           -235. \n19         19    405.   286            -119. \n20         20    356.   226.           -130. \n\n\nThere are three arguments in pivot_wider():\n\nid_cols = patient_id tells it that each observational unit is determined by patient_id\nnames_from = time says that there will be new columns, with names from the time column (in this case, there are two values in there, morning and evening)\nvalues_from = cortisol populates the new columns with the values coming from the cortisol column\n\nLastly, we create a new column cortisol_change that contains the difference between the evening and morning measurements.\nAfter this we can plot our data:\n\n# plot the data\nggplot(cortisol_diff,\n       aes(y = cortisol_change)) +\n  geom_boxplot() +\n  ylab(\"Change in cortisol (nmol/l)\")\n\n\n\n\n\n\n\n\nThe differences in cortisol levels appear to be very much less than zero, meaning that the evening cortisol levels appear to be much lower than the morning ones. As such we would expect that the test would give a pretty significant result.\nAn alternative representation would be to plot the data points for both evening and morning and connect them by patient:\n\n# plot cortisol levels by patient\nggplot(cortisol,\n       aes(x = time,\n           y = cortisol,\n           group = patient_id)) +\n  geom_point() +\n  geom_line()\n\n\n\n\n\n\n\n\nThis gives a similar picture to what the boxplot was telling us, that for most patients the cortisol levels are higher in the morning than in the evening.\n\n\n\n# reformat the data into a 'wide' format\ncortisol_diff_py = pd.pivot(cortisol_py, index = \"patient_id\", columns = \"time\", values = \"cortisol\")\n\n# add a new column with difference between\n# evening and morning cortisol levels\ncortisol_diff_py[\"cortisol_change\"] = cortisol_diff_py[\"evening\"].subtract(cortisol_diff_py[\"morning\"])\n      \n# have a look at the format\ncortisol_diff_py.head()\n\ntime        evening  morning  cortisol_change\npatient_id                                   \n1             273.2    310.6            -37.4\n2              65.7    146.1            -80.4\n3             256.6    297.0            -40.4\n4             321.0    270.9             50.1\n5              80.3    267.5           -187.2\n\n\nAfter this we can plot our data:\n\n# plot the data\n(ggplot(cortisol_diff_py,\n        aes(x = \"1\",\n            y = \"cortisol_change\")) +\n     geom_boxplot() +\n     ylab(\"Change in cortisol (nmol/l)\"))\n\n\n\n\n\n\n\n\nThe differences in cortisol levels appear to be very much less than zero, meaning that the evening cortisol levels appear to be much lower than the morning ones. As such we would expect that the test would give a pretty significant result.\nAn alternative representation would be to plot the data points for both evening and morning and connect them by patient:\n\n# plot cortisol levels by patient\n(ggplot(cortisol_py,\n        aes(x = \"time\",\n            y = \"cortisol\",\n            group = \"patient_id\")) +\n     geom_point() +\n     geom_line())\n\n\n\n\n\n\n\n\nThis gives a similar picture to what the boxplot was telling us, that for most patients the cortisol levels are higher in the morning than in the evening.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Paired data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_two-samples-paired.html#assumptions",
    "href": "materials/cs1_practical_two-samples-paired.html#assumptions",
    "title": "7  Paired data",
    "section": "7.4 Assumptions",
    "text": "7.4 Assumptions\nYou will do this in the exercise!",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Paired data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_two-samples-paired.html#implement-and-interpret-the-test",
    "href": "materials/cs1_practical_two-samples-paired.html#implement-and-interpret-the-test",
    "title": "7  Paired data",
    "section": "7.5 Implement and interpret the test",
    "text": "7.5 Implement and interpret the test\nPerform a two-sample, two-tailed, paired t-test:\n\nRPython\n\n\n\n# perform the test\nt.test(cortisol ~ time,\n       alternative = \"two.sided\",\n       paired = TRUE,\n       data = cortisol)\n\n\nThe first two arguments define the formula\nThe third argument gives the type of alternative hypothesis and must be one of two.sided, greater or less\nThe fourth argument says that the data are paired\n\nFrom our perspective the value of interest is the p-value.\n\n\nTo perform a paired t-test we can use the same pg.ttest() as before, but set the argument paired = True.\nAnnoyingly, the output is not entirely visible because the data frame is too wide. To deal with that, we can simply transpose it with transpose()\n\npg.ttest(cortisol_diff_py[\"evening\"],\n         cortisol_diff_py[\"morning\"],\n         alternative = \"two-sided\",\n         paired = True).transpose()\n\n                        T-test\nT                     -5.18329\ndof                         19\nalternative          two-sided\np-val                 0.000053\nCI95%        [-162.96, -69.21]\ncohen-d               1.434359\nBF10                   491.599\npower                  0.99998\n\n\nFrom our perspective the value of interest is the p-val.\n\n\n\nSince the p-value = 5.29 \\(\\times\\) 10-5) and thus substantially less than 0.05 we can reject the null hypothesis and state:\n\nA two-tailed, paired t-test indicated that the average cortisol level in adult females differed significantly between the morning (313.5 nmol/l) and the evening (197.4 nmol/l, p = 5.3 * 10-5).",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Paired data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_two-samples-paired.html#dealing-with-non-normal-data",
    "href": "materials/cs1_practical_two-samples-paired.html#dealing-with-non-normal-data",
    "title": "7  Paired data",
    "section": "7.6 Dealing with non-normal data",
    "text": "7.6 Dealing with non-normal data\nThe example above assumes that the paired data come from parent distributions that are normal. As we’ve seen before, we may have data where we can’t rely on that assumption. Fortunately, there is very little that we need to change in our approach if we want to analyse paired data that violate the assumption of normality.\nAs with the non-normal two-sample data, there is the underlying assumption that the parent distributions of the samples are comparable in shape and variance.\n\n7.6.1 Data and hypotheses\nUsing the cortisol data from before we form the following null and alternative hypotheses:\n\n\\(H_0\\): The median of the difference in cortisol levels between the two groups is 0 \\((\\mu M = \\mu E)\\)\n\\(H_1\\): The median of the difference in cortisol levels between the two groups is not 0 \\((\\mu M \\neq \\mu E)\\)\n\nWe use a two-tailed Wilcoxon signed rank test to see if we can reject the null hypothesis.\n\n\n7.6.2 Summarise and visualise\nAlready implemented previously.\n\n\n7.6.3 Assumptions\nThese have been checked previously.\n\n\n7.6.4 Implement and interpret the test\nPerform a two-tailed, Wilcoxon signed rank test:\n\nRPython\n\n\n\nwilcox.test(cortisol ~ time,\n            alternative = \"two.sided\",\n            paired = TRUE,\n            data = cortisol)\n\n\n    Wilcoxon signed rank exact test\n\ndata:  cortisol by time\nV = 13, p-value = 0.0001678\nalternative hypothesis: true location shift is not equal to 0\n\n\n\nThe first argument gives the formula\nThe second argument gives the type of alternative hypothesis and must be one of two.sided, greater or less\nThe third argument indicates that the test is paired\nThe last argument is the data set\n\n\n\nWe’ll use the wide format data set that we created previously:\n\npg.wilcoxon(x = cortisol_diff_py[\"evening\"],\n            y = cortisol_diff_py[\"morning\"],\n            alternative = \"two-sided\",\n            correction = True)\n\n          W-val alternative     p-val      RBC  CLES\nWilcoxon   13.0   two-sided  0.000168 -0.87619  0.16\n\n\n\n\n\nThe p-value is given in the p column (p-value = 0.000168). Given that this is less than 0.05 we can still reject the null hypothesis.\n\nA two-tailed, Wilcoxon signed rank test indicated that the median cortisol level in adult females differed significantly between the morning (320.5 nmol/l) and the evening (188.9 nmol/l, p = 0.00017).",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Paired data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_two-samples-paired.html#exercises",
    "href": "materials/cs1_practical_two-samples-paired.html#exercises",
    "title": "7  Paired data",
    "section": "7.7 Exercises",
    "text": "7.7 Exercises\n\n7.7.1 Cortisol levels\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLevel: \nCheck the assumptions necessary for this this paired t-test. Was a paired t-test an appropriate test?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe actually don’t care too much about the distributions of the individual groups. Instead we care about the properties of the differences. So for a paired t-test to be valid for this data set, we need the differences between the morning and evening values to be normally distributed.\nLet’s check this with the Shapiro-Wilk test and Q-Q plots, using the wide data frames we created earlier.\n\nRPython\n\n\nPerform Shapiro-Wilk test:\n\n# perform Shapiro-Wilk test on cortisol differences\nshapiro.test(cortisol_diff$cortisol_change)\n\n\n    Shapiro-Wilk normality test\n\ndata:  cortisol_diff$cortisol_change\nW = 0.92362, p-value = 0.1164\n\n\nCreate Q-Q plot:\n\n# create the Q-Q plot\nggplot(cortisol_diff,\n       aes(sample = cortisol_change)) +\n  stat_qq() +\n  stat_qq_line(colour = \"blue\")\n\n\n\n\n\n\n\n\n\n\nPerform Shapiro-Wilk test:\n\n# perform Shapiro-Wilk test on cortisol differences\npg.normality(cortisol_diff_py[\"cortisol_change\"])\n\n                        W      pval  normal\ncortisol_change  0.923622  0.116355    True\n\n\nCreate Q-Q plot:\n\n# create the Q-Q plot\n(ggplot(cortisol_diff_py,\n        aes(sample = \"cortisol_change\")) +\n     stat_qq() +\n     stat_qq_line(colour = \"red\"))\n\n\n\n\n\n\n\n\n\n\n\nThe Shapiro-Wilk test says that the data are normal enough and whilst the Q-Q plot is mostly fine, there is some suggestion of snaking at the bottom left. I’m actually OK with this because the suggestion of snaking is actually only due to a single point (the last point on the left). If you cover that point up with your thumb (or finger of your choice) then the remaining points in the Q-Q plot look pretty darn good, and so the suggestion of snaking is actually driven by only a single point (which can happen by chance). As such I’m happy that the assumption of normality is well-met in this case. This single point check is a useful thing to remember when assessing diagnostic plots.\nSo, yep, a paired t-test is appropriate for this data set.\n\n\n\n\n\n\n\n\n\n\n7.7.2 Deer legs\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLevel: \nUsing the following data on deer legs (yes, really!), test the null hypothesis that the fore and hind legs of the deer in this data set are the same length.\n\n\n# A tibble: 10 × 2\n   hindleg foreleg\n     &lt;dbl&gt;   &lt;dbl&gt;\n 1     142     138\n 2     140     136\n 3     144     147\n 4     144     139\n 5     142     143\n 6     146     141\n 7     149     143\n 8     150     145\n 9     142     136\n10     148     146\n\n\nDo these results provide any evidence to suggest that fore- and hind-leg length differ in deer?\n\nWrite down the null and alternative hypotheses\nImport the data from data/CS1-deer.csv\nSummarise and visualise the data\nCheck your assumptions (normality and variance) using appropriate tests\nDiscuss with your (virtual) neighbour which test is most appropriate?\nPerform the test\nWrite down a sentence that summarises the results that you have found\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nHypotheses\n\\(H_0\\) : foreleg average (mean or median) \\(=\\) hindleg average (mean or median)\n\\(H_1\\) : foreleg average \\(\\neq\\) hindleg average\n\n\nImport data, summarise and visualise\nFirst of all, we need to load in the data.\n\nRPython\n\n\n\n# load the data\ndeer &lt;- read_csv(\"data/CS1-deer.csv\")\n\n# have a look\ndeer\n\n# A tibble: 20 × 3\n      id leg     length\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n 1     1 hindleg    142\n 2     2 hindleg    140\n 3     3 hindleg    144\n 4     4 hindleg    144\n 5     5 hindleg    142\n 6     6 hindleg    146\n 7     7 hindleg    149\n 8     8 hindleg    150\n 9     9 hindleg    142\n10    10 hindleg    148\n11     1 foreleg    138\n12     2 foreleg    136\n13     3 foreleg    147\n14     4 foreleg    139\n15     5 foreleg    143\n16     6 foreleg    141\n17     7 foreleg    143\n18     8 foreleg    145\n19     9 foreleg    136\n20    10 foreleg    146\n\n\nThe ordering of the data is important here; the first hind leg row corresponds to the first fore leg row, the second to the second and so on. To indicate this we use an id column, where each observation has a unique ID.\nLet’s look at the data and see what it tells us.\n\n# summarise the data\nsummary(deer)\n\n       id           leg                length     \n Min.   : 1.0   Length:20          Min.   :136.0  \n 1st Qu.: 3.0   Class :character   1st Qu.:140.8  \n Median : 5.5   Mode  :character   Median :143.0  \n Mean   : 5.5                      Mean   :143.1  \n 3rd Qu.: 8.0                      3rd Qu.:146.0  \n Max.   :10.0                      Max.   :150.0  \n\n\nWe can also summarise some of the main summary statistics for each type of leg. We don’t need summary statistics for the id column, so we unselect it with select(-id).\nTo make life easy we use the get_summary_stats() function from the rstatix package. Have a look at the help function to see what kind of summary statistics it can produce. In this case I’m using the type = \"common\" option to specify that I want to find commonly used statistics (e.g. sample number, min, max, median, mean etc.)\n\n# or even summarise by leg type\ndeer %&gt;% \n  select(-id) %&gt;% \n  group_by(leg) %&gt;% \n  get_summary_stats(type = \"common\")\n\n# A tibble: 2 × 11\n  leg     variable     n   min   max median   iqr  mean    sd    se    ci\n  &lt;chr&gt;   &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 foreleg length      10   136   147    142  6.25  141.  4.03  1.27  2.88\n2 hindleg length      10   140   150    144  5.5   145.  3.40  1.08  2.43\n\n\nVisualising the data is often more useful:\n\n# we can also visualise the data\nggplot(deer,\n       aes(x = leg, y = length)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\nAll of this suggests that there might be a difference between the legs, with hind legs being longer than forelegs. However, this representation obscures the fact that we have paired data. What we really need to look at is the difference in leg length for each observation:\n\n# create a data set that contains the difference in leg length\nleg_diff &lt;- deer %&gt;% \n  pivot_wider(id_cols = id,\n              names_from = leg,\n              values_from = length) %&gt;% \n  mutate(leg_diff = hindleg - foreleg)\n\n\n# plot the difference in leg length\nggplot(leg_diff,\n       aes(y = leg_diff)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nAdditionally, we can also plot the data by observation:\n\n# plot the data by observation\nggplot(deer,\n       aes(x = leg, y = length, group = id)) +\n  geom_point() +\n  geom_line()\n\n\n\n\n\n\n\n\n\n\n\n# load the data\ndeer_py = pd.read_csv(\"data/CS1-deer.csv\")\n\n# have a look\ndeer_py.head()\n\n   id      leg  length\n0   1  hindleg     142\n1   2  hindleg     140\n2   3  hindleg     144\n3   4  hindleg     144\n4   5  hindleg     142\n\n\nThe ordering of the data is important here; the first hind leg row corresponds to the first fore leg row, the second to the second and so on. To indicate this we use an id column, where each observation has a unique ID.\nLet’s look at the data and see what we can see.\n\n# summarise the data\ndeer_py.describe()\n\n              id      length\ncount  20.000000   20.000000\nmean    5.500000  143.050000\nstd     2.946898    4.006245\nmin     1.000000  136.000000\n25%     3.000000  140.750000\n50%     5.500000  143.000000\n75%     8.000000  146.000000\nmax    10.000000  150.000000\n\n\nWe can also summarise by leg type:\n\ndeer_py.groupby(\"leg\")[\"length\"].describe()\n\n         count   mean       std    min     25%    50%    75%    max\nleg                                                                \nforeleg   10.0  141.4  4.033196  136.0  138.25  142.0  144.5  147.0\nhindleg   10.0  144.7  3.400980  140.0  142.00  144.0  147.5  150.0\n\n\nIt might be more helpful to look at the difference in leg length. In order to calculate that, we need to reformat our data into a ‘wide’ format first:\n\n# reformat the data into a 'wide' format\nleg_diff_py = pd.pivot(deer_py,\n                       index = \"id\",\n                       columns = \"leg\",\n                       values = \"length\")\n\n# have a look at the format\nleg_diff_py.head()\n\nleg  foreleg  hindleg\nid                   \n1        138      142\n2        136      140\n3        147      144\n4        139      144\n5        143      142\n\n\nNext, we can add a new column leg_diff that contains the leg difference:\n\n# add a new column with difference between\n# hind and fore leg length\nleg_diff_py[\"leg_diff\"] = leg_diff_py[\"hindleg\"].subtract(leg_diff_py[\"foreleg\"])\n \n\nFinally, we can visualise this:\n\n# we can also visualise the data\n(ggplot(leg_diff_py,\n        aes(x = \"1\",\n            y = \"leg_diff\")) +\n     geom_boxplot())\n\n\n\n\n\n\n\n\nAll of this suggests that there might be a difference between the legs, with hind legs being longer than forelegs. However, this representation obscures the fact that we have paired data. What we really need to look at is the difference in leg length for each observation:\n\n# plot paired observations\n(ggplot(deer_py,\n        aes(x = \"leg\",\n            y = \"length\",\n            group = \"id\")) +\n     geom_point() +\n     geom_line())\n\n\n\n\n\n\n\n\n\n\n\nAll of this gives us a much clearer picture. It looks as though the hindlegs are about 4 cm longer than the forelegs, on average. It also suggests that our leg differences might not be normally distributed (the data look a bit skewed in the boxplot).\n\n\nAssumptions\nWe need to consider the distribution of the difference in leg lengths rather than the individual distributions.\n\nRPython\n\n\nShapiro-Wilk test:\n\n# perform Shapiro-Wilk test on leg differences\nshapiro.test(leg_diff$leg_diff)\n\n\n    Shapiro-Wilk normality test\n\ndata:  leg_diff$leg_diff\nW = 0.81366, p-value = 0.02123\n\n\nQ-Q plot:\n\n# create a Q-Q plot\nggplot(leg_diff,\n       aes(sample = leg_diff)) +\n  stat_qq() +\n  stat_qq_line(colour = \"blue\")\n\n\n\n\n\n\n\n\n\n\nShapiro-Wilk test:\n\n# perform Shapiro-Wilk test on leg length differences\npg.normality(leg_diff_py[\"leg_diff\"])\n\n                 W      pval  normal\nleg_diff  0.813656  0.021235   False\n\n\nCreate the Q-Q plot:\n\n# create the Q-Q plot\n(ggplot(leg_diff_py,\n        aes(sample = \"leg_diff\")) +\n     stat_qq() +\n     stat_qq_line(colour = \"red\"))\n\n\n\n\n\n\n\n\n\n\n\nBoth our Shapiro-Wilk test and our Q-Q plot suggest that the difference data aren’t normally distributed, which rules out a paired t-test. We should therefore consider a paired Wilcoxon signed rank test next. Remember that this test requires that the distribution of differences be of a similar shape, whereas our box plot from before suggested that the data were very much skewed.\nThis means that we’re not able to perform a paired Wilcoxon signed rank test either!\n\n\nConclusions\nSo, frustratingly, neither of the tests at our disposal are appropriate for this data set. The differences in fore leg and hind leg lengths are neither normal enough for a paired t-test nor are they symmetric enough for a Wilcoxon signed rank test. We also don’t have enough data to just use the t-test (we’d need more than 30 points or so). So what do we do in this situation? Well, the answer is that there aren’t actually any traditional statistical tests that are valid for this data set as it stands!\nThere are two options available to someone:\n\ntry transforming the raw data (take logs, square root, reciprocals) and hope that one of them leads to a modified data set that satisfies the assumptions of one of the tests we’ve covered, or\nuse a permutation test approach (which would work but is beyond the scope of this course).\n\nThe reason I included this example in the first practical is purely to illustrate how a very simple data set with an apparently clear message (leg lengths differ within deer) can be intractable. You don’t need to have very complex data sets before you go beyond the capabilities of classical statistics.\nAs Jeremy Clarkson would put it:\n\nAnd on that bombshell, it’s time to end. Goodnight!",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Paired data</span>"
    ]
  },
  {
    "objectID": "materials/cs1_practical_two-samples-paired.html#summary",
    "href": "materials/cs1_practical_two-samples-paired.html#summary",
    "title": "7  Paired data",
    "section": "7.8 Summary",
    "text": "7.8 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nPaired t-tests are used when you have two paired samples of continuous data, which are normally distributed and have equal variance\nA good way of assessing the assumption of normality is by checking the data against a Q-Q plot\nWe can check equality of variance (homoscedasticity) with Bartlett’s (normal data) or Levene’s (non-normal data) test\nThe Wilcoxon signed rank test is used when you have two paired samples of continuous data, which are not normally distributed (but have comparable distributional shapes), and have equal variance.",
    "crumbs": [
      "Statistical inference",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Paired data</span>"
    ]
  },
  {
    "objectID": "materials/variables-and-data-types.html",
    "href": "materials/variables-and-data-types.html",
    "title": "8  Variables and data types",
    "section": "",
    "text": "8.1 Categorical vs continuous variables\nSo far in the materials you’ve worked through, you’ve actually encountered both categorical and continuous variables.\nTake the fish length length ~ river from the section on two-sample data. The river variable is an example of a categorical variable. There are two discrete options, Guanapo or Aripo, with nothing in between, and a fish can only come from one of the two rivers. A categorical variable, then, is one that only has a discrete set of outcomes. There can be as many categories as we like (within reason) so long as it is some finite number, and each observation must belong to just one of those categories.\nMeanwhile, fishlength variable is a continuous variable - the exact length of each fish can take any value along a range (with some sensible/realistic maximum and minimum, in this case). In theory, there an infinite number of exact lengths that a fish can be, although of course a human observer won’t have a tool that can measure beyond a certain length of precision.\nThis coarse distinction between categorical and continuous variables is pretty important for the course. But for those who want a bit of extra information about variable types, we can actually be a little more detailed.",
    "crumbs": [
      "Variables & distributions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Variables and data types</span>"
    ]
  },
  {
    "objectID": "materials/variables-and-data-types.html#categorical-vs-continuous-variables",
    "href": "materials/variables-and-data-types.html#categorical-vs-continuous-variables",
    "title": "8  Variables and data types",
    "section": "",
    "text": "Nominal, ordinal, interval and ratio data\nAnother set of descriptors you’ll sometimes see applied to variables is nominal, ordinal, interval and ratio. Helpfully, these actually map onto the categorical and continuous types, as shown below.\n\n\n\nDifferent types of data\n\n\nYou can think about each of these data types as “adding a feature” to the previous one.\nNominal data are categorical in the simplest way: there are a bunch of groups. Examples include colour, flavour, degree subject, which river you found the fish in, and so on.\nOrdinal data still have groups, making them still categorical, but have the added feature of a meaningful order to those categories. Examples include spice level or where you placed in a running race. In a research context, a common souce of ordinal data is survey questions, where participants rate things on a scale of “agree to disagree” or “between 1 and 10”. We have a specific way of dealing with this sort of variable in R - they are treated as factors, which are categorical variables where a specific order is preserved. We refer to the groups/categories as “levels” of the factor, and specify the order they should go in.\nNow, we cross over into continuous data. A variable is an example of interval data if there are equal spaces between the values, or a consistent scale. Examples include temperature, time, or credit score. Importantly, a rating of 1 to 10 wouldn’t qualify as interval data, because although the values appear to be numerical, they actually represent categories, and there is no way of being sure that the difference between a rating of 4 and 5 is equivalent to the difference between 8 and 9.\nFinally, we have ratio data. Now, the distinction between interval and ratio data is very subtle. The only difference between them is that a ratio variable has a true zero, which makes it meaningful for us to talk about something being “twice as heavy” or “half the speed”. Compare this to telling the time on a 12-hr clock. Although there are consistent intervals of seconds, minutes and hours, 6 o’clock is not “twice as time” as 3 o’clock - time is an interval variable. (Now, of course, if you were measuring time taken for something to happen, that would start at zero, and would qualify as ratio data!)\nDon’t worry if that last bit feels a bit too nuanced or specific - interval/ratio data are treated exactly the same in statistical contexts, and it’s really not essential to wrap your head around it. Suffice it to say, continuous data requires us to have equal spacing, and often there will also be some true zero in there as well.",
    "crumbs": [
      "Variables & distributions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Variables and data types</span>"
    ]
  },
  {
    "objectID": "materials/variables-and-data-types.html#independent-vs-paired-samples",
    "href": "materials/variables-and-data-types.html#independent-vs-paired-samples",
    "title": "8  Variables and data types",
    "section": "8.2 Independent vs paired samples",
    "text": "8.2 Independent vs paired samples\nYou’ve already encountered the distinction between independent and paired samples in this course, when looking at different types of t-tests. Here, we’ll review that distinction more explicitly, and think about the reasons why we might choose these different approaches from an experimental design perspective.\n\nIndependent samples\nThis means that a separate group undergoes each of the experimental conditions. In other words, the datapoints in each of your categories represent a completely different set of biological units or samples.\nIn lots of cases, you won’t have a choice to collect anything other than independent samples, because you don’t always get to assign the biological units/samples to the conditions as an experimenter. If you’re comparing patients to controls in a clinical study, for instance, or plants from two different species, they will be “pre-assigned” by nature, and cannot belong to both categories.\nSometimes you do get to decide, though, and will still choose to collect independent samples. The primary reason for doing so is that having separate groups undertake the different conditions guarantees that those conditions can’t affect one another. For instance, if you’re testing the outcomes of different drugs, you may want a completely different set of people for each new drug, since giving multiple drugs to the same people could make it difficult to detect their individual efficacy if they have interacting effects.\n\n\nPaired samples\nThis means that the same group undergoes each condition, and the group is therefore compared to itself.\nPaired sampling usually occurs when you want to compare the same individuals or samples across multiple time points, e.g., if measuring improvement after a treatment.\nOne of the major advantages of paired sampling is that matching of samples between conditions is perfect, because individuals are matched directly to themselves. This is useful in situations where you have unusual or extreme individuals; if you have a quirky person in your psychology study, they will bring their quirkiness to all of the conditions, and so you can factor out that quirkiness when comparing the conditions. This innate matching is a strength of paired sampling that you don’t get with independent samples (but see below for some discussion of matched pairs designs).\nAnother advantage of paired samples is that you typically need fewer unique samples or participants in your study, and so it can be easier to collect the data.\nHowever, as discussed above, using the same biological units can cause “contamination” between conditions, introducing unexpected interactions or bias; or, if you’re testing on humans or animals, they may get tired or bored and not participate as effectively as time goes on. So, there are plenty of situations where using paired samples wouldn’t be appropriate.\n\n\n\nIndependent vs paired samples\n\n\nIn summary: the pros of one approach are the cons of the other. You may well find that the nature of your research makes the decision for you, in which case your main job is identifying which type of sampling you have, and using the appropriate statistical test. However, if you’re in a situation where you get to choose, it’s worth thinking carefully about which of these methods will give you the most confidence about your conclusions.\n\n\n\n\n\n\nMatched pairs design\n\n\n\n\n\nAs discussed above, one of the problems with independent sampling is that you may end up with mismatched groups, and it can be difficult to tell whether differences between the groups are truly due to the predictor variable(s) of interest, or were already present. One way to offset this is by using a matched pairs design, to ensure that your groups are as similar to one another as possible.\nThis is perhaps most common in clinical studies, where researchers want to compare a patient group to some healthy controls. In order to make sure that the comparison is useful, the healthy controls ought to be matched in terms of demographic variables like age, sex, perhaps educational background or socioeconomic status and so on. To make this matching as precise as possible, the construction of matched pairs may occur, where for each patient, a control is selected who matches that patient as closely as possible on all those demographic variables.\n\n\n\nMatched pairs design\n\n\nThis does eliminate some of the unwanted confounding variables (more on confounds later in the course), but as you can imagine, is also quite time consuming for the researcher.",
    "crumbs": [
      "Variables & distributions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Variables and data types</span>"
    ]
  },
  {
    "objectID": "materials/variables-and-data-types.html#summary",
    "href": "materials/variables-and-data-types.html#summary",
    "title": "8  Variables and data types",
    "section": "8.3 Summary",
    "text": "8.3 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nVariables can be categorical, with distinct groups, or continuous, taking any value along a range\nCategorical variables can take the form of nominal or ordinal data, depending on whether there is an order to the categories\nContinuous variables can be either interval or ratio data, depending on whether there is a “true” zero\nSamples can be independent, with different samples taking part in each group, or paired, where the same group undergoes each condition\nThe categorical vs continuous and independent vs paired distinctions both have implications for choosing statistical tests",
    "crumbs": [
      "Variables & distributions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Variables and data types</span>"
    ]
  },
  {
    "objectID": "materials/distributions-and-nonnormal-data.html",
    "href": "materials/distributions-and-nonnormal-data.html",
    "title": "9  Dealing with non-normal data",
    "section": "",
    "text": "9.1 What is a distribution?\nA distribution is all about probability. The technical definition of a distribution is that it is a probability function, describing a phenomenon or variable in terms of its sample space and probabilities of events. I prefer to rephrase this in my head as: a distribution describes the possible outcomes for a variable, along with how likely those outcomes are.\nThe example distribution below is a normal distribution, also known as a Gaussian distribution or sometimes a “bell curve”, representing height across the global human population:\nOn the x axis, we have our variable of interest - height, measured in cm. On the y axis is probability density, which reflects the relative likelihood of the values along the x axis. So, when the curve peaks in the middle, that reflects that a height of 169cm is the most likely value for us to measure, if we were drawing totally randomly from all the humans in the world. At the tails of the distribution (where it drops away at either side), we have heights that we are much less likely to observe.\nWe use parameters to describe distributions (and statistics to describe samples - and then we try to estimate parameters from those statistics!). The normal distribution, specifically, is described by two important parameters: the mean, and the standard deviation (a measure of variation). As these two numbers change, so does the precise shape of the bell curve; smaller standard deviations lead to narrower curves, and as the mean increases/decreases, the curve will shift up and down the x axis.",
    "crumbs": [
      "Variables & distributions",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Dealing with non-normal data</span>"
    ]
  },
  {
    "objectID": "materials/distributions-and-nonnormal-data.html#what-is-a-distribution",
    "href": "materials/distributions-and-nonnormal-data.html#what-is-a-distribution",
    "title": "9  Dealing with non-normal data",
    "section": "",
    "text": "An example of the normal (Gaussian) distribution",
    "crumbs": [
      "Variables & distributions",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Dealing with non-normal data</span>"
    ]
  },
  {
    "objectID": "materials/distributions-and-nonnormal-data.html#going-beyond-the-normal-distribution",
    "href": "materials/distributions-and-nonnormal-data.html#going-beyond-the-normal-distribution",
    "title": "9  Dealing with non-normal data",
    "section": "9.2 Going beyond the normal distribution",
    "text": "9.2 Going beyond the normal distribution\nThe normal distribution is really important in statistics, and for most of the analyses we perform in the course, normality will continue to be one of the key assumptions.\nHowever, we can’t always assume normality. The bell curve is not the only distribution that exists in the world - in fact, you may well be familiar with some of the others. Here’s a few examples, to hopefully get you thinking a little bit about distributions more broadly:\n The main point of this figure is not to encourage you to memorise all the possible distributions that exist - in fact, for now, if you only remember the normal distribution, I’m very happy with that. It’s mostly just to flag up that other distributions exist, and how they look relative to the bell curve you’re used to seeing.\nIf you’re interested in learning more about some other distributions, though, I’ve added a bit of info below about some of the more common ones!\n\n\n\n\n\n\nThe uniform distribution\n\n\n\n\n\nThe uniform distribution is just a flat line - i.e., the value of y is constant across all values of x. So, all outcomes are equally relatively likely as each other.\nA classic example of a real-world phenomenon that follows a uniform distribution is a standard six-sided die. Each time you roll the die, the likelihood of each of the six numbers coming up is 1/6. Each outcome is equally likely - contrast this to a normal distribution, where outcomes closer to the mean are much more likely to occur, compared to extreme values far from the mean.\nA uniform distribution has two parameters: the minimum (a) and maximum (b). In the case of the six-sided die, a = 1 and b = 6. The probability of any values outside of this range would be 0.\n\n\n\n\n\n\n\n\n\nThe exponential distribution\n\n\n\n\n\nThis one isn’t shown on the figure above (although technically, the exponential distribution is a special case of the gamma distribution). The exponential distribution is a gradually decreasing curve that asymptotes towards zero as x increases.\nThe exponential distribution is most often used to describe the amount of time until an event takes place, such as in time-to-event or survival analysis in biology. A more fun example, though: the value of the change you have in your pocket also roughly follows an exponential distribution!\nThis distribution has one parameter, \\(\\lambda\\), the rate parameter (i.e., how steep is the drop-off in y as x increases).\n\n\n\n\n\n\n\n\n\nF, t and chi-square distributions\n\n\n\n\n\nI want to flag these distributions here for any of you who are a bit more interested in how the maths of statistical hypothesis testing works. F, t and chi-square are all examples of a statistic - a value that we can calculate from our dataset using some formula. Each of these statistics have their own distributions, which is what allows us to calculate a p-value from them. Once you’ve calculated the statistic, you can find the probability of getting that value or higher by calculating the area under the curve (or, more specifically, letting R/Python do that calculation for you!).\nAn additional note about statistics distributions: they look a little different depending on the number of degrees of freedom. So, the same value for a statistic will be associated with a different probability for samples of different sizes. This is why, when we report the results of a statistical test, we also give the degrees of freedom in brackets, e.g., t(31) = 1.984, p &lt; 0.05.\n\n\n\n\n\n\n\n\n\nOther properties of a distribution: kurtosis and skewness\n\n\n\n\n\nYou will occasionally hear statisticians talking about the kurtosis or skewness of a distribution, relative to the normal distribution.\nKurtosis is a measure of how often outliers occur; you’ll also sometimes see people talking about the “tailedness” of a curve. The perfect normal distribution is what we refer to as “mesokurtic”, and we use this as our baseline. A curve can be heavy-tailed, or “leptokurtic”, which means it is more peaked and narrow than the standard normal distribution, with wider/fatter tails - the range of values that are considered extreme/outliers is wider than in the normal distribution. The Student’s t distribution, shown above, is heavy-tailed. A curve can also be light/thin-tailed, or “platykurtic”, which means that it’s flattened and stretched wider than the normal distribution. An extreme example of a platykurtic distribution is the uniform distribution, which has absolutely no curve to it at all - and therefore, by definition, there is actually no such thing as a outlier in the uniform distribution!\nSkewness is a little easier to get your head around - it refers to the symmetry, or lack of symmetry, of a distribution. The perfect normal distribution has no skew, because it’s perfectly symmetrical. The F and gamma distributions shown above, relative to the normal distribution, have what we refer to as positive or right skew, because the right hand tail extends much further out. The opposite is called negative or left skew.",
    "crumbs": [
      "Variables & distributions",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Dealing with non-normal data</span>"
    ]
  },
  {
    "objectID": "materials/distributions-and-nonnormal-data.html#dealing-with-non-normal-data",
    "href": "materials/distributions-and-nonnormal-data.html#dealing-with-non-normal-data",
    "title": "9  Dealing with non-normal data",
    "section": "9.3 Dealing with non-normal data",
    "text": "9.3 Dealing with non-normal data\nThe tests you’ve looked at so far - one-sample, paired and Students’ t-tests - as well as other tests that you’ll be learning about shortly, all make an assumption of normality. They’re known as parametric tests - parametric, because they make assumptions about the parameters of the underlying distribution that the data are drawn from.\nHowever, not all variables in the world are normally distributed. So what do we do when they’re not?\nIn contrast to parametric tests, there exists a number of tests that are non-parametric. They don’t assume that your data are normally distributed. (Note that not assuming normality doesn’t mean they don’t make any assumptions - we still typically expect the datapoints to be independent, and sometimes we expect the distribution to at least be symmetric, if not normal.)\nThis means, in situations where we can’t assume normality and we don’t have the ability to transform our dataset, we can fall back on these tests instead.\nBelow is a little cheat-table, that gives you some ideas of the non-parametric equivalent of popular parametric tests. As we go through the course, you’ll start practising both.\n\n\n\n\n\n\n\n\nType of analysis\nParametric test\nNon-parametric test\n\n\n\n\nComparing two independent groups\nStudents’ t-test\nMann-Whitney U test (aka Wilcoxon rank-sum test)\n\n\nComparing paired samples\nPaired t-test\nWilcoxon signed-rank test\n\n\nComparing 3+ groups\nOne-way ANOVA\nKruskal Wallis test\n\n\nCorrelation\nPearson’s r\nSpearman’s \\(\\rho\\)\n\n\nTesting frequency distributions\nN/A\nChi-square tests\n\n\n\nNow that you know this, you can go back to the previous section on different types of t-test and complete the additional exercises at the bottom of each page.",
    "crumbs": [
      "Variables & distributions",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Dealing with non-normal data</span>"
    ]
  },
  {
    "objectID": "materials/distributions-and-nonnormal-data.html#summary",
    "href": "materials/distributions-and-nonnormal-data.html#summary",
    "title": "9  Dealing with non-normal data",
    "section": "9.4 Summary",
    "text": "9.4 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nA distribution is a probability function that represents the possible values a variable can take, and the relative likelihood of those values occurring\nThe normal distribution comes up often in statistics to describe continuous variables, but there are many others\nParametric tests assume normality, but when we aren’t able to satisfy this assumption, we may be able to run a non-parametric test instead",
    "crumbs": [
      "Variables & distributions",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Dealing with non-normal data</span>"
    ]
  },
  {
    "objectID": "materials/cs2_practical_anova.html",
    "href": "materials/cs2_practical_anova.html",
    "title": "10  ANOVA",
    "section": "",
    "text": "10.1 Purpose and aim\nAnalysis of variance or ANOVA is a test than can be used when we have multiple samples of continuous response data. Whilst it is possible to use ANOVA with only two samples, it is generally used when we have three or more groups. It is used to find out if the samples came from parent distributions with the same mean. It can be thought of as a generalisation of the two-sample Student’s t-test.",
    "crumbs": [
      "Categorical predictors",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "materials/cs2_practical_anova.html#libraries-and-functions",
    "href": "materials/cs2_practical_anova.html#libraries-and-functions",
    "title": "10  ANOVA",
    "section": "10.2 Libraries and functions",
    "text": "10.2 Libraries and functions\n\n\n\n\n\n\nClick to expand\n\n\n\n\n\n\nRPython\n\n\n\n10.2.1 Libraries\n\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\n# Converts stats functions to a tidyverse-friendly format\nlibrary(rstatix)\n\n# Creates diagnostic plots using ggplot2\nlibrary(ggResidpanel)\n\n\n\n10.2.2 Functions\n\n# Computes summary statistics\nrstatix::get_summary_stats()\n\n# Perform Tukey's range test\nrstatix::tukey_hsd()\n\n# Creates diagnostic plots\nggResidpanel::resid_panel()\n\n# Fits a linear model  \nstats::lm()\n\n# Carries out an ANOVA on a linear model \nstats::anova()\n\n# Performs a Shapiro-Wilk test for normality\nstats::shapiro.test()\n\n\n\n\n\n10.2.3 Libraries\n\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Simple yet exhaustive stats functions.\nimport pingouin as pg\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# Statistical models, conducting tests and statistical data exploration\nimport statsmodels.api as sm\n\n# Convenience interface for specifying models using formula strings and DataFrames\nimport statsmodels.formula.api as smf\n\n\n\n10.2.4 Functions\n\n# Summary statistics\npandas.DataFrame.describe()\n\n# Plots the first few rows of a DataFrame\npandas.DataFrame.head()\n\n# Query the columns of a DataFrame with a boolean expression\npandas.DataFrame.query()\n\n# Reads in a .csv file\npandas.read_csv\n\n# Performs an analysis of variance\npingouin.anova()\n\n# Tests for equality of variance\npingouin.homoscedasticity()\n\n# Performs the Shapiro-Wilk test for normality\npingouin.normality()\n\n# Creates a model from a formula and data frame\nstatsmodels.formula.api.ols\n\n# Creates an ANOVA table for one or more fitted linear models\nstatsmodels.stats.anova.anova_lm",
    "crumbs": [
      "Categorical predictors",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "materials/cs2_practical_anova.html#data-and-hypotheses",
    "href": "materials/cs2_practical_anova.html#data-and-hypotheses",
    "title": "10  ANOVA",
    "section": "10.3 Data and hypotheses",
    "text": "10.3 Data and hypotheses\nFor example, suppose we measure the feeding rate of oyster catchers (shellfish per hour) at three sites characterised by their degree of shelter from the wind, imaginatively called exposed (E), partially sheltered (P) and sheltered (S). We want to test whether the data support the hypothesis that feeding rates don’t differ between locations. We form the following null and alternative hypotheses:\n\n\\(H_0\\): The mean feeding rates at all three sites is the same \\(\\mu E = \\mu P = \\mu S\\)\n\\(H_1\\): The mean feeding rates are not all equal.\n\nWe will use a one-way ANOVA test to check this.\n\nWe use a one-way ANOVA test because we only have one predictor variable (the categorical variable location).\nWe’re using ANOVA because we have more than two groups and we don’t know any better yet with respect to the exact assumptions.\n\nThe data are stored in the file data/CS2-oystercatcher-feeding.csv.",
    "crumbs": [
      "Categorical predictors",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "materials/cs2_practical_anova.html#summarise-and-visualise",
    "href": "materials/cs2_practical_anova.html#summarise-and-visualise",
    "title": "10  ANOVA",
    "section": "10.4 Summarise and visualise",
    "text": "10.4 Summarise and visualise\n\nRPython\n\n\nFirst we read in the data.\n\n# load data\noystercatcher &lt;- read_csv(\"data/CS2-oystercatcher-feeding.csv\")\n\n# and have a look\noystercatcher\n\n# A tibble: 120 × 2\n   site    feeding\n   &lt;chr&gt;     &lt;dbl&gt;\n 1 exposed    12.2\n 2 exposed    13.1\n 3 exposed    17.9\n 4 exposed    13.9\n 5 exposed    14.1\n 6 exposed    18.4\n 7 exposed    15.0\n 8 exposed    10.3\n 9 exposed    11.8\n10 exposed    12.5\n# ℹ 110 more rows\n\n\nThe oystercatcher data set contains two columns:\n\na site column with information on the amount of shelter of the feeding location\na feeding column containing feeding rates\n\nNext, we get some basic descriptive statistics:\n\n# get some basic descriptive statistics\noystercatcher %&gt;% \n  group_by(site) %&gt;% \n  get_summary_stats(type = \"common\")\n\n# A tibble: 3 × 11\n  site      variable     n   min   max median   iqr  mean    sd    se    ci\n  &lt;chr&gt;     &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 exposed   feeding     40  8.35  18.6   13.9  3.40  13.8  2.44 0.386 0.781\n2 partial   feeding     40 10.8   23.0   16.9  2.82  17.1  2.62 0.414 0.838\n3 sheltered feeding     40 18.9   28.5   23.2  3.79  23.4  2.42 0.383 0.774\n\n\nFinally, we plot the data by site:\n\n# plot the data\nggplot(oystercatcher,\n       aes(x = site, y = feeding)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\nFirst, we read in the data.\n\n# load the data\noystercatcher_py = pd.read_csv(\"data/CS2-oystercatcher-feeding.csv\")\n\n# and have a look\noystercatcher_py.head()\n\n      site    feeding\n0  exposed  12.175506\n1  exposed  13.073917\n2  exposed  17.939687\n3  exposed  13.891783\n4  exposed  14.051663\n\n\nThe oystercatcher_py data set contains two columns:\n\na site column with information on the amount of shelter of the feeding location\na feeding column containing feeding rates\n\nNext, we get some basic descriptive statistics per group. Here we use the pd.groupby() function to group by site. We only want to have summary statistics for the feeding variable, so we specify that as well:\n\noystercatcher_py.groupby(\"site\")[\"feeding\"].describe()\n\n           count       mean       std  ...        50%        75%        max\nsite                                   ...                                 \nexposed     40.0  13.822899  2.441974  ...  13.946420  15.581748  18.560404\npartial     40.0  17.081666  2.619906  ...  16.927683  18.416708  23.021250\nsheltered   40.0  23.355503  2.419825  ...  23.166246  25.197096  28.451252\n\n[3 rows x 8 columns]\n\n\nFinally, we plot the data:\n\n# plot the data\n(ggplot(oystercatcher_py,\n        aes(x = \"site\",\n            y = \"feeding\")) +\n     geom_boxplot())\n\n\n\n\n\n\n\n\n\n\n\nLooking at the data, there appears to be a noticeable difference in feeding rates between the three sites. We would probably expect a reasonably significant statistical result here.",
    "crumbs": [
      "Categorical predictors",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "materials/cs2_practical_anova.html#assumptions",
    "href": "materials/cs2_practical_anova.html#assumptions",
    "title": "10  ANOVA",
    "section": "10.5 Assumptions",
    "text": "10.5 Assumptions\nTo use an ANOVA test, we have to make three assumptions:\n\nThe parent distributions from which the samples are taken are normally distributed\nEach data point in the samples is independent of the others\nThe parent distributions should have the same variance\n\nIn a similar way to the two-sample tests we will consider the normality and equality of variance assumptions both using tests and by graphical inspection (and ignore the independence assumption).\n\n10.5.1 Normality\nFirst we perform a Shapiro-Wilk test on each site separately.\n\nRPython\n\n\nWe take the data, filter for each type of site, extract the feeding rates and send those data to the shapiro.test() function.\n\n# Shapiro-Wilk test on each site\noystercatcher %&gt;% \n    filter(site == \"exposed\") %&gt;% \n    pull(feeding) %&gt;% \n    shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.98859, p-value = 0.953\n\noystercatcher %&gt;% \n    filter(site == \"partial\") %&gt;% \n    pull(feeding) %&gt;% \n    shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.98791, p-value = 0.9398\n\noystercatcher %&gt;% \n    filter(site == \"sheltered\") %&gt;% \n    pull(feeding) %&gt;% \n    shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.97511, p-value = 0.5136\n\n\n\n\nWe use the pg.normality() function to calculate the statistic. This requires:\n\nthe dv dependent variable (feeding in our case)\nthe group variable (site)\nand some data\n\n\npg.normality(dv = \"feeding\",\n             group = \"site\",\n             data = oystercatcher_py)\n\n                  W      pval  normal\nexposed    0.988586  0.953029    True\npartial    0.987907  0.939833    True\nsheltered  0.975106  0.513547    True\n\n\n\n\n\nWe can see that all three groups appear to be normally distributed which is good.\n\n\n\n\n\n\nImportant\n\n\n\nFor ANOVA however, considering each group in turn is often considered quite excessive and, in most cases, it is sufficient to consider the normality of the combined set of residuals from the data. We’ll explain residuals properly in the next session, but effectively they are the difference between each data point and its group mean.\nTo get hold of these residuals, we need to create a linear model. Again, this will be explained in more detail in the next section. For now, we see it as a way to describe the relationship between the feeding rate and the site.\n\n\nSo, we create a linear model, extract the residuals and check their normality:\n\nRPython\n\n\nWe use the lm() function to define the linear model that describes the relationship between feeding and site. The notation is similar to what we used previously when we were dealing with two samples of data.\n\n# define the model\nlm_oystercatcher &lt;- lm(feeding ~ site,\n                       data = oystercatcher)\n\nWe can read this as “create a linear model (lm) where the feeding rate (feeding) depends on the site (site), using the oystercatcher data”.\nWe store the output of that in an object called lm_oystercatcher. We’ll look into what this object contains in more detail in later sections.\nFor now, we extract the residuals from this object using the residuals() function and then use this in the shapiro.test() function.\n\n# extract the residuals\nresid_oyster &lt;- residuals(lm_oystercatcher)\n\n# perform Shapiro-Wilk test on residuals\nshapiro.test(resid_oyster)\n\n\n    Shapiro-Wilk normality test\n\ndata:  resid_oyster\nW = 0.99355, p-value = 0.8571\n\n\n\n\nUnfortunately pingouin does not have a straightforward way of extracting residuals (if you know more, please let me know!).\nTo get our residuals we use statsmodels, a module that provides functions for statistical models. We’ll be using this in upcoming sessions, so you’ll have a head start!\nAt this point you shouldn’t concern yourself too much with the exact syntax, just run it an have a look.\n\n\n\n\n\n\nTechnical details (optional)\n\n\n\n\n\nWe need to import a few extra modules. First, we load the statsmodels.api module, which contains an OLS() function (Ordinary Least Squares - the equivalent of the lm() function in R).\nWe also import stats.models.formula.api so we can use the formula notation in our linear model. We define the formula as formula = \"feeding ~ C(site)\" with C conveying that the site variable is a category. Lastly we can .fit() the model.\nIf you’re familiar with this stuff then you can look at the model itself by running summary(lm_oystercatcher_py). But we’ll cover all of this in later sessions.\n\n\n\nWe load the modules, define a linear model, create a fit() and we get the residuals from the linear model fit with .resid.\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n\n# create a linear model\nmodel = smf.ols(formula= \"feeding ~ C(site)\", data = oystercatcher_py)\n# and get the fitted parameters of the model\nlm_oystercatcher_py = model.fit()\n\n\n# get the residuals from the model fit\n# and perform Shapiro-Wilk test\npg.normality(lm_oystercatcher_py.resid)\n\n          W      pval  normal\n0  0.993545  0.857013    True\n\n\n\n\n\nAgain, we can see that the combined residuals from all three groups appear to be normally distributed (which is as we would have expected given that they were all normally distributed individually!)\n\n\n10.5.2 Equality of variance\nWe now test for equality of variance using Bartlett’s test (since we’ve just found that all of the individual groups are normally distributed).\nPerform Bartlett’s test on the data:\n\nRPython\n\n\n\n# check equality of variance\nbartlett.test(feeding ~ site,\n              data = oystercatcher)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  feeding by site\nBartlett's K-squared = 0.29598, df = 2, p-value = 0.8624\n\n\nWhere the relevant p-value is given on the 3rd line. Here we see that each group appears to have comparable variance.\n\n\nWe use the homoscedasticity() function from pingouin (homoscedasticity is another way of describing equality of variance). The default method is levene, so we need to specify that we want to use bartlett.\n\npg.homoscedasticity(dv = \"feeding\",\n                    group = \"site\",\n                    method = \"bartlett\",\n                    data = oystercatcher_py)\n\n                 T      pval  equal_var\nbartlett  0.295983  0.862439       True\n\n\nWhere the relevant p-value is given in the pval column. Here we see that each group appears to have the same variance.\n\n\n\n\n\n10.5.3 Graphical interpretation and diagnostic plots\nAssessing assumptions via these tests can be cumbersome, but also a bit misleading at times. It reduces the answer to the question “is the assumption met?” to a yes/no, based on some statistic and associated p-value.\nThis does not convey that things aren’t always so clear-cut and that there is a lot of grey area that we need to navigate. As such, assessing assumptions through graphical means - using diagnostic plots - is often preferred.\n\nRPython\n\n\nIn the first session we already created diagnostic Q-Q plots directly from our data, using stat_qq() and stat_qq_line(). For more specific plots this becomes a bit cumbersome. There is an option to create ggplot-friendly diagnostic plots, using the ggResidPanel package.\nIf you haven’t got ggResidpanel installed, please run the following code:\n\n# install package\ninstall.packages(\"ggResidpanel\")\n\n# load library\nlibrary(ggResidpanel)\n\nLet’s create the diagnostic plots we’re interested in using ggResidPanel. It takes a linear model object as input (lm_oystercatcher) and you can define which plots you want to see using the plots = argument. I have also added a smoother line (smoother = TRUE) to the plots, which we’ll use to compare against.\n\nresid_panel(lm_oystercatcher,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\n\n\nThe top left graph plots the Residuals plot. If the data are best explained by a linear line then there should be a uniform distribution of points above and below the horizontal blue line (and if there are sufficient points then the red line, which is a smoother line, should be on top of the blue line). This plot looks pretty good.\nThe top right graph shows the Q-Q plot which allows a visual inspection of normality. If the residuals are normally distributed, then the points should lie on the diagonal blue line. This plot looks good.\nThe bottom left Location-Scale graph allows us to investigate whether there is any correlation between the residuals and the predicted values and whether the variance of the residuals changes significantly. If not, then the red line should be horizontal. If there is any correlation or change in variance then the red line will not be horizontal. This plot is fine.\nThe last graph shows the Cook’s distance and tests if any one point has an unnecessarily large effect on the fit. A rule of thumb is that if any value is larger than 1.0, then it might have a large effect on the model. If not, then no point has undue influence. This plot is good. There are different ways to determine the threshold (apart from simply setting it to 1) and in this plot the blue dashed line is at 4/n, with n being the number of samples. At this threshold there are some data points that may be influential, but I personally find this threshold rather strict.\n\n\n\nUnfortunately Python doesn’t provide a convenient way of displaying the same diagnostic plots as R does.\nI created a function dgplots() (which stands for Diagnostic Plots - very original I know…) that does this for you. All you need to do is create a linear model, get the fit and feed that to the dgplots() function.\nYou can of course plot the model values yourself by extracting them from the linear model fit, but this should provide a convenient way to avoid that kind of stuff.\n\ndgplots(lm_oystercatcher_py)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe top left graph plots the Residuals plot. If the data are best explained by a linear line then there should be a uniform distribution of points above and below the horizontal blue line (and if there are sufficient points then the red line, which is a smoother line, should be on top of the blue line). This plot looks pretty good.\nThe top right graph shows the Q-Q plot which allows a visual inspection of normality. If the residuals are normally distributed, then the points should lie on the diagonal blue line. This plot looks good.\nThe bottom left Location-Scale graph allows us to investigate whether there is any correlation between the residuals and the predicted values and whether the variance of the residuals changes significantly. If not, then the red line should be horizontal. If there is any correlation or change in variance then the red line will not be horizontal. This plot is fine.\nThe last graph shows the Influential points and tests if any one point has an unnecessarily large effect on the fit. Here we’re using the Cook’s distance as a measure. A rule of thumb is that if any value is larger than 1.0, then it might have a large effect on the model. If not, then no point has undue influence. This plot is good. There are different ways to determine the threshold (apart from simply setting it to 1) and in this plot the blue dashed line is at 4/n, with n being the number of samples. At this threshold there are some data points that may be influential, but I personally find this threshold rather strict.\n\n\n\n\nWe can see that these graphs are very much in line with what we’ve just looked at using the test, which is reassuring. The groups all appear to have the same spread of data, and the Q-Q plot shows that the assumption of normality is alright.\n\n\n\n\n\n\nAssessing assumptions\n\n\n\nAt this stage, I should point out that I nearly always stick with the graphical method for assessing the assumptions of a test. Assumptions are rarely either completely met or not met and there is always some degree of personal assessment.\nWhilst the formal statistical tests (like Shapiro-Wilk) are technically fine, they can often create a false sense of things being absolutely right or wrong in spite of the fact that they themselves are still probabilistic statistical tests. In these exercises we are using both approaches whilst you gain confidence and experience in interpreting the graphical output and whilst it is absolutely fine to use both in the future I would strongly recommend that you don’t rely solely on the statistical tests in isolation.",
    "crumbs": [
      "Categorical predictors",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "materials/cs2_practical_anova.html#implement-and-interpret-the-test",
    "href": "materials/cs2_practical_anova.html#implement-and-interpret-the-test",
    "title": "10  ANOVA",
    "section": "10.6 Implement and interpret the test",
    "text": "10.6 Implement and interpret the test\nAs is often the case, performing the actual statistical test is the least of your efforts.\n\nRPython\n\n\nIn R we perform the ANOVA on the linear model object, lm_oystercatcher in this case. We do this with the anova() function:\n\nanova(lm_oystercatcher)\n\nAnalysis of Variance Table\n\nResponse: feeding\n           Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nsite        2 1878.02  939.01  150.78 &lt; 2.2e-16 ***\nResiduals 117  728.63    6.23                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis takes the linear model (i.e. finds the means of the three groups and calculates a load of intermediary data that we need for the statistical analysis) and stores this information in an R object (which we’ve called lm_oystercatcher, but which you can call what you like).\nIn the output:\n\nThe 1st line just tells you the that this is an ANOVA test\nThe 2nd line tells you what the response variable is (in this case feeding)\nThe 3rd, 4th and 5th lines are an ANOVA table which contain some useful values:\n\nThe Df column contains the degrees of freedom values on each row, 2 and 117 (which we can use for the reporting)\nThe F value column contains the F statistic, 150.78 (which again we’ll need for reporting).\nThe p-value is 2.2e-16 and is the number directly under the Pr(&gt;F) on the 4th line (to be precise, it is 4.13e-33 but anything smaller than 2.2e-16 gets reported as &lt; 2.2e-16).\nThe other values in the table (in the Sum Sq and Mean Sq) columns are used to calculate the F statistic itself and we don’t need to know these.\n\nThe 6th line has some symbolic codes to represent how big (small) the p-value is; so, a p-value smaller than 0.001 would have a *** symbol next to it (which ours does). Whereas if the p-value was between 0.01 and 0.05 then there would simply be a * character next to it, etc. Thankfully we can all cope with actual numbers and don’t need a short-hand code to determine the reporting of our experiments (please tell me that’s true…!)\n\n\n\nThere are different ways of conducting an ANOVA in Python, with scipy.stats proving an option. However, I find that the anova() function in pingouin provides the easiest and most-detailed option to do this.\nIt takes the following arguments:\n\ndv: dependent variable (response variable; in our case feeding)\nbetween: between-subject factor (predictor variable; in our case site)\ndata: which function doesn’t!?\ndetailed: optional True or False, we’re setting it to True because we like to know what we’re doing!\n\n\npg.anova(dv = \"feeding\",\n         between = \"site\",\n         data = oystercatcher_py,\n         detailed = True)\n\n   Source           SS   DF          MS           F         p-unc       np2\n0    site  1878.015371    2  939.007685  150.782449  4.128088e-33  0.720473\n1  Within   728.625249  117    6.227566         NaN           NaN       NaN\n\n\nThis creates a linear model based on the data, i.e. finds the means of the three groups and calculates a load of intermediary data that we need for the statistical analysis.\nIn the output:\n\nSource: Factor names - in our case these are the different sites (site)\nSS: Sums of squares (we’ll get to that in a bit)\nDF: Degrees of freedom (at the moment only used for reporting)\nMS: Mean squares\nF: Our F-statistic\np-unc: p-value (unc stands for “uncorrected” - more on multiple testing correction later)\nnp2: Partial eta-square effect sizes (more on this later)\n\nAlternatively, and we’ll be using this method later on in the course, you can perform an ANOVA on the lm_oystercatcher_py object we created earlier.\nThis uses the sm.stats.anova_lm() function from statsmodels. As you’ll see, the output is very similar:\n\nsm.stats.anova_lm(lm_oystercatcher_py)\n\n             df       sum_sq     mean_sq           F        PR(&gt;F)\nC(site)     2.0  1878.015371  939.007685  150.782449  4.128088e-33\nResidual  117.0   728.625249    6.227566         NaN           NaN\n\n\n\n\n\nAgain, the p-value is what we’re most interested in here and shows us the probability of getting samples such as ours if the null hypothesis were actually true.\nSince the p-value is very small (much smaller than the standard significance level of 0.05) we can say “that it is very unlikely that these three samples came from the same parent distribution” and as such we can reject our null hypothesis and state that:\n\nA one-way ANOVA showed that the mean feeding rate of oystercatchers differed significantly between locations (p = 4.13e-33).",
    "crumbs": [
      "Categorical predictors",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "materials/cs2_practical_anova.html#post-hoc-testing-tukeys-range-test",
    "href": "materials/cs2_practical_anova.html#post-hoc-testing-tukeys-range-test",
    "title": "10  ANOVA",
    "section": "10.7 Post-hoc testing (Tukey’s range test)",
    "text": "10.7 Post-hoc testing (Tukey’s range test)\nOne drawback with using an ANOVA test is that it only tests to see if all of the means are the same. If we get a significant result using ANOVA then all we can say is that not all of the means are the same, rather than anything about how the pairs of groups differ. For example, consider the following box plot for three samples.\n\n\n\n\n\n\n\n\n\nEach group is a random sample of 20 points from a normal distribution with variance 1. Groups 1 and 2 come from a parent population with mean 0 whereas group 3 come from a parent population with mean 2. The data clearly satisfy the assumptions of an ANOVA test.\nHow do we find out if there are any differences between these groups and, if so, which groups are different from each other?\n\n10.7.1 Read in data and plot\n\nRPython\n\n\n\n# load the data\ntukey &lt;- read_csv(\"data/CS2-tukey.csv\")\n\nRows: 60 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): group\ndbl (1): response\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# have a look at the data\ntukey\n\n# A tibble: 60 × 2\n   response group  \n      &lt;dbl&gt; &lt;chr&gt;  \n 1    1.58  sample1\n 2    0.380 sample1\n 3   -0.997 sample1\n 4   -0.771 sample1\n 5    0.169 sample1\n 6   -0.698 sample1\n 7   -0.167 sample1\n 8    1.38  sample1\n 9   -0.839 sample1\n10   -1.05  sample1\n# ℹ 50 more rows\n\n\n\n# plot the data\nggplot(tukey,\n       aes(x = group, y = response)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\n# load the data\ntukey_py = pd.read_csv(\"data/CS2-tukey.csv\")\n\n# have a look at the data\ntukey_py.head()\n\n   response    group\n0  1.580048  sample1\n1  0.379544  sample1\n2 -0.996505  sample1\n3 -0.770799  sample1\n4  0.169046  sample1\n\n\n\n# plot the data\n(ggplot(tukey_py,\n        aes(x = \"group\",\n            y = \"response\")) +\n     geom_boxplot())\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.7.2 Test for a significant difference in group means\n\nRPython\n\n\n\n# create a linear model\nlm_tukey &lt;- lm(response ~ group,\n               data = tukey)\n\n# perform an ANOVA\nanova(lm_tukey)\n\nAnalysis of Variance Table\n\nResponse: response\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ngroup      2 33.850 16.9250   20.16 2.392e-07 ***\nResiduals 57 47.854  0.8395                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\npg.anova(dv = \"response\",\n         between = \"group\",\n         data = tukey_py,\n         detailed = True)\n\n   Source         SS  DF         MS          F         p-unc       np2\n0   group  33.850044   2  16.925022  20.159922  2.391626e-07  0.414302\n1  Within  47.853670  57   0.839538        NaN           NaN       NaN\n\n\n\n\n\nHere we have a p-value of 2.39 \\(\\times\\) 10-7 and so the test has very conclusively rejected the hypothesis that all means are equal.\nHowever, this was not due to all of the sample means being different, but rather just because one of the groups is very different from the others. In order to drill down and investigate this further we use a new test called Tukey’s range test (or Tukey’s honest significant difference test – this always makes me think of some terrible cowboy/western dialogue).\nThis will compare all of the groups in a pairwise fashion and reports on whether a significant difference exists.\n\n\n10.7.3 Performing Tukey’s test\n\nRPython\n\n\nTo perform Tukey’s range test we can use the tukey_hsd() function from the rstatix package. Note, there is a TukeyHSD() function in base R as well, but the tukey_hsd() function can take a linear model object as input, whereas the TukeyHSD() function cannot. This makes the tukey_hsd() function a bit easier to work with.\n\n# perform Tukey's range test on linear model\ntukey_hsd(lm_tukey)\n\n# A tibble: 3 × 9\n  term  group1  group2  null.value estimate conf.low conf.high       p.adj\n* &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 group sample1 sample2          0    0.304   -0.393      1.00 0.55       \n2 group sample1 sample3          0    1.72     1.03       2.42 0.000000522\n3 group sample2 sample3          0    1.42     0.722      2.12 0.0000246  \n# ℹ 1 more variable: p.adj.signif &lt;chr&gt;\n\n\nThe tukey_hsd() function takes our linear model (lm_tukey) as its input. The output is a pair-by-pair comparison between the different groups (samples 1 to 3). We are interested in the p.adj column, which gives us the adjusted p-value. The null hypothesis in each case is that there is no difference in the mean between the two groups.\n\n\n\npg.pairwise_tukey(dv = \"response\",\n                  between = \"group\",\n                  data = tukey_py).transpose()\n\n                0         1         2\nA         sample1   sample1   sample2\nB         sample2   sample3   sample3\nmean(A) -0.049878 -0.049878  0.253878\nmean(B)  0.253878  1.673481  1.673481\ndiff    -0.303756 -1.723359 -1.419603\nse       0.289748  0.289748  0.289748\nT       -1.048347 -5.947789 -4.899442\np-tukey  0.549801  0.000001  0.000025\nhedges   -0.32493 -1.843488 -1.518558\n\n\nThe dv argument is the response variable, whereas the between argument defines the explanatory variable.\nWe .transpose() the data, so we can look at the output a bit easier. Doing so, we focus on the p-tukey values.\n\n\n\nAs we can see that there isn’t a significant difference between sample1 and sample2 but there is a significant difference between sample1 and sample3, as well as sample2 and sample3. This matches with what we expected based on the box plot.\n\n\n10.7.4 Assumptions\nWhen to use Tukey’s range test is a matter of debate (strangely enough a lot of statistical analysis techniques are currently matters of opinion rather than mathematical fact – it does explain a little why this whole field appears so bloody confusing!)\n\nSome people claim that we should only perform Tukey’s range test (or any other post-hoc tests) if the preceding ANOVA test showed that there was a significant difference between the groups and that if the ANOVA test had not shown any significant differences between groups then we would have to stop there.\nOther people say that this is rubbish and we can do whatever we like as long as we tell people what we did.\n\nThe background to this is rather involved but one of the reasons for this debate is to prevent so-called data-dredging or p-hacking. This is where scientists/analysts are so fixated on getting a “significant” result that they perform a huge variety of statistical techniques until they find one that shows that their data is significant (this was a particular problem in psychological studies for while – not to point fingers though, they are working hard to sort their stuff out. Kudos!).\nWhether you should use post-hoc testing or not will depend on your experimental design and the questions that you’re attempting to answer.\nTukey’s range test, when we decide to use it, requires the same three assumptions as an ANOVA test:\n\nNormality of distributions\nEquality of variance between groups\nIndependence of observations",
    "crumbs": [
      "Categorical predictors",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "materials/cs2_practical_anova.html#exercises",
    "href": "materials/cs2_practical_anova.html#exercises",
    "title": "10  ANOVA",
    "section": "10.8 Exercises",
    "text": "10.8 Exercises\n\n10.8.1 Lobster weight\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLevel: \nJuvenile lobsters in aquaculture were grown on three different diets (fresh mussels, semi-dry pellets and dry flakes). After nine weeks, their wet weight was:\n\n\n# A tibble: 7 × 3\n  Mussels Pellets Flakes\n    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1    152.    118.  102. \n2    132.    111.  103. \n3    104.    129.   90.4\n4    154.    110.  133. \n5    132     175.  129. \n6    119      NA   129. \n7    162.     NA    NA  \n\n\nIs there any evidence that diet affects the growth rate of lobsters?\n\nWrite down the null and alternative hypotheses\nImport the data from data/CS2-lobsters.csv\nSummarise and visualise the data\nCheck the assumptions using appropriate tests and graphical analyses\nPerform an ANOVA\nWrite down a sentence that summarises the results that you have found\nPerform a post-hoc test and report the findings\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n10.9 Answer\n\nHypotheses\n\\(H_0\\) : all means are equal\n\\(H_1\\) : not all means are equal\n\n\nImport Data, summarise and visualise\n\nRPython\n\n\n\n# load the data\nlobsters &lt;- read_csv(\"data/CS2-lobsters.csv\")\n\n# look at the data\nlobsters\n\n# A tibble: 18 × 3\n      id weight diet   \n   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  \n 1     1  152.  Mussels\n 2     2  132.  Mussels\n 3     3  104.  Mussels\n 4     4  154.  Mussels\n 5     5  132   Mussels\n 6     6  119   Mussels\n 7     7  162.  Mussels\n 8     8  118.  Pellets\n 9     9  111.  Pellets\n10    10  129.  Pellets\n11    11  110.  Pellets\n12    12  175.  Pellets\n13    13  102.  Flakes \n14    14  103.  Flakes \n15    15   90.4 Flakes \n16    16  133.  Flakes \n17    17  129.  Flakes \n18    18  129.  Flakes \n\n\nThe data have a unique id column, which we don’t need any summary statistics for, so we deselect it:\n\n# create some summary statistics\nlobsters %&gt;% \n  select(-id) %&gt;% \n  group_by(diet) %&gt;% \n  get_summary_stats(type = \"common\")\n\n# A tibble: 3 × 11\n  diet    variable     n   min   max median   iqr  mean    sd    se    ci\n  &lt;chr&gt;   &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Flakes  weight       6  90.4  133.   116.  27.3  114.  18.2  7.42  19.1\n2 Mussels weight       7 104.   162.   132.  27.0  136.  20.6  7.79  19.1\n3 Pellets weight       5 110.   175.   118.  17.8  128.  27.2 12.1   33.7\n\n\nNext, we visualise the data:\n\nlobsters %&gt;% \n  ggplot(aes(x = diet, y = weight)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\nlobsters_py = pd.read_csv(\"data/CS2-lobsters.csv\")\n\nNext, we visualise the data:\n\n(ggplot(lobsters_py,\n        aes(x = \"diet\",\n            y = \"weight\")) +\n     geom_boxplot())\n\n\n\n\n\n\n\n\n\n\n\nAs always we use the plot and summary to assess three things:\n\nDid we load the data in properly?\n\n\nWe see three groups with reasonable values. There aren’t any data points that are obviously wrong (negative, zero or massively big) and we have the right number of groups. So it looks as if we didn’t do anything obviously wrong.\n\n\nWhat do we expect as a result of a statistical test?\n\n\nWhilst the Mussels group does look higher than the other two groups, Pellets and Flakes appear almost identical in terms of average values, and there’s quite a bit of overlap with the Mussels group. A non-significant result is the most likely answer, and I would be surprised to see a significant p-value - especially given the small sample size that we have here.\n\n\nWhat do we think about assumptions?\n\n\nThe groups appear mainly symmetric (although Pellets is a bit weird) and so we’re not immediately massively worried about lack of normality. Again, Flakes and Mussels appear to have very similar variances but it’s a bit hard to decide what’s going on with Pellets. It’s hard to say what’s going on with the assumptions and so I’ll wait and see what the other tests say.\n\n\n\nExplore Assumptions\nWe’ll explore the assumption of normality and equality of variance, assuming that the data are independent.\n\nRPython\n\n\nNormality\nWe’ll be really thorough here and consider the normality of each group separately and jointly using the Shapiro-Wilk test, as well as looking at the Q-Q plot. In reality, and after these examples , we’ll only use the Q-Q plot.\nFirst, we perform the Shapiro-Wilk test on the individual groups:\n\n# Shapiro-Wilk on lobster groups\nlobsters %&gt;% \n    filter(diet == \"Flakes\") %&gt;% \n    pull(weight) %&gt;% \n    shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.84368, p-value = 0.1398\n\nlobsters %&gt;% \n    filter(diet == \"Mussels\") %&gt;% \n    pull(weight) %&gt;% \n    shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.94784, p-value = 0.71\n\nlobsters %&gt;% \n    filter(diet == \"Pellets\") %&gt;% \n    pull(weight) %&gt;% \n    shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.76706, p-value = 0.0425\n\n\nFlakes and Mussels are fine, but, as we suspected from earlier, Pellets appears to have a marginally significant Normality test result.\nLet’s look at the Shapiro-Wilk test for all of the data together:\n\n# create a linear model\nlm_lobsters &lt;- lm(weight ~ diet,\n                  data = lobsters)\n\n# extract the residuals\nresid_lobsters &lt;- residuals(lm_lobsters)\n\n# and perform the Shapiro-Wilk test on the residuals\nresid_lobsters %&gt;% \n  shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.94779, p-value = 0.3914\n\n\nThis on the other hand says that everything is fine. Let’s look at Q-Q plot.\n\n# Q-Q plots\nresid_panel(lm_lobsters,\n            plots = \"qq\")\n\n\n\n\n\n\n\n\nHere, I’ve used an extra argument to the normal diagnostic plots call. The default option is to plot 4 diagnostic plots. You can tell resid_panel() to only plot a specific one, using the plots = arguments. If you want to know more about this have a look at the help documentation or by using ?resid_panel.\nThe Q-Q plot looks OK, not perfect, but more than good enough for us to have confidence in the normality of the data.\nOverall, I’d be happy that the assumption of normality has been adequately well met here. The suggested lack of normality in the Pellets was only just significant and we have to take into account that there are only 5 data points in that group. If there had been a lot more points in that group, or if the Q-Q plot was considerably worse then I wouldn’t be confident.\nEquality of Variance\nWe’ll consider the Bartlett test and we’ll look at some diagnostic plots too.\n\n# perform Bartlett's test\nbartlett.test(weight ~ diet,\n              data = lobsters)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  weight by diet\nBartlett's K-squared = 0.71273, df = 2, p-value = 0.7002\n\n# plot the residuals and scale-location plots\nresid_panel(lm_lobsters,\n            plots = c(\"resid\", \"ls\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\n\nIn the above code I’ve specified which diagnostic plots I wanted. I have also added a smoother line (smoother = TRUE) to the plots.\n\nThe Residuals Plot. What we’re looking for there is that the points are evenly spread on either side of the line. Looks good.\nThe Location-Scale Plot (this is displayed by default in base R’s diagnostic plots). Here we’re looking at the red line. If that line is more or less horizontal, then the equality of variance assumption is met.\n\nHere all three methods agree that there aren’t any issues with equality of variance:\n\nthe Bartlett test p-value is large and non-significant\nthe spread of points in all three groups in the residuals vs fitted graph are roughly the same\nthe red line in the scale-location graph is pretty horizontal\n\nOverall, this assumption is pretty well met.\n\n\nNormality\nWe’ll be really thorough here and consider the normality of each group separately and jointly using the Shapiro-Wilk test, as well as looking at the Q-Q plot. In reality, and after these examples , we’ll only use the Q-Q plot.\nFirst, we perform the Shapiro-Wilk test on the individual groups:\n\n# Shapiro-Wilk on lobster groups\npg.normality(dv = \"weight\",\n             group = \"diet\",\n             data = lobsters_py)\n\n                W      pval  normal\nMussels  0.947836  0.709968    True\nPellets  0.767059  0.042495   False\nFlakes   0.843678  0.139796    True\n\n\nFlakes and Mussels are fine, but, as we suspected from earlier, Pellets appears to have a marginally significant Normality test result.\nLet’s look at the Shapiro-Wilk test for all of the data together:\n\n# create a linear model\nmodel = smf.ols(formula = \"weight ~ C(diet)\", data = lobsters_py)\n# and get the fitted parameters of the model\nlm_lobsters_py = model.fit()\n\n\n# get the residuals from the model fit\n# and perform Shapiro-Wilk test\npg.normality(lm_lobsters_py.resid)\n\n          W      pval  normal\n0  0.947787  0.391367    True\n\n\nThis on the other hand says that everything is fine. Let’s look at Q-Q plot.\n\n# Q-Q plots\n(ggplot(lobsters_py,\n        aes(sample = \"weight\")) +\n     stat_qq() +\n     stat_qq_line(colour = \"red\"))\n\n\n\n\n\n\n\n\nThe Q-Q plot looks OK, not perfect, but more than good enough for us to have confidence in the normality of the data.\nOverall, I’d be happy that the assumption of normality has been adequately well met here. The suggested lack of normality in the Pellets was only just significant and we have to take into account that there are only 5 data points in that group. If there had been a lot more points in that group, or if the Q-Q plot was considerably worse then I wouldn’t be confident.\nEquality of Variance\nWe’ll consider the Bartlett test and we’ll look at the diagnostic plots too.\n\npg.homoscedasticity(dv = \"weight\",\n                    group = \"diet\",\n                    method = \"bartlett\",\n                    data = lobsters_py)\n\n                 T      pval  equal_var\nbartlett  0.712733  0.700216       True\n\n\n\ndgplots(lm_lobsters_py)\n\n\n\n\n\n\n\n\n\n\nWe’ll just focus on the following:\n\nThe Residuals Plot. What we’re looking for there is that the points are evenly spread on either side of the line. Looks good.\nThe Location-Scale Plot (this is displayed by default in base R’s diagnostic plots). Here we’re looking at the red line. If that line is more or less horizontal, then the equality of variance assumption is met.\n\nHere all three methods agree that there aren’t any issues with equality of variance:\n\nthe Bartlett test p-value is large and non-significant\nthe spread of points in all three groups in the residuals vs fitted graph are roughly the same\nthe red line in the scale-location graph is pretty horizontal\n\nOverall, this assumption is pretty well met.\n\n\n\n\n\nCarry out one-way ANOVA\nWith our assumptions of normality and equality of variance met we can be confident that a one-way ANOVA is an appropriate test.\n\nRPython\n\n\n\nanova(lm_lobsters)\n\nAnalysis of Variance Table\n\nResponse: weight\n          Df Sum Sq Mean Sq F value Pr(&gt;F)\ndiet       2 1567.2  783.61  1.6432 0.2263\nResiduals 15 7153.1  476.87               \n\n\n\n\n\npg.anova(dv = \"weight\",\n         between = \"diet\",\n         data = lobsters_py,\n         detailed = True)\n\n   Source           SS  DF          MS        F     p-unc       np2\n0    diet  1567.229381   2  783.614690  1.64324  0.226313  0.179722\n1  Within  7153.075619  15  476.871708      NaN       NaN       NaN\n\n\n\n\n\n\nA one-way ANOVA test indicated that the mean weight of juvenile lobsters did not differ significantly between diets (p = 0.23).\n\n\n\nPost-hoc testing with Tukey\nIn this case we did not find any significant differences between the different diets. So that is a good time for me to reiterate that carrying out the post-hoc test after getting a non-significant result with ANOVA is something that you have to think very carefully about and it all depends on what your research question it.\nIf your research question was:\n\nDoes diet affect lobster weight?\n\nor\n\nIs there any effect of diet on lobster weight?\n\nThen when we got the non-significant result from the ANOVA test we should have just stopped there as we have our answer. Going digging for “significant” results by running more tests is a main factor that contributes towards lack of reproducibility in research.\nIf on the other hand your research question was:\n\nAre any specific diets better or worse for lobster weight than others?\n\nThen we should probably have just skipped the one-way ANOVA test entirely and just jumped straight in with the Tukey’s range test. The important point here is that the result of the one-way ANOVA test doesn’t stop you from carrying out the Tukey test - but it’s up to you to decide whether it is sensible.",
    "crumbs": [
      "Categorical predictors",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "materials/cs2_practical_anova.html#answer",
    "href": "materials/cs2_practical_anova.html#answer",
    "title": "10  ANOVA",
    "section": "10.9 Answer",
    "text": "10.9 Answer\n\nHypotheses\n\\(H_0\\) : all means are equal\n\\(H_1\\) : not all means are equal\n\n\nImport Data, summarise and visualise\n\nRPython\n\n\n\n# load the data\nlobsters &lt;- read_csv(\"data/CS2-lobsters.csv\")\n\n# look at the data\nlobsters\n\n# A tibble: 18 × 3\n      id weight diet   \n   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  \n 1     1  152.  Mussels\n 2     2  132.  Mussels\n 3     3  104.  Mussels\n 4     4  154.  Mussels\n 5     5  132   Mussels\n 6     6  119   Mussels\n 7     7  162.  Mussels\n 8     8  118.  Pellets\n 9     9  111.  Pellets\n10    10  129.  Pellets\n11    11  110.  Pellets\n12    12  175.  Pellets\n13    13  102.  Flakes \n14    14  103.  Flakes \n15    15   90.4 Flakes \n16    16  133.  Flakes \n17    17  129.  Flakes \n18    18  129.  Flakes \n\n\nThe data have a unique id column, which we don’t need any summary statistics for, so we deselect it:\n\n# create some summary statistics\nlobsters %&gt;% \n  select(-id) %&gt;% \n  group_by(diet) %&gt;% \n  get_summary_stats(type = \"common\")\n\n# A tibble: 3 × 11\n  diet    variable     n   min   max median   iqr  mean    sd    se    ci\n  &lt;chr&gt;   &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Flakes  weight       6  90.4  133.   116.  27.3  114.  18.2  7.42  19.1\n2 Mussels weight       7 104.   162.   132.  27.0  136.  20.6  7.79  19.1\n3 Pellets weight       5 110.   175.   118.  17.8  128.  27.2 12.1   33.7\n\n\nNext, we visualise the data:\n\nlobsters %&gt;% \n  ggplot(aes(x = diet, y = weight)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\nlobsters_py = pd.read_csv(\"data/CS2-lobsters.csv\")\n\nNext, we visualise the data:\n\n(ggplot(lobsters_py,\n        aes(x = \"diet\",\n            y = \"weight\")) +\n     geom_boxplot())\n\n\n\n\n\n\n\n\n\n\n\nAs always we use the plot and summary to assess three things:\n\nDid we load the data in properly?\n\n\nWe see three groups with reasonable values. There aren’t any data points that are obviously wrong (negative, zero or massively big) and we have the right number of groups. So it looks as if we didn’t do anything obviously wrong.\n\n\nWhat do we expect as a result of a statistical test?\n\n\nWhilst the Mussels group does look higher than the other two groups, Pellets and Flakes appear almost identical in terms of average values, and there’s quite a bit of overlap with the Mussels group. A non-significant result is the most likely answer, and I would be surprised to see a significant p-value - especially given the small sample size that we have here.\n\n\nWhat do we think about assumptions?\n\n\nThe groups appear mainly symmetric (although Pellets is a bit weird) and so we’re not immediately massively worried about lack of normality. Again, Flakes and Mussels appear to have very similar variances but it’s a bit hard to decide what’s going on with Pellets. It’s hard to say what’s going on with the assumptions and so I’ll wait and see what the other tests say.\n\n\n\nExplore Assumptions\nWe’ll explore the assumption of normality and equality of variance, assuming that the data are independent.\n\nRPython\n\n\nNormality\nWe’ll be really thorough here and consider the normality of each group separately and jointly using the Shapiro-Wilk test, as well as looking at the Q-Q plot. In reality, and after these examples , we’ll only use the Q-Q plot.\nFirst, we perform the Shapiro-Wilk test on the individual groups:\n\n# Shapiro-Wilk on lobster groups\nlobsters %&gt;% \n    filter(diet == \"Flakes\") %&gt;% \n    pull(weight) %&gt;% \n    shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.84368, p-value = 0.1398\n\nlobsters %&gt;% \n    filter(diet == \"Mussels\") %&gt;% \n    pull(weight) %&gt;% \n    shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.94784, p-value = 0.71\n\nlobsters %&gt;% \n    filter(diet == \"Pellets\") %&gt;% \n    pull(weight) %&gt;% \n    shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.76706, p-value = 0.0425\n\n\nFlakes and Mussels are fine, but, as we suspected from earlier, Pellets appears to have a marginally significant Normality test result.\nLet’s look at the Shapiro-Wilk test for all of the data together:\n\n# create a linear model\nlm_lobsters &lt;- lm(weight ~ diet,\n                  data = lobsters)\n\n# extract the residuals\nresid_lobsters &lt;- residuals(lm_lobsters)\n\n# and perform the Shapiro-Wilk test on the residuals\nresid_lobsters %&gt;% \n  shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.94779, p-value = 0.3914\n\n\nThis on the other hand says that everything is fine. Let’s look at Q-Q plot.\n\n# Q-Q plots\nresid_panel(lm_lobsters,\n            plots = \"qq\")\n\n\n\n\n\n\n\n\nHere, I’ve used an extra argument to the normal diagnostic plots call. The default option is to plot 4 diagnostic plots. You can tell resid_panel() to only plot a specific one, using the plots = arguments. If you want to know more about this have a look at the help documentation or by using ?resid_panel.\nThe Q-Q plot looks OK, not perfect, but more than good enough for us to have confidence in the normality of the data.\nOverall, I’d be happy that the assumption of normality has been adequately well met here. The suggested lack of normality in the Pellets was only just significant and we have to take into account that there are only 5 data points in that group. If there had been a lot more points in that group, or if the Q-Q plot was considerably worse then I wouldn’t be confident.\nEquality of Variance\nWe’ll consider the Bartlett test and we’ll look at some diagnostic plots too.\n\n# perform Bartlett's test\nbartlett.test(weight ~ diet,\n              data = lobsters)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  weight by diet\nBartlett's K-squared = 0.71273, df = 2, p-value = 0.7002\n\n# plot the residuals and scale-location plots\nresid_panel(lm_lobsters,\n            plots = c(\"resid\", \"ls\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\n\nIn the above code I’ve specified which diagnostic plots I wanted. I have also added a smoother line (smoother = TRUE) to the plots.\n\nThe Residuals Plot. What we’re looking for there is that the points are evenly spread on either side of the line. Looks good.\nThe Location-Scale Plot (this is displayed by default in base R’s diagnostic plots). Here we’re looking at the red line. If that line is more or less horizontal, then the equality of variance assumption is met.\n\nHere all three methods agree that there aren’t any issues with equality of variance:\n\nthe Bartlett test p-value is large and non-significant\nthe spread of points in all three groups in the residuals vs fitted graph are roughly the same\nthe red line in the scale-location graph is pretty horizontal\n\nOverall, this assumption is pretty well met.\n\n\nNormality\nWe’ll be really thorough here and consider the normality of each group separately and jointly using the Shapiro-Wilk test, as well as looking at the Q-Q plot. In reality, and after these examples , we’ll only use the Q-Q plot.\nFirst, we perform the Shapiro-Wilk test on the individual groups:\n\n# Shapiro-Wilk on lobster groups\npg.normality(dv = \"weight\",\n             group = \"diet\",\n             data = lobsters_py)\n\n                W      pval  normal\nMussels  0.947836  0.709968    True\nPellets  0.767059  0.042495   False\nFlakes   0.843678  0.139796    True\n\n\nFlakes and Mussels are fine, but, as we suspected from earlier, Pellets appears to have a marginally significant Normality test result.\nLet’s look at the Shapiro-Wilk test for all of the data together:\n\n# create a linear model\nmodel = smf.ols(formula = \"weight ~ C(diet)\", data = lobsters_py)\n# and get the fitted parameters of the model\nlm_lobsters_py = model.fit()\n\n\n# get the residuals from the model fit\n# and perform Shapiro-Wilk test\npg.normality(lm_lobsters_py.resid)\n\n          W      pval  normal\n0  0.947787  0.391367    True\n\n\nThis on the other hand says that everything is fine. Let’s look at Q-Q plot.\n\n# Q-Q plots\n(ggplot(lobsters_py,\n        aes(sample = \"weight\")) +\n     stat_qq() +\n     stat_qq_line(colour = \"red\"))\n\n\n\n\n\n\n\n\nThe Q-Q plot looks OK, not perfect, but more than good enough for us to have confidence in the normality of the data.\nOverall, I’d be happy that the assumption of normality has been adequately well met here. The suggested lack of normality in the Pellets was only just significant and we have to take into account that there are only 5 data points in that group. If there had been a lot more points in that group, or if the Q-Q plot was considerably worse then I wouldn’t be confident.\nEquality of Variance\nWe’ll consider the Bartlett test and we’ll look at the diagnostic plots too.\n\npg.homoscedasticity(dv = \"weight\",\n                    group = \"diet\",\n                    method = \"bartlett\",\n                    data = lobsters_py)\n\n                 T      pval  equal_var\nbartlett  0.712733  0.700216       True\n\n\n\ndgplots(lm_lobsters_py)\n\n\n\n\n\n\n\n\n\n\nWe’ll just focus on the following:\n\nThe Residuals Plot. What we’re looking for there is that the points are evenly spread on either side of the line. Looks good.\nThe Location-Scale Plot (this is displayed by default in base R’s diagnostic plots). Here we’re looking at the red line. If that line is more or less horizontal, then the equality of variance assumption is met.\n\nHere all three methods agree that there aren’t any issues with equality of variance:\n\nthe Bartlett test p-value is large and non-significant\nthe spread of points in all three groups in the residuals vs fitted graph are roughly the same\nthe red line in the scale-location graph is pretty horizontal\n\nOverall, this assumption is pretty well met.\n\n\n\n\n\nCarry out one-way ANOVA\nWith our assumptions of normality and equality of variance met we can be confident that a one-way ANOVA is an appropriate test.\n\nRPython\n\n\n\nanova(lm_lobsters)\n\nAnalysis of Variance Table\n\nResponse: weight\n          Df Sum Sq Mean Sq F value Pr(&gt;F)\ndiet       2 1567.2  783.61  1.6432 0.2263\nResiduals 15 7153.1  476.87               \n\n\n\n\n\npg.anova(dv = \"weight\",\n         between = \"diet\",\n         data = lobsters_py,\n         detailed = True)\n\n   Source           SS  DF          MS        F     p-unc       np2\n0    diet  1567.229381   2  783.614690  1.64324  0.226313  0.179722\n1  Within  7153.075619  15  476.871708      NaN       NaN       NaN\n\n\n\n\n\n\nA one-way ANOVA test indicated that the mean weight of juvenile lobsters did not differ significantly between diets (p = 0.23).\n\n\n\nPost-hoc testing with Tukey\nIn this case we did not find any significant differences between the different diets. So that is a good time for me to reiterate that carrying out the post-hoc test after getting a non-significant result with ANOVA is something that you have to think very carefully about and it all depends on what your research question it.\nIf your research question was:\n\nDoes diet affect lobster weight?\n\nor\n\nIs there any effect of diet on lobster weight?\n\nThen when we got the non-significant result from the ANOVA test we should have just stopped there as we have our answer. Going digging for “significant” results by running more tests is a main factor that contributes towards lack of reproducibility in research.\nIf on the other hand your research question was:\n\nAre any specific diets better or worse for lobster weight than others?\n\nThen we should probably have just skipped the one-way ANOVA test entirely and just jumped straight in with the Tukey’s range test. The important point here is that the result of the one-way ANOVA test doesn’t stop you from carrying out the Tukey test - but it’s up to you to decide whether it is sensible.",
    "crumbs": [
      "Categorical predictors",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "materials/cs2_practical_anova.html#summary",
    "href": "materials/cs2_practical_anova.html#summary",
    "title": "10  ANOVA",
    "section": "10.10 Summary",
    "text": "10.10 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nWe use an ANOVA to test if there is a difference in means between multiple continuous response variables\nWe check assumptions with diagnostic plots and check if the residuals are normally distributed\nWe use post-hoc testing to check for significant differences between the group means, for example using Tukey’s range test",
    "crumbs": [
      "Categorical predictors",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "materials/cs2_practical_kruskal-wallis.html",
    "href": "materials/cs2_practical_kruskal-wallis.html",
    "title": "11  Kruskal-Wallis",
    "section": "",
    "text": "11.1 Purpose and aim\nThe Kruskal-Wallis one-way analysis of variance test is an analogue of ANOVA that can be used when the assumption of normality cannot be met. In this way it is an extension of the Mann-Whitney test for two groups.",
    "crumbs": [
      "Categorical predictors",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Kruskal-Wallis</span>"
    ]
  },
  {
    "objectID": "materials/cs2_practical_kruskal-wallis.html#libraries-and-functions",
    "href": "materials/cs2_practical_kruskal-wallis.html#libraries-and-functions",
    "title": "11  Kruskal-Wallis",
    "section": "11.2 Libraries and functions",
    "text": "11.2 Libraries and functions\n\n\n\n\n\n\nClick to expand\n\n\n\n\n\n\nRPython\n\n\n\n11.2.1 Libraries\n\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\n# Converts stats functions to a tidyverse-friendly format\nlibrary(rstatix)\n\n# Creates diagnostic plots using ggplot2\nlibrary(ggResidpanel)\n\n\n\n11.2.2 Functions\n\n# Performs a Kruskal-Wallis test\nstats::kruskal.test()\n\n# Performs Dunn's test for pairwise multiple comparisons of the ranked data\nrstatix::dunn_test()\n\n\n\n\n\n11.2.3 Libraries\n\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Simple yet exhaustive stats functions.\nimport pingouin as pg\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# Post-hoc tests\nimport scikit_posthocs as sp\n\n# Statistical models, conducting tests and statistical data exploration\nimport statsmodels.api as sm\n\n# Convenience interface for specifying models using formula strings and DataFrames\nimport statsmodels.formula.api as smf\n\n\n\n11.2.4 Functions\n\n# Summary statistics\npandas.DataFrame.describe()\n\n# Plots the first few rows of a DataFrame\npandas.DataFrame.head()\n\n# Query the columns of a DataFrame with a boolean expression\n#pandas.DataFrame.query()\n\n# Reads in a .csv file\npandas.read_csv\n\n# Performs an analysis of variance\n#pingouin.anova()\n\n# Tests for equality of variance\npingouin.homoscedasticity()\n\n# Performs the Kruskal-Wallis test\npingouin.kruskal\n\n# Performs the Shapiro-Wilk test for normality\npingouin.normality()\n\n# Creates a model from a formula and data frame\nstatsmodels.formula.api.ols\n\n# Creates an ANOVA table for one or more fitted linear models\nstatsmodels.stats.anova.anova_lm",
    "crumbs": [
      "Categorical predictors",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Kruskal-Wallis</span>"
    ]
  },
  {
    "objectID": "materials/cs2_practical_kruskal-wallis.html#data-and-hypotheses",
    "href": "materials/cs2_practical_kruskal-wallis.html#data-and-hypotheses",
    "title": "11  Kruskal-Wallis",
    "section": "11.3 Data and hypotheses",
    "text": "11.3 Data and hypotheses\nFor example, suppose a behavioural ecologist records the rate at which spider monkeys behaved aggressively towards one another, as a function of how closely related the monkeys are. The familiarity of the two monkeys involved in each interaction is classified as high, low or none. We want to test if the data support the hypothesis that aggression rates differ according to strength of relatedness. We form the following null and alternative hypotheses:\n\n\\(H_0\\): The median aggression rates for all types of familiarity are the same\n\\(H_1\\): The median aggression rates are not all equal\n\nWe will use a Kruskal-Wallis test to check this.\nThe data are stored in the file data/CS2-spidermonkey.csv.",
    "crumbs": [
      "Categorical predictors",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Kruskal-Wallis</span>"
    ]
  },
  {
    "objectID": "materials/cs2_practical_kruskal-wallis.html#summarise-and-visualise",
    "href": "materials/cs2_practical_kruskal-wallis.html#summarise-and-visualise",
    "title": "11  Kruskal-Wallis",
    "section": "11.4 Summarise and visualise",
    "text": "11.4 Summarise and visualise\n\nRPython\n\n\nFirst we read the data in:\n\nspidermonkey &lt;- read_csv(\"data/CS2-spidermonkey.csv\")\n\n\n# look at the data\nspidermonkey\n\n# A tibble: 21 × 3\n      id aggression familiarity\n   &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;      \n 1     1        0.2 high       \n 2     2        0.1 high       \n 3     3        0.4 high       \n 4     4        0.8 high       \n 5     5        0.3 high       \n 6     6        0.5 high       \n 7     7        0.2 high       \n 8     8        0.5 low        \n 9     9        0.4 low        \n10    10        0.3 low        \n# ℹ 11 more rows\n\n# summarise the data\nspidermonkey %&gt;% \n  select(-id) %&gt;% \n  group_by(familiarity) %&gt;% \n  get_summary_stats(type = \"common\")\n\n# A tibble: 3 × 11\n  familiarity variable       n   min   max median   iqr  mean    sd    se    ci\n  &lt;chr&gt;       &lt;fct&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 high        aggression     7   0.1   0.8    0.3  0.25 0.357 0.237 0.09  0.219\n2 low         aggression     7   0.3   1.2    0.5  0.3  0.629 0.315 0.119 0.291\n3 none        aggression     7   0.9   1.6    1.2  0.25 1.26  0.23  0.087 0.213\n\n# create boxplot\nggplot(spidermonkey,\n       aes(x = familiarity, y = aggression)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\nFirst we read the data in:\n\nspidermonkey_py = pd.read_csv(\"data/CS2-spidermonkey.csv\")\n\n\n# look at the data\nspidermonkey_py.head()\n\n   id  aggression familiarity\n0   1         0.2        high\n1   2         0.1        high\n2   3         0.4        high\n3   4         0.8        high\n4   5         0.3        high\n\n# summarise the data\nspidermonkey_py.describe()[\"aggression\"]\n\ncount    21.000000\nmean      0.747619\nstd       0.460021\nmin       0.100000\n25%       0.400000\n50%       0.600000\n75%       1.200000\nmax       1.600000\nName: aggression, dtype: float64\n\n\n\n# create boxplot\n(ggplot(spidermonkey_py,\n        aes(x = \"familiarity\",\n            y = \"aggression\")) +\n     geom_boxplot())\n\n\n\n\n\n\n\n\n\n\n\nThe data appear to show a very significant difference in aggression rates between the three types of familiarity. We would probably expect a reasonably significant result here.",
    "crumbs": [
      "Categorical predictors",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Kruskal-Wallis</span>"
    ]
  },
  {
    "objectID": "materials/cs2_practical_kruskal-wallis.html#assumptions",
    "href": "materials/cs2_practical_kruskal-wallis.html#assumptions",
    "title": "11  Kruskal-Wallis",
    "section": "11.5 Assumptions",
    "text": "11.5 Assumptions\nTo use the Kruskal-Wallis test we have to make three assumptions:\n\nThe parent distributions from which the samples are drawn have the same shape (if they’re normal then we should use a one-way ANOVA)\nEach data point in the samples is independent of the others\nThe parent distributions should have the same variance\n\nIndependence we’ll ignore as usual. Similar shape is best assessed from the earlier visualisation of the data. That means that we only need to check equality of variance.\n\n11.5.1 Equality of variance\nWe test for equality of variance using Levene’s test (since we can’t assume normal parent distributions which rules out Bartlett’s test).\n\nRPython\n\n\n\n# perform Levene's test\nlevene_test(aggression ~ familiarity,\n            data = spidermonkey)\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     2    18     0.114 0.893\n\n\nThe relevant p-value is given in the p column (0.893). As it is quite large we see that each group do appear to have the same variance.\nThere is also a warning about group coerced to factor. There is no need to worry about this - Levene’s test needs to compare different groups and because familiarity is encoded as a character value, it converts it to a categorical one before running the test.\n\n\nWe can run Levene’s test with the pg.homoscedasticity() function. We previously used this for Bartlett’s test, but it allows us to define Levene’s instead.\n\npg.homoscedasticity(dv = \"aggression\",\n                    group = \"familiarity\",\n                    method = \"levene\",\n                    data = spidermonkey_py)\n\n               W      pval  equal_var\nlevene  0.113924  0.892964       True",
    "crumbs": [
      "Categorical predictors",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Kruskal-Wallis</span>"
    ]
  },
  {
    "objectID": "materials/cs2_practical_kruskal-wallis.html#implement-and-interpret-the-test",
    "href": "materials/cs2_practical_kruskal-wallis.html#implement-and-interpret-the-test",
    "title": "11  Kruskal-Wallis",
    "section": "11.6 Implement and interpret the test",
    "text": "11.6 Implement and interpret the test\nPerform a Kruskal-Wallis test on the data:\n\nRPython\n\n\n\n# implement Kruskal-Wallis test\nkruskal.test(aggression ~ familiarity,\n             data = spidermonkey)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  aggression by familiarity\nKruskal-Wallis chi-squared = 13.597, df = 2, p-value = 0.001115\n\n\n\nThe first argument must be in the formula format: variable ~ category\nThe second argument must be the name of the data frame\n\nThe p-value is given in the 3rd line. This shows us the probability of getting samples such as ours if the null hypothesis were actually true.\n\n\nWe can use the kruskal() function from pingouin to perform the Kruskal-Wallis test:\n\npg.kruskal(dv = \"aggression\",\n           between = \"familiarity\",\n           data = spidermonkey_py)\n\n              Source  ddof1          H     p-unc\nKruskal  familiarity      2  13.597156  0.001115\n\n\n\n\n\nSince the p-value is very small (much smaller than the standard significance level of 0.05) we can say “that it is very unlikely that these three samples came from the same parent distribution and as such we can reject our null hypothesis” and state that:\n\nA Kruskal-Wallis test showed that aggression rates between spidermonkeys depends upon the degree of familiarity between them (p = 0.0011).",
    "crumbs": [
      "Categorical predictors",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Kruskal-Wallis</span>"
    ]
  },
  {
    "objectID": "materials/cs2_practical_kruskal-wallis.html#post-hoc-testing-dunns-test",
    "href": "materials/cs2_practical_kruskal-wallis.html#post-hoc-testing-dunns-test",
    "title": "11  Kruskal-Wallis",
    "section": "11.7 Post-hoc testing (Dunn’s test)",
    "text": "11.7 Post-hoc testing (Dunn’s test)\nThe equivalent of Tukey’s range test for non-normal data is Dunn’s test.\nDunn’s test is used to check for significant differences in group medians:\n\nRPython\n\n\nThe dunn_test() function comes from the rstatix package, so make sure you have that loaded.\n\n# perform Dunn's test\ndunn_test(aggression ~ familiarity,\n          data = spidermonkey)\n\n# A tibble: 3 × 9\n  .y.        group1 group2    n1    n2 statistic        p    p.adj p.adj.signif\n* &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;       \n1 aggression high   low        7     7      1.41 0.160    0.160    ns          \n2 aggression high   none       7     7      3.66 0.000257 0.000771 ***         \n3 aggression low    none       7     7      2.25 0.0245   0.0490   *           \n\n\nThe dunn_test() function performs a Kruskal-Wallis test on the data, followed by a post-hoc pairwise multiple comparison.\nThe comparison between the pairs of groups is reported in the table at the bottom. Each row contains a single comparison. We are interested in the p and p.adj columns, which contain the the p-values that we want. This table shows that there isn’t a significant difference between the high and low groups, as the p-value (0.1598) is too high. The other two comparisons between the high familiarity and no familiarity groups and between the low and no groups are significant though.\nThe dunn_test() function has several arguments, of which the p.adjust.method is likely to be of interest. Here you can define which method needs to be used to account for multiple comparisons. The default is \"none\". We’ll cover more about this in the chapter on Power analysis.\n\n\nUnfortunately pingouin does not seem to have function that can perform Dunn’s test, so we need to import this from elsewhere.\nThere is a series of post-hocs tests available via scikit_posthocs. You’ll need to install this by running:\n\npip install scikit-posthocs\n\nAfter installation, load it with:\n\nimport scikit_posthocs as sp\n\nFinally, we can perform Dunn’s test as follows:\n\nsp.posthoc_dunn(spidermonkey_py,\n                val_col = \"aggression\",\n                group_col = \"familiarity\")\n\n          high       low      none\nhigh  1.000000  0.159777  0.000257\nlow   0.159777  1.000000  0.024493\nnone  0.000257  0.024493  1.000000\n\n\nThe p-values of the pairwise comparisons are reported in the table. This table shows that there isn’t a significant difference between the high and low groups, as the p-value (0.1598) is too high. The other two comparisons between the high familiarity and no familiarity groups and between the low and no groups are significant though.\nThe sp.posthoc_dunn() function has several arguments, of which the p_adjust is likely to be of interest. Here you can define which method needs to be used to account for multiple comparisons. We’ll cover more about this in the chapter on Power analysis.",
    "crumbs": [
      "Categorical predictors",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Kruskal-Wallis</span>"
    ]
  },
  {
    "objectID": "materials/cs2_practical_kruskal-wallis.html#exercises",
    "href": "materials/cs2_practical_kruskal-wallis.html#exercises",
    "title": "11  Kruskal-Wallis",
    "section": "11.8 Exercises",
    "text": "11.8 Exercises\n\n11.8.1 Lobster weight (revisited)\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLevel: \nPerform a Kruskal-Wallis test and do a post-hoc test on the lobster data set.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n11.9 Answer\n\nHypothesis\n\n\\(H_0\\) : all medians are equal\n\\(H_1\\) : not all medians are equal\n\n\n\nImport data, summarise and visualise\nAll done previously.\n\n\nAssumptions\nFrom before, since the data are normal enough they are definitely similar enough for a Kruskal-Wallis test and they do all have equality of variance from out assessment of the diagnostic plots. For completeness though we will look at Levene’s test.\n\nRPython\n\n\n\nlevene_test(weight ~ diet,\n            data = lobsters)\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     2    15   0.00280 0.997\n\n\n\n\n\npg.homoscedasticity(dv = \"weight\",\n                    group = \"diet\",\n                    method = \"levene\",\n                    data = lobsters_py)\n\n               W      pval  equal_var\nlevene  0.002796  0.997209       True\n\n\n\n\n\nGiven that the p-value is so high, this again agrees with our previous assessment that the equality of variance assumption is well met. Rock on.\n\n\nKruskal-Wallis test\nSo, we perform the Kruskall-Wallis test.\n\nRPython\n\n\n\n# implement Kruskal-Wallis test\nkruskal.test(weight ~ diet,\n             data = lobsters)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  weight by diet\nKruskal-Wallis chi-squared = 3.2565, df = 2, p-value = 0.1963\n\n\n\n\n\npg.kruskal(dv = \"weight\",\n           between = \"diet\",\n           data = lobsters_py)\n\n        Source  ddof1         H     p-unc\nKruskal   diet      2  3.256475  0.196275\n\n\n\n\n\n\nA Kruskal-Wallis test indicated that the median weight of juvenile lobsters did not differ significantly between diets (p = 0.20).\n\n\n\nPost-hoc testing\nIn this case we should not be doing any post-hoc testing, because we did not detect any statistically significant differences. Doing so anyway and then reporting any incidental groups that would differ, would be p-hacking. And naughty.",
    "crumbs": [
      "Categorical predictors",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Kruskal-Wallis</span>"
    ]
  },
  {
    "objectID": "materials/cs2_practical_kruskal-wallis.html#answer",
    "href": "materials/cs2_practical_kruskal-wallis.html#answer",
    "title": "11  Kruskal-Wallis",
    "section": "11.9 Answer",
    "text": "11.9 Answer\n\nHypothesis\n\n\\(H_0\\) : all medians are equal\n\\(H_1\\) : not all medians are equal\n\n\n\nImport data, summarise and visualise\nAll done previously.\n\n\nAssumptions\nFrom before, since the data are normal enough they are definitely similar enough for a Kruskal-Wallis test and they do all have equality of variance from out assessment of the diagnostic plots. For completeness though we will look at Levene’s test.\n\nRPython\n\n\n\nlevene_test(weight ~ diet,\n            data = lobsters)\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     2    15   0.00280 0.997\n\n\n\n\n\npg.homoscedasticity(dv = \"weight\",\n                    group = \"diet\",\n                    method = \"levene\",\n                    data = lobsters_py)\n\n               W      pval  equal_var\nlevene  0.002796  0.997209       True\n\n\n\n\n\nGiven that the p-value is so high, this again agrees with our previous assessment that the equality of variance assumption is well met. Rock on.\n\n\nKruskal-Wallis test\nSo, we perform the Kruskall-Wallis test.\n\nRPython\n\n\n\n# implement Kruskal-Wallis test\nkruskal.test(weight ~ diet,\n             data = lobsters)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  weight by diet\nKruskal-Wallis chi-squared = 3.2565, df = 2, p-value = 0.1963\n\n\n\n\n\npg.kruskal(dv = \"weight\",\n           between = \"diet\",\n           data = lobsters_py)\n\n        Source  ddof1         H     p-unc\nKruskal   diet      2  3.256475  0.196275\n\n\n\n\n\n\nA Kruskal-Wallis test indicated that the median weight of juvenile lobsters did not differ significantly between diets (p = 0.20).\n\n\n\nPost-hoc testing\nIn this case we should not be doing any post-hoc testing, because we did not detect any statistically significant differences. Doing so anyway and then reporting any incidental groups that would differ, would be p-hacking. And naughty.",
    "crumbs": [
      "Categorical predictors",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Kruskal-Wallis</span>"
    ]
  },
  {
    "objectID": "materials/cs2_practical_kruskal-wallis.html#summary",
    "href": "materials/cs2_practical_kruskal-wallis.html#summary",
    "title": "11  Kruskal-Wallis",
    "section": "11.10 Summary",
    "text": "11.10 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nWe use a Kruskal-Wallis test to see if there is a difference in medians between multiple continuous response variables\nWe assume parent distributions have the same shape; each data point is independent and the parent distributions have the same variance\nWe test for equality of variance using Levene’s test\nPost-hoc testing to check for significant differences in the group medians is done with Dunn’s test",
    "crumbs": [
      "Categorical predictors",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Kruskal-Wallis</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_linear-regression.html",
    "href": "materials/cs3_practical_linear-regression.html",
    "title": "12  Linear regression",
    "section": "",
    "text": "12.1 Libraries and functions",
    "crumbs": [
      "Continuous predictors",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_linear-regression.html#libraries-and-functions",
    "href": "materials/cs3_practical_linear-regression.html#libraries-and-functions",
    "title": "12  Linear regression",
    "section": "",
    "text": "Click to expand\n\n\n\n\n\n\nRPython\n\n\n\n12.1.1 Libraries\n\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\n# Converts stats functions to a tidyverse-friendly format\nlibrary(rstatix)\n\n# Creates diagnostic plots using ggplot2\nlibrary(ggResidpanel)\n\n\n\n12.1.2 Functions\n\n# Creates diagnostic plots\nggResidpanel::resid_panel()\n\n# Creates a linear model\nstats::lm()\n\n# Creates an ANOVA table for a linear model\nstats::anova()\n\n\n\n\n\n12.1.3 Libraries\n\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Simple yet exhaustive stats functions.\nimport pingouin as pg\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# Statistical models, conducting tests and statistical data exploration\nimport statsmodels.api as sm\n\n# Convenience interface for specifying models using formula strings and DataFrames\nimport statsmodels.formula.api as smf\n\n\n\n12.1.4 Functions\n\n# Summary statistics\npandas.DataFrame.describe()\n\n# Plots the first few rows of a DataFrame\npandas.DataFrame.head()\n\n# Query the columns of a DataFrame with a boolean expression\npandas.DataFrame.query()\n\n# Reads in a .csv file\npandas.read_csv()\n\n# Creates a model from a formula and data frame\nstatsmodels.formula.api.ols()\n\n# Creates an ANOVA table for one or more fitted linear models\nstatsmodels.stats.anova.anova_lm()",
    "crumbs": [
      "Continuous predictors",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_linear-regression.html#purpose-and-aim",
    "href": "materials/cs3_practical_linear-regression.html#purpose-and-aim",
    "title": "12  Linear regression",
    "section": "12.2 Purpose and aim",
    "text": "12.2 Purpose and aim\nRegression analysis not only tests for an association between two or more variables, but also allows you to investigate quantitatively the nature of any relationship which is present. This can help you determine if one variable may be used to predict values of another. Simple linear regression essentially models the dependence of a scalar dependent variable (\\(y\\)) on an independent (or explanatory) variable (\\(x\\)) according to the relationship:\n\\[\\begin{equation*}\ny = \\beta_0 + \\beta_1 x\n\\end{equation*}\\]\nwhere \\(\\beta_0\\) is the value of the intercept and \\(\\beta_1\\) is the slope of the fitted line. A linear regression analysis assesses if the coefficient of the slope, \\(\\beta_1\\), is actually different from zero. If it is different from zero then we can say that \\(x\\) has a significant effect on \\(y\\) (since changing \\(x\\) leads to a predicted change in \\(y\\)). If it isn’t significantly different from zero, then we say that there isn’t sufficient evidence of such a relationship. To assess whether the slope is significantly different from zero we first need to calculate the values of \\(\\beta_0\\) and \\(\\beta_1\\).",
    "crumbs": [
      "Continuous predictors",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_linear-regression.html#data-and-hypotheses",
    "href": "materials/cs3_practical_linear-regression.html#data-and-hypotheses",
    "title": "12  Linear regression",
    "section": "12.3 Data and hypotheses",
    "text": "12.3 Data and hypotheses\nWe will perform a simple linear regression analysis on the two variables murder and assault from the USArrests data set. This rather bleak data set contains statistics on arrests per 100,000 residents for assault, murder and robbery in each of the 50 US states in 1973, alongside the proportion of the population who lived in urban areas at that time. We wish to determine whether the assault variable is a significant predictor of the murder variable. This means that we will need to find the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) that best fit the following macabre equation:\n\\[\\begin{equation*}\nMurder  = \\beta_0 + \\beta_1 \\times Assault\n\\end{equation*}\\]\nAnd then will be testing the following null and alternative hypotheses:\n\n\\(H_0\\): assault is not a significant predictor of murder, \\(\\beta_1 = 0\\)\n\\(H_1\\): assault is a significant predictor of murder, \\(\\beta_1 \\neq 0\\)",
    "crumbs": [
      "Continuous predictors",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_linear-regression.html#summarise-and-visualise",
    "href": "materials/cs3_practical_linear-regression.html#summarise-and-visualise",
    "title": "12  Linear regression",
    "section": "12.4 Summarise and visualise",
    "text": "12.4 Summarise and visualise\n\nRPython\n\n\nFirst, we read in the data:\n\nUSArrests &lt;- read_csv(\"data/CS3-usarrests.csv\")\n\nYou can visualise the data with:\n\n# create scatterplot of the data\nggplot(USArrests,\n       aes(x = assault, y = murder)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\nFirst, we read in the data:\n\nUSArrests_py = pd.read_csv(\"data/CS3-usarrests.csv\")\n\nYou can visualise the data with:\n\n# create scatterplot of the data\n(ggplot(USArrests_py,\n         aes(x = \"assault\",\n             y = \"murder\")) +\n     geom_point())\n\n\n\n\n\n\n\n\n\n\n\nPerhaps unsurprisingly, there appears to be a relatively strong positive relationship between these two variables. Whilst there is a reasonable scatter of the points around any trend line, we would probably expect a significant result in this case.",
    "crumbs": [
      "Continuous predictors",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_linear-regression.html#assumptions",
    "href": "materials/cs3_practical_linear-regression.html#assumptions",
    "title": "12  Linear regression",
    "section": "12.5 Assumptions",
    "text": "12.5 Assumptions\nIn order for a linear regression analysis to be valid 4 key assumptions need to be met:\n\n\n\n\n\n\nImportant\n\n\n\n\nThe data must be linear (it is entirely possible to calculate a straight line through data that is not straight - it doesn’t mean that you should!)\nThe residuals must be normally distributed\nThe residuals must not be correlated with their fitted values (i.e. they should be independent)\nThe fit should not depend overly much on a single point (no point should have high leverage).\n\n\n\nWhether these assumptions are met can easily be checked visually by producing four key diagnostic plots.\n\nRPython\n\n\nFirst we need to define the linear model:\n\nlm_1 &lt;- lm(murder ~ assault,\n           data = USArrests)\n\n\nThe first argument to lm is a formula saying that murder depends on assault. As we have seen before, the syntax is generally dependent variable ~ independent variable.\nThe second argument specifies which data to use.\n\nNext, we can create diagnostic plots for the model:\n\nresid_panel(lm_1,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\n\n\nThe top left graph plots the Residuals plot. If the data are best explained by a straight line then there should be a uniform distribution of points above and below the horizontal blue line (and if there are sufficient points then the red line, which is a smoother line, should be on top of the blue line). This plot is pretty good.\nThe top right graph shows the Q-Q plot which allows a visual inspection of normality. If the residuals are normally distributed, then the points should lie on the diagonal dotted line. This isn’t too bad but there is some slight snaking towards the upper end and there appears to be an outlier.\nThe bottom left Location-scale graph allows us to investigate whether there is any correlation between the residuals and the predicted values and whether the variance of the residuals changes significantly. If not, then the red line should be horizontal. If there is any correlation or change in variance then the red line will not be horizontal. This plot is fine.\nThe last graph shows the Cook’s distance and tests if any one point has an unnecessarily large effect on the fit. The important aspect here is to see if any points are larger than 0.5 (meaning you’d have to be careful) or 1.0 (meaning you’d definitely have to check if that point has an large effect on the model). If not, then no point has undue influence. This plot is good.\n\n\n\nIf you haven’t loaded statsmodels yet, run the following:\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nNext, we create a linear model and get the .fit():\n\n# create a linear model\nmodel = smf.ols(formula= \"murder ~ assault\", data = USArrests_py)\n# and get the fitted parameters of the model\nlm_USArrests_py = model.fit()\n\nThen we use dgplots() to create the diagnostic plots:\n\ndgplots(lm_USArrests_py)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFormally, if there is any concern after looking at the diagnostic plots then a linear regression is not valid. However, disappointingly, very few people ever check whether the linear regression assumptions have been met before quoting the results.\nLet’s change this through leading by example!",
    "crumbs": [
      "Continuous predictors",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_linear-regression.html#implement-and-interpret-test",
    "href": "materials/cs3_practical_linear-regression.html#implement-and-interpret-test",
    "title": "12  Linear regression",
    "section": "12.6 Implement and interpret test",
    "text": "12.6 Implement and interpret test\nWe have already defined the linear model, so we can have a closer look at it:\n\nRPython\n\n\n\n# show the linear model\nlm_1\n\n\nCall:\nlm(formula = murder ~ assault, data = USArrests)\n\nCoefficients:\n(Intercept)      assault  \n    0.63168      0.04191  \n\n\nThe lm() function returns a linear model object which is essentially a list containing everything necessary to understand and analyse a linear model. However, if we just type the model name (as we have above) then it just prints to the screen the actual coefficients of the model i.e. the intercept and the slope of the line.\n\n\n\n\n\n\nThe linear model object: would you like to know more?\n\n\n\n\n\nIf you wanted to know more about the lm object we created, then type in:\n\nView(lm_1)\n\nThis shows a list (a type of object in R), containing all of the information associated with the linear model. The most relevant ones at the moment are:\n\ncoefficients contains the values of the coefficients we found earlier.\nresiduals contains the residual associated for each individual data point.\nfitted.values contains the values that the linear model predicts for each individual data point.\n\n\n\n\n\n\n\nprint(lm_USArrests_py.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 murder   R-squared:                       0.643\nModel:                            OLS   Adj. R-squared:                  0.636\nMethod:                 Least Squares   F-statistic:                     86.45\nDate:                Tue, 16 Apr 2024   Prob (F-statistic):           2.60e-12\nTime:                        07:42:51   Log-Likelihood:                -118.26\nNo. Observations:                  50   AIC:                             240.5\nDf Residuals:                      48   BIC:                             244.4\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.6317      0.855      0.739      0.464      -1.087       2.350\nassault        0.0419      0.005      9.298      0.000       0.033       0.051\n==============================================================================\nOmnibus:                        4.799   Durbin-Watson:                   1.796\nProb(Omnibus):                  0.091   Jarque-Bera (JB):                3.673\nSkew:                           0.598   Prob(JB):                        0.159\nKurtosis:                       3.576   Cond. No.                         436.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nA rather large table, but the values we’re interested in can be found more or less in the middle. We are after the coef values, where the intercept is 0.6317 and the slope is 0.0419.\n\n\n\nSo here we have found that the line of best fit is given by:\n\\[\\begin{equation*}\nMurder = 0.63 + 0.042 \\times Assault\n\\end{equation*}\\]\nNext we can assess whether the slope is significantly different from zero:\n\nRPython\n\n\n\nanova(lm_1)\n\nAnalysis of Variance Table\n\nResponse: murder\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nassault    1 597.70  597.70  86.454 2.596e-12 ***\nResiduals 48 331.85    6.91                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere, we again use the anova() command to assess significance. This shouldn’t be too surprising at this stage if the introductory lectures made any sense. From a mathematical perspective, one-way ANOVA and simple linear regression are exactly the same as each other and it makes sense that we should use the same command to analyse them in R.\nThis is exactly the same format as the table we saw for one-way ANOVA:\n\nThe 1st line just tells you the that this is an ANOVA test\nThe 2nd line tells you what the response variable is (in this case Murder)\nThe 3rd, 4th and 5th lines are an ANOVA table which contain some useful values:\n\nThe Df column contains the degrees of freedom values on each row, 1 and 48\nThe F value column contains the F statistic, 86.454\nThe p-value is 2.596e-12 and is the number directly under the Pr(&gt;F) on the 4th line.\nThe other values in the table (in the Sum Sq and Mean Sq) column are used to calculate the F statistic itself and we don’t need to know these.\n\n\n\n\nWe can perform an ANOVA on the lm_USArrests_py object using the anova_lm() function from the statsmodels package.\n\nsm.stats.anova_lm(lm_USArrests_py, typ = 2)\n\n              sum_sq    df          F        PR(&gt;F)\nassault   597.703202   1.0  86.454086  2.595761e-12\nResidual  331.849598  48.0        NaN           NaN\n\n\n\n\n\nAgain, the p-value is what we’re most interested in here and shows us the probability of getting data such as ours if the null hypothesis were actually true and the slope of the line were actually zero. Since the p-value is excruciatingly tiny we can reject our null hypothesis and state that:\n\nA simple linear regression showed that the assault rate in US states was a significant predictor of the number of murders (p = 2.59x10-12).\n\n\n12.6.1 Plotting the regression line\nIt can be very helpful to plot the regression line with the original data to see how far the data are from the predicted linear values. We can do this as follows:\n\nRPython\n\n\n\n# plot the data\nggplot(USArrests,\n       aes(x = assault, y = murder)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\nWe plot all the data using geom_point()\nNext, we add the linear model using geom_smooth(method = \"lm\"), hiding the confidence intervals (se = FALSE)\n\n\n\n\n(ggplot(USArrests_py,\n        aes(x = \"assault\", y = \"murder\")) +\n     geom_point() +\n     geom_smooth(method = \"lm\",\n                 se = False,\n                 colour = \"blue\"))",
    "crumbs": [
      "Continuous predictors",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_linear-regression.html#exercises",
    "href": "materials/cs3_practical_linear-regression.html#exercises",
    "title": "12  Linear regression",
    "section": "12.7 Exercises",
    "text": "12.7 Exercises\n\n12.7.1 State data: Life expectancy and murder\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLevel: \nWe will use the data from the file data/CS3-statedata.csv data set for this exercise. This rather more benign data set contains information on more general properties of each US state, such as population (1975), per capita income (1974), illiteracy proportion (1970), life expectancy (1969), murder rate per 100,000 people (there’s no getting away from it), percentage of the population who are high-school graduates, average number of days where the minimum temperature is below freezing between 1931 and 1960, and the state area in square miles. The data set contains 50 rows and 8 columns, with column names: population, income, illiteracy, life_exp, murder, hs_grad, frost and area.\nPerform a linear regression on the variables life_exp and murder and do the following:\n\nFind the value of the slope and intercept coefficients for both regressions\nDetermine if the slope is significantly different from zero (i.e. is there a relationship between the two variables)\nProduce a scatter plot of the data with the line of best fit superimposed on top.\nProduce diagnostic plots and discuss with your (virtual) neighbour if you should have carried out a simple linear regression in each case\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n12.8 Answer\n\nLoad and visualise the data\n\nRPython\n\n\nFirst, we read in the data:\n\nUSAstate &lt;- read_csv(\"data/CS3-statedata.csv\")\n\nNext, we visualise the murder variable against the life_exp variable. We also add a regression line.\n\n# plot the data and add the regression line\nggplot(USAstate,\n       aes(x = murder, y = life_exp)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\n\nFirst, we read in the data:\n\nUSAstate_py = pd.read_csv(\"data/CS3-statedata.csv\")\n\nNext, we visualise the murder variable against the life_exp variable. We also add a regression line.\n\n(ggplot(USAstate_py,\n        aes(x = \"life_exp\", y = \"murder\")) +\n     geom_point() +\n     geom_smooth(method = \"lm\",\n                 se = False,\n                 colour = \"blue\"))\n\n\n\n\n\n\n\n\n\n\n\nWe visualise for the same reasons as before:\n\nWe check that the data aren’t obviously wrong. Here we have sensible values for life expectancy (nothing massively large or small), and plausible values for murder rates (not that I’m that au fait with US murder rates in 1973 but small positive numbers seem plausible).\nWe check to see what we would expect from the statistical analysis. Here there does appear to be a reasonable downward trend to the data. I would be surprised if we didn’t get a significant result given the amount of data and the spread of the data about the line\nWe check the assumptions (only roughly though as we’ll be doing this properly in a minute). Nothing immediately gives me cause for concern; the data appear linear, the spread of the data around the line appears homogeneous and symmetrical. No outliers either.\n\n\n\nCheck assumptions\nNow, let’s check the assumptions with the diagnostic plots.\n\nRPython\n\n\n\n# create a linear model\nlm_murder &lt;- lm(life_exp ~ murder,\n           data = USAstate)\n\n# create the diagnostic plots\nresid_panel(lm_murder,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\n\nThe Residuals plot appears symmetric enough (similar distribution of points above and below the horizontal blue line) for me to happy with linearity. Similarly the red line in the Location-Scale plot looks horizontal enough for me to be happy with homogeneity of variance. There aren’t any influential points in the Cook’s distance plot. The only plot that does give me a bit of concern is the Q-Q plot. Here we see clear evidence of snaking, although the degree of snaking isn’t actually that bad. This just means that we can be pretty certain that the distribution of residuals isn’t normal, but also that it isn’t very non-normal.\n\n\nFirst, we create a linear model:\n\n# create a linear model\nmodel = smf.ols(formula= \"life_exp ~ murder\", data = USAstate_py)\n# and get the fitted parameters of the model\nlm_USAstate_py = model.fit()\n\nNext, we can create the diagnostic plots:\n\ndgplots(lm_USAstate_py)\n\n\n\n\n\n\n\n\n\n\nThe Residuals plot appears symmetric enough (similar distribution of points above and below the horizontal blue line) for me to happy with linearity. Similarly the red line in the Location-Scale plot looks horizontal enough for me to be happy with homogeneity of variance. There aren’t any influential points in the Influential points plot. The only plot that does give me a bit of concern is the Q-Q plot. Here we see clear evidence of snaking, although the degree of snaking isn’t actually that bad. This just means that we can be pretty certain that the distribution of residuals isn’t normal, but also that it isn’t very non-normal.\n\n\n\nWhat do we do in this situation? Well, there are three possible options:\n\nAppeal to the Central Limit Theorem. This states that if we have a large enough sample size we don’t have to worry about whether the distribution of the residuals are normally distributed. Large enough is a bit of a moving target here and to be honest it depends on how non-normal the underlying data are. If the data are only a little bit non-normal then we can get away with using a smaller sample than if the data are massively skewed (for example). This is not an exact science, but anything over 30 data points is considered a lot for mild to moderate non-normality (as we have in this case). If the data were very skewed then we would be looking for more data points (50-100). So, for this example we can legitimately just carry on with our analysis without worrying.\nTry transforming the data. Here we would try applying some mathematical functions to the response variable (life_exp) in the hope that repeating the analysis with this transformed variable would make things better. To be honest with you it might not work and we won’t know until we try. Dealing with transformed variables is legitimate as an approach but it can make interpreting the model a bit more challenging. In this particular example none of the traditional transformations (log, square-root, reciprocal) do anything to fix the slight lack of normality.\nGo with permutation methods / bootstrapping. This approach would definitely work. I don’t have time to explain it here (it’s the subject of an entire other practical). This approach also requires us to have a reasonably large sample size to work well as we have to assume that the distribution of the sample is a good approximation for the distribution of the entire data set.\n\nSo in this case, because we have a large enough sample size and our deviation from normality isn’t too bad, we can just crack on with the standard analysis.\n\n\nImplement and interpret test\nSo, let’s actually do the analysis:\n\nRPython\n\n\n\nanova(lm_murder)\n\nAnalysis of Variance Table\n\nResponse: life_exp\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nmurder     1 53.838  53.838  74.989 2.26e-11 ***\nResiduals 48 34.461   0.718                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nsm.stats.anova_lm(lm_USAstate_py, typ = 2)\n\n             sum_sq    df          F        PR(&gt;F)\nmurder    53.837675   1.0  74.988652  2.260070e-11\nResidual  34.461327  48.0        NaN           NaN\n\n\n\n\n\nAnd after all of that we find that the murder rate is a statistically significant predictor of life expectancy in US states. Woohoo!\n\n\n\n\n\n\n\n\n\n\n\n\n\n12.8.1 State data: Graduation and frost days\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLevel: \nNow let’s investigate the relationship between the proportion of High School Graduates a state has (hs_grad) and the mean number of days below freezing (frost) within each state.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n12.9 Answer\nWe’ll run through this a bit quicker:\n\nRPython\n\n\n\n# plot the data\nggplot(USAstate,\n       aes(x = frost, y = hs_grad)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n(ggplot(USAstate_py,\n        aes(x = \"hs_grad\", y = \"frost\")) +\n     geom_point() +\n     geom_smooth(method = \"lm\",\n                 se = False,\n                 colour = \"blue\"))\n\n\n\n\n\n\n\n\n\n\n\nOnce again, we look at the data.\n\nThere doesn’t appear to be any ridiculous errors with the data; High School graduation proportions are in the 0-100% range and the mean number of sub-zero days for each state are between 0 and 365, so these numbers are plausible.\nWhilst there is a trend upwards, which wouldn’t surprise me if it came back as being significant, I’m a bit concerned about…\nThe assumptions. I’m mainly concerned that the data aren’t very linear. There appears to be a noticeable pattern to the data with some sort of minimum around 50-60 Frost days. This means that it’s hard to assess the other assumptions.\n\nLet’s check these out properly\nNow, let’s check the assumptions with the diagnostic plots.\n\nRPython\n\n\n\n# create a linear model\nlm_frost &lt;- lm(hs_grad ~ frost,\n               data = USAstate)\n\n# create the diagnostic plots\nresid_panel(lm_frost,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\n\n\n\nFirst, we create a linear model:\n\n# create a linear model\nmodel = smf.ols(formula= \"hs_grad ~ frost\", data = USAstate_py)\n# and get the fitted parameters of the model\nlm_frost_py = model.fit()\n\nNext, we can create the diagnostic plots:\n\ndgplots(lm_frost_py)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that what we suspected from before is backed up by the residuals plot. The data aren’t linear and there appears to be some sort of odd down-up pattern here. Given the lack of linearity it just isn’t worth worrying about the other plots because our model is misspecified: a straight line just doesn’t represent our data at all.\nJust for reference, and as practice for looking at diagnostic plots, if we ignore the lack of linearity then we can say that\n\nNormality is pretty good from the Q-Q plot\nHomogeneity of variance isn’t very good and there appears to be a noticeable drop in variance as we go from left to right (from consideration of the Location-Scale plot)\nThere don’t appear to be any influential points (by looking at the Cook’s distance plot)\n\nHowever, none of that is relevant in this particular case since the data aren’t linear and a straight line would be the wrong model to fit.\nSo what do we do in this situation?\nWell actually, this is a bit tricky as there aren’t any easy fixes here. There are two broad solutions for dealing with a misspecified model.\n\nThe most common solution is that we need more predictor variables in the model. Here we’re trying to explain/predict high school graduation only using the number of frost days. Obviously there are many more things that would affect the proportion of high school graduates than just how cold it is in a State (which is a weird potential predictor when you think about it) and so what we would need is a statistical approach that allows us to look at multiple predictor variables. We’ll cover that approach in the next two sessions.\nThe other potential solution is to say that high school graduation can in fact be predicted only by the number of frost days but that the relationship between them isn’t linear. We would then need to specify a relationship (a curve basically) and then try to fit the data to the new, non-linear, curve. This process is called, unsurprisingly, non-linear regression and we don’t cover that in this course. This process is best used when there is already a strong theoretical reason for a non-linear relationship between two variables (such as sigmoidal dose-response curves in pharmacology or exponential relationships in cell growth). In this case we don’t have any such preconceived notions and so it wouldn’t really be appropriate in this case.\n\nNeither of these solutions can be tackled with the knowledge that we have so far in the course but we can definitely say that based upon this data set, there isn’t a linear relationship (significant or otherwise) between frosty days and high school graduation rates.",
    "crumbs": [
      "Continuous predictors",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_linear-regression.html#answer",
    "href": "materials/cs3_practical_linear-regression.html#answer",
    "title": "12  Linear regression",
    "section": "12.8 Answer",
    "text": "12.8 Answer\n\nLoad and visualise the data\n\nRPython\n\n\nFirst, we read in the data:\n\nUSAstate &lt;- read_csv(\"data/CS3-statedata.csv\")\n\nNext, we visualise the murder variable against the life_exp variable. We also add a regression line.\n\n# plot the data and add the regression line\nggplot(USAstate,\n       aes(x = murder, y = life_exp)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\n\nFirst, we read in the data:\n\nUSAstate_py = pd.read_csv(\"data/CS3-statedata.csv\")\n\nNext, we visualise the murder variable against the life_exp variable. We also add a regression line.\n\n(ggplot(USAstate_py,\n        aes(x = \"life_exp\", y = \"murder\")) +\n     geom_point() +\n     geom_smooth(method = \"lm\",\n                 se = False,\n                 colour = \"blue\"))\n\n\n\n\n\n\n\n\n\n\n\nWe visualise for the same reasons as before:\n\nWe check that the data aren’t obviously wrong. Here we have sensible values for life expectancy (nothing massively large or small), and plausible values for murder rates (not that I’m that au fait with US murder rates in 1973 but small positive numbers seem plausible).\nWe check to see what we would expect from the statistical analysis. Here there does appear to be a reasonable downward trend to the data. I would be surprised if we didn’t get a significant result given the amount of data and the spread of the data about the line\nWe check the assumptions (only roughly though as we’ll be doing this properly in a minute). Nothing immediately gives me cause for concern; the data appear linear, the spread of the data around the line appears homogeneous and symmetrical. No outliers either.\n\n\n\nCheck assumptions\nNow, let’s check the assumptions with the diagnostic plots.\n\nRPython\n\n\n\n# create a linear model\nlm_murder &lt;- lm(life_exp ~ murder,\n           data = USAstate)\n\n# create the diagnostic plots\nresid_panel(lm_murder,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\n\nThe Residuals plot appears symmetric enough (similar distribution of points above and below the horizontal blue line) for me to happy with linearity. Similarly the red line in the Location-Scale plot looks horizontal enough for me to be happy with homogeneity of variance. There aren’t any influential points in the Cook’s distance plot. The only plot that does give me a bit of concern is the Q-Q plot. Here we see clear evidence of snaking, although the degree of snaking isn’t actually that bad. This just means that we can be pretty certain that the distribution of residuals isn’t normal, but also that it isn’t very non-normal.\n\n\nFirst, we create a linear model:\n\n# create a linear model\nmodel = smf.ols(formula= \"life_exp ~ murder\", data = USAstate_py)\n# and get the fitted parameters of the model\nlm_USAstate_py = model.fit()\n\nNext, we can create the diagnostic plots:\n\ndgplots(lm_USAstate_py)\n\n\n\n\n\n\n\n\n\n\nThe Residuals plot appears symmetric enough (similar distribution of points above and below the horizontal blue line) for me to happy with linearity. Similarly the red line in the Location-Scale plot looks horizontal enough for me to be happy with homogeneity of variance. There aren’t any influential points in the Influential points plot. The only plot that does give me a bit of concern is the Q-Q plot. Here we see clear evidence of snaking, although the degree of snaking isn’t actually that bad. This just means that we can be pretty certain that the distribution of residuals isn’t normal, but also that it isn’t very non-normal.\n\n\n\nWhat do we do in this situation? Well, there are three possible options:\n\nAppeal to the Central Limit Theorem. This states that if we have a large enough sample size we don’t have to worry about whether the distribution of the residuals are normally distributed. Large enough is a bit of a moving target here and to be honest it depends on how non-normal the underlying data are. If the data are only a little bit non-normal then we can get away with using a smaller sample than if the data are massively skewed (for example). This is not an exact science, but anything over 30 data points is considered a lot for mild to moderate non-normality (as we have in this case). If the data were very skewed then we would be looking for more data points (50-100). So, for this example we can legitimately just carry on with our analysis without worrying.\nTry transforming the data. Here we would try applying some mathematical functions to the response variable (life_exp) in the hope that repeating the analysis with this transformed variable would make things better. To be honest with you it might not work and we won’t know until we try. Dealing with transformed variables is legitimate as an approach but it can make interpreting the model a bit more challenging. In this particular example none of the traditional transformations (log, square-root, reciprocal) do anything to fix the slight lack of normality.\nGo with permutation methods / bootstrapping. This approach would definitely work. I don’t have time to explain it here (it’s the subject of an entire other practical). This approach also requires us to have a reasonably large sample size to work well as we have to assume that the distribution of the sample is a good approximation for the distribution of the entire data set.\n\nSo in this case, because we have a large enough sample size and our deviation from normality isn’t too bad, we can just crack on with the standard analysis.\n\n\nImplement and interpret test\nSo, let’s actually do the analysis:\n\nRPython\n\n\n\nanova(lm_murder)\n\nAnalysis of Variance Table\n\nResponse: life_exp\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nmurder     1 53.838  53.838  74.989 2.26e-11 ***\nResiduals 48 34.461   0.718                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nsm.stats.anova_lm(lm_USAstate_py, typ = 2)\n\n             sum_sq    df          F        PR(&gt;F)\nmurder    53.837675   1.0  74.988652  2.260070e-11\nResidual  34.461327  48.0        NaN           NaN\n\n\n\n\n\nAnd after all of that we find that the murder rate is a statistically significant predictor of life expectancy in US states. Woohoo!",
    "crumbs": [
      "Continuous predictors",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_linear-regression.html#answer-1",
    "href": "materials/cs3_practical_linear-regression.html#answer-1",
    "title": "12  Linear regression",
    "section": "12.9 Answer",
    "text": "12.9 Answer\nWe’ll run through this a bit quicker:\n\nRPython\n\n\n\n# plot the data\nggplot(USAstate,\n       aes(x = frost, y = hs_grad)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n(ggplot(USAstate_py,\n        aes(x = \"hs_grad\", y = \"frost\")) +\n     geom_point() +\n     geom_smooth(method = \"lm\",\n                 se = False,\n                 colour = \"blue\"))\n\n\n\n\n\n\n\n\n\n\n\nOnce again, we look at the data.\n\nThere doesn’t appear to be any ridiculous errors with the data; High School graduation proportions are in the 0-100% range and the mean number of sub-zero days for each state are between 0 and 365, so these numbers are plausible.\nWhilst there is a trend upwards, which wouldn’t surprise me if it came back as being significant, I’m a bit concerned about…\nThe assumptions. I’m mainly concerned that the data aren’t very linear. There appears to be a noticeable pattern to the data with some sort of minimum around 50-60 Frost days. This means that it’s hard to assess the other assumptions.\n\nLet’s check these out properly\nNow, let’s check the assumptions with the diagnostic plots.\n\nRPython\n\n\n\n# create a linear model\nlm_frost &lt;- lm(hs_grad ~ frost,\n               data = USAstate)\n\n# create the diagnostic plots\nresid_panel(lm_frost,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\n\n\n\nFirst, we create a linear model:\n\n# create a linear model\nmodel = smf.ols(formula= \"hs_grad ~ frost\", data = USAstate_py)\n# and get the fitted parameters of the model\nlm_frost_py = model.fit()\n\nNext, we can create the diagnostic plots:\n\ndgplots(lm_frost_py)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that what we suspected from before is backed up by the residuals plot. The data aren’t linear and there appears to be some sort of odd down-up pattern here. Given the lack of linearity it just isn’t worth worrying about the other plots because our model is misspecified: a straight line just doesn’t represent our data at all.\nJust for reference, and as practice for looking at diagnostic plots, if we ignore the lack of linearity then we can say that\n\nNormality is pretty good from the Q-Q plot\nHomogeneity of variance isn’t very good and there appears to be a noticeable drop in variance as we go from left to right (from consideration of the Location-Scale plot)\nThere don’t appear to be any influential points (by looking at the Cook’s distance plot)\n\nHowever, none of that is relevant in this particular case since the data aren’t linear and a straight line would be the wrong model to fit.\nSo what do we do in this situation?\nWell actually, this is a bit tricky as there aren’t any easy fixes here. There are two broad solutions for dealing with a misspecified model.\n\nThe most common solution is that we need more predictor variables in the model. Here we’re trying to explain/predict high school graduation only using the number of frost days. Obviously there are many more things that would affect the proportion of high school graduates than just how cold it is in a State (which is a weird potential predictor when you think about it) and so what we would need is a statistical approach that allows us to look at multiple predictor variables. We’ll cover that approach in the next two sessions.\nThe other potential solution is to say that high school graduation can in fact be predicted only by the number of frost days but that the relationship between them isn’t linear. We would then need to specify a relationship (a curve basically) and then try to fit the data to the new, non-linear, curve. This process is called, unsurprisingly, non-linear regression and we don’t cover that in this course. This process is best used when there is already a strong theoretical reason for a non-linear relationship between two variables (such as sigmoidal dose-response curves in pharmacology or exponential relationships in cell growth). In this case we don’t have any such preconceived notions and so it wouldn’t really be appropriate in this case.\n\nNeither of these solutions can be tackled with the knowledge that we have so far in the course but we can definitely say that based upon this data set, there isn’t a linear relationship (significant or otherwise) between frosty days and high school graduation rates.",
    "crumbs": [
      "Continuous predictors",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_linear-regression.html#summary",
    "href": "materials/cs3_practical_linear-regression.html#summary",
    "title": "12  Linear regression",
    "section": "12.10 Summary",
    "text": "12.10 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nLinear regression tests if a linear relationship exists between two or more variables\nIf so, we can use one variable to predict another\nA linear model has an intercept and slope and we test if the slope differs from zero\nWe create linear models and perform an ANOVA to assess the slope coefficient\nWe can only use a linear regression if these four assumptions are met:\n\nThe data are linear\nResiduals are normally distributed\nResiduals are not correlated with their fitted values\nNo single point should have a large influence on the linear model\n\nWe can use diagnostic plots to evaluate these assumptions",
    "crumbs": [
      "Continuous predictors",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_correlations.html",
    "href": "materials/cs3_practical_correlations.html",
    "title": "13  Correlations",
    "section": "",
    "text": "13.1 Libraries and functions",
    "crumbs": [
      "Continuous predictors",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_correlations.html#libraries-and-functions",
    "href": "materials/cs3_practical_correlations.html#libraries-and-functions",
    "title": "13  Correlations",
    "section": "",
    "text": "Click to expand\n\n\n\n\n\n\nRPython\n\n\n\n13.1.1 Libraries\n\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\n\n\n13.1.2 Functions\n\n# Computes the absolute value\nbase::abs()\n\n# Creates a matrix of scatter plots\ngraphics::pairs()\n\n# Computes a correlation matrix\nstats::cor()\n\n# Creates a heat map\nstats::heatmap()\n\n# Turns object into tibble\ntibble::as.tibble()\n\n# Lengthens the data\ntidyr::pivot_longer()\n\n\n\n\n\n13.1.3 Libraries\n\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n\n\n13.1.4 Functions\n\n# Compute pairwise correlation of columns\npandas.DataFrame.corr()\n\n# Plots the first few rows of a DataFrame\npandas.DataFrame.head()\n\n# Query the columns of a DataFrame with a boolean expression\npandas.DataFrame.query()\n\n# Set the name of the axis for the index or columns\npandas.DataFrame.rename_axis()\n\n# Unpivot a DataFrame from wide to long format\npandas.melt()\n\n# Reads in a .csv file\npandas.read_csv()",
    "crumbs": [
      "Continuous predictors",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_correlations.html#purpose-and-aim",
    "href": "materials/cs3_practical_correlations.html#purpose-and-aim",
    "title": "13  Correlations",
    "section": "13.2 Purpose and aim",
    "text": "13.2 Purpose and aim\nCorrelation refers to the relationship of two variables (or data sets) to one another. Two data sets are said to be correlated if they are not independent from one another. Correlations can be useful because they can indicate if a predictive relationship may exist. However just because two data sets are correlated does not mean that they are causally related.",
    "crumbs": [
      "Continuous predictors",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_correlations.html#data-and-hypotheses",
    "href": "materials/cs3_practical_correlations.html#data-and-hypotheses",
    "title": "13  Correlations",
    "section": "13.3 Data and hypotheses",
    "text": "13.3 Data and hypotheses\nWe will use the USArrests data set for this example. This rather bleak data set contains statistics in arrests per 100,000 residents for assault, murder and robbery in each of the 50 US states in 1973, alongside the proportion of the population who lived in urban areas at that time. USArrests is a data frame with 50 observations of five variables: state, murder, assault, urban_pop and robbery.\nWe will be using these data to explore if there are correlations between these variables.\nThe data are stored in the file data/CS3-usarrests.csv.",
    "crumbs": [
      "Continuous predictors",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_correlations.html#summarise-and-visualise",
    "href": "materials/cs3_practical_correlations.html#summarise-and-visualise",
    "title": "13  Correlations",
    "section": "13.4 Summarise and visualise",
    "text": "13.4 Summarise and visualise\nFirst, we load the data:\n\nRPython\n\n\n\n# load the data\nUSArrests &lt;- read_csv(\"data/CS3-usarrests.csv\")\n\n# have a look at the data\nUSArrests\n\n# A tibble: 50 × 5\n   state       murder assault urban_pop robbery\n   &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 Alabama       13.2     236        58    21.2\n 2 Alaska        10       263        48    44.5\n 3 Arizona        8.1     294        80    31  \n 4 Arkansas       8.8     190        50    19.5\n 5 California     9       276        91    40.6\n 6 Colorado       7.9     204        78    38.7\n 7 Connecticut    3.3     110        77    11.1\n 8 Delaware       5.9     238        72    15.8\n 9 Florida       15.4     335        80    31.9\n10 Georgia       17.4     211        60    25.8\n# ℹ 40 more rows\n\n\nWe can create a visual overview of the potential correlations that might exist between the variables. There are different ways of doing this, for example by creating scatter plots between variable pairs:\n\n# murder vs robbery\nggplot(USArrests,\n       aes(x = murder, y = robbery)) +\n    geom_point()\n\n\n\n\n\n\n\n# assault vs urban_pop\nggplot(USArrests,\n       aes(x = assault, y = urban_pop)) +\n    geom_point()\n\n\n\n\n\n\n\n\nThis gets a bit tedious if there are many unique variable pairs. Unfortunately ggplot() does not have a pairwise function, but we can borrow the one from base R. The pairs() function only wants numerical data, so we need to remove the state column for this. The pairs() function has a lower.panel argument that allows you to remove duplicate combinations (after all murder vs assault is the same as assault vs murder):\n\nUSArrests %&gt;% \n    select(-state) %&gt;% \n    pairs(lower.panel = NULL)\n\n\n\n\n\n\n\n\n\n\nFirst, we load the data:\n\nUSArrests_py = pd.read_csv(\"data/CS3-usarrests.csv\")\n\nUSArrests_py.head()\n\n        state  murder  assault  urban_pop  robbery\n0     Alabama    13.2      236         58     21.2\n1      Alaska    10.0      263         48     44.5\n2     Arizona     8.1      294         80     31.0\n3    Arkansas     8.8      190         50     19.5\n4  California     9.0      276         91     40.6\n\n\nWe can create a visual overview of the potential correlations that might exist between the variables. There are different ways of doing this, for example by creating scatter plots between variable pairs:\n\n# murder vs robbery\n(ggplot(USArrests_py,\n       aes(x = \"murder\",\n           y = \"robbery\")) +\n     geom_point())\n\n\n\n\n\n\n\n# assault vs urban_pop\n(ggplot(USArrests_py,\n       aes(x = \"assault\",\n           y = \"urban_pop\")) +\n     geom_point())\n\n\n\n\n\n\n\n\nThis gets a bit tedious if there are many unique variable pairs. There is an option to automatically create a matrix of scatter plots, using Seaborn. But that would involve installing the seaborn package just for this. And frankly, I don’t want to - not least because staring at tons of scatter plots is probably not the best way forward anyway!\nIf you have your heart set on creating a pairplot, then have a look at the seaborn documentation.\n\n\n\nFrom the visual inspection we can see that there appears to be a slight positive correlation between all pairs of variables, although this may be very weak in some cases (murder and urban_pop for example).",
    "crumbs": [
      "Continuous predictors",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_correlations.html#correlation-coefficients",
    "href": "materials/cs3_practical_correlations.html#correlation-coefficients",
    "title": "13  Correlations",
    "section": "13.5 Correlation coefficients",
    "text": "13.5 Correlation coefficients\nInstead of visualising the variables against each other in a scatter plot, we can also calculate correlation coefficients for each variable pair. There are different types of correlation coefficients, but the most well-known one is probably Pearson’s r. This is a measure of the linear correlation between two variables. It has a value between -1 and +1, where +1 means a perfect positive correlation, -1 means a perfect negative correlation and 0 means no correlation at all.\nThere are other correlation coefficients, most notably the Spearman’s rank correlation coefficient, a non-parametric measure of rank correlation and is generally less sensitive to outliers.\nSo, let’s calculate Pearson’s r for our data:\n\nRPython\n\n\nWe can do this using the cor() function. Since we can only calculate correlations between numbers, we have to remove the state column from our data before calculating the correlations:\n\nUSArrests %&gt;% \n    select(-state) %&gt;% \n    cor()\n\n              murder   assault  urban_pop   robbery\nmurder    1.00000000 0.8018733 0.06957262 0.5635788\nassault   0.80187331 1.0000000 0.25887170 0.6652412\nurban_pop 0.06957262 0.2588717 1.00000000 0.4113412\nrobbery   0.56357883 0.6652412 0.41134124 1.0000000\n\n\nThis gives us a numerical overview of the Pearson’s r correlation coefficients between each variable pair. Note that across the diagonal the correlation coefficients are 1 - this should make sense since, for example, murder is perfectly correlated with itself.\nAs before, the values are mirrored across the diagonal, since the correlation between, for example, murder and assault is the same as the one between assault and murder.\n\n13.5.1 Visualise the correlation matrix\nJust staring at a matrix of numbers might not be very useful. It would be good to create some sort of heatmap of the values, so we can visually inspect the data a bit better. There are dedicated packages that allow you to do this (for example the corrr) package).\nHere we’ll just use the standard stats::heatmap() function. The symm argument tells the function that we have a symmetric matrix and in conjunction with the Rowv = NA argument stops the plot from reordering the rows and columns. The Rowv = NA argument also stops the function from adding dendrograms to the margins of the plot.\nThe plot itself is coloured from yellow, indicating the smallest values (which in this case correspond to no difference in correlation coefficients), through orange to dark red, indicating the biggest values (which in this case correspond to the variables with the biggest difference in correlation coefficients).\nThe plot is symmetric along the leading diagonal (hopefully for obvious reasons).\n\nUSArrests %&gt;% \n  select(-state) %&gt;% \n  cor() %&gt;% \n  heatmap(symm = TRUE, Rowv = NA)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlternative method 1: ggplot\n\n\n\n\n\nBefore we can plot the data we need to reformat the data. We’re taking the following steps:\n\nwe calculate the correlation matrix with cor() using the (default) method of method = \"pearson\"\nconvert the output to a tibble so we can use\npivot_longer() to reformat the data into pairwise variables and a column with the Pearson’s r value\nuse the mutate() and round() functions to round the Pearson’s r values\n\n\nUSArrests_pear &lt;- USArrests %&gt;% \n    select(-state) %&gt;% \n    cor(method = \"pearson\") %&gt;% \n    as_tibble(rownames = \"var1\") %&gt;% \n    pivot_longer(cols = -var1,\n                 names_to = \"var2\",\n                 values_to = \"pearson_cor\") %&gt;% \n    mutate(pearson_cor = round(pearson_cor, digits = 3))\n\nThe output of that looks like this:\n\nhead(USArrests_pear)\n\n# A tibble: 6 × 3\n  var1    var2      pearson_cor\n  &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;\n1 murder  murder          1    \n2 murder  assault         0.802\n3 murder  urban_pop       0.07 \n4 murder  robbery         0.564\n5 assault murder          0.802\n6 assault assault         1    \n\n\nAfter all that, we can visualise the data with geom_tile(), adding the Pearson’s r values as text labels:\n\nggplot(USArrests_pear,\n       aes(x = var1, y = var2, fill = pearson_cor)) +\n    geom_tile() +\n    geom_text(aes(label = pearson_cor),\n              color = \"white\",\n              size = 4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlternative method 2: rstatix\n\n\n\n\n\nAs always, there are multiple ways to skin a proverbial cat. If you’d rather use a function from the rstatix package (which we’ve loaded before), then you can run the following code, which uses the rstatix::cor_test() function:\n\nUSArrests %&gt;% \n    select(-state) %&gt;% \n    cor_test() %&gt;%\n    select(var1, var2, cor) %&gt;% \n    ggplot(aes(x = var1, y = var2, fill = cor)) +\n    geom_tile() +\n    geom_text(aes(label = cor),\n              color = \"white\",\n              size = 4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can do this using the pandas.DataFrame.corr() function. This function takes the default method = \"pearson\" and ignores any non-numerical columns (such as the state column in our data set).\n\nUSArrests_py.corr()\n\n             murder   assault  urban_pop   robbery\nmurder     1.000000  0.801873   0.069573  0.563579\nassault    0.801873  1.000000   0.258872  0.665241\nurban_pop  0.069573  0.258872   1.000000  0.411341\nrobbery    0.563579  0.665241   0.411341  1.000000\n\n\nThis gives us a numerical overview of the Pearson’s r correlation coefficients between each variable pair. Note that across the diagonal the correlation coefficients are 1 - this should make sense since, for example, murder is perfectly correlated with itself.\nAs before, the values are mirrored across the diagonal, since the correlation between, for example, murder and assault is the same as the one between assault and murder.\n\n13.5.2 Visualise the correlation matrix\nJust staring at a matrix of numbers might not be very useful. It would be good to create some sort of heatmap of the values, so we can visually inspect the data a bit better.\n\n# create correlation matrix\nUSArrests_cor_py = USArrests_py.corr()\n# put the row names into a column\nUSArrests_cor_py = USArrests_cor_py.rename_axis(\"var1\").reset_index()\n\nUSArrests_cor_py.head()\n\n        var1    murder   assault  urban_pop   robbery\n0     murder  1.000000  0.801873   0.069573  0.563579\n1    assault  0.801873  1.000000   0.258872  0.665241\n2  urban_pop  0.069573  0.258872   1.000000  0.411341\n3    robbery  0.563579  0.665241   0.411341  1.000000\n\n\nNow that we have the correlation matrix in a workable format, we need to restructure it so that we can plot the data. For this, we need to create a “long” format, using the melt() function.\n\nUSArrests_pear_py = pd.melt(USArrests_cor_py,\n        id_vars=['var1'],\n        value_vars=['murder', 'assault', 'urban_pop', 'robbery'],\n        var_name='var2',\n        value_name='cor').round(3)\n\nHave a look at the structure:\n\nUSArrests_pear_py.head()\n\n        var1     var2    cor\n0     murder   murder  1.000\n1    assault   murder  0.802\n2  urban_pop   murder  0.070\n3    robbery   murder  0.564\n4     murder  assault  0.802\n\n\n\n(ggplot(USArrests_pear_py,\n        aes(x = \"var1\", y = \"var2\", fill = \"cor\")) +\n     geom_tile() +\n     geom_text(aes(label = \"cor\"),\n               colour = \"white\",\n               size = 10))\n\n\n\n\n\n\n\n\n\n\n\n\nThe correlation matrix and visualisations give us the insight that we need. The most correlated variables are murder and assault with an \\(r\\) value of 0.80. This appears to agree well with the set plots that we produced earlier.",
    "crumbs": [
      "Continuous predictors",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_correlations.html#spearmans-rank-correlation-coefficient",
    "href": "materials/cs3_practical_correlations.html#spearmans-rank-correlation-coefficient",
    "title": "13  Correlations",
    "section": "13.6 Spearman’s rank correlation coefficient",
    "text": "13.6 Spearman’s rank correlation coefficient\nThis test first calculates the rank of the numerical data (i.e. their position from smallest (or most negative) to the largest (or most positive)) in the two variables and then calculates Pearson’s product moment correlation coefficient using the ranks. As a consequence, this test is less sensitive to outliers in the distribution.\n\nRPython\n\n\n\nUSArrests %&gt;% \n    select(-state) %&gt;% \n    cor(method = \"spearman\")\n\n             murder   assault urban_pop   robbery\nmurder    1.0000000 0.8172735 0.1067163 0.6794265\nassault   0.8172735 1.0000000 0.2752133 0.7143681\nurban_pop 0.1067163 0.2752133 1.0000000 0.4381068\nrobbery   0.6794265 0.7143681 0.4381068 1.0000000\n\n\n\n\n\nUSArrests_py.corr(method = \"spearman\")\n\n             murder   assault  urban_pop   robbery\nmurder     1.000000  0.817274   0.106716  0.679427\nassault    0.817274  1.000000   0.275213  0.714368\nurban_pop  0.106716  0.275213   1.000000  0.438107\nrobbery    0.679427  0.714368   0.438107  1.000000",
    "crumbs": [
      "Continuous predictors",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_correlations.html#exercises",
    "href": "materials/cs3_practical_correlations.html#exercises",
    "title": "13  Correlations",
    "section": "13.7 Exercises",
    "text": "13.7 Exercises\n\n13.7.1 Pearson’s r\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLevel: \nPearson’s correlation for USA state data\nWe will again use the data from the file data/CS3-statedata.csv data set for this exercise. The data set contains 50 rows and 8 columns, with column names: population, income, illiteracy, life_exp, murder, hs_grad, frost and area.\nVisually identify 3 different pairs of variables that appear to be\n\nthe most positively correlated\nthe most negatively correlated\nnot correlated at all\n\nCalculate Pearson’s r for all variable pairs and see how well you were able to identify correlation visually.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n13.8 Answer\nVisually determining the most negative/positively and uncorrelated pairs of variables:\n\nRPython\n\n\n\nUSAstate &lt;- read_csv(\"data/CS3-statedata.csv\")\n\n# have a look at the data\nUSAstate\n\n# A tibble: 50 × 9\n   state       population income illiteracy life_exp murder hs_grad frost   area\n   &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 Alabama           3615   3624        2.1     69.0   15.1    41.3    20  50708\n 2 Alaska             365   6315        1.5     69.3   11.3    66.7   152 566432\n 3 Arizona           2212   4530        1.8     70.6    7.8    58.1    15 113417\n 4 Arkansas          2110   3378        1.9     70.7   10.1    39.9    65  51945\n 5 California       21198   5114        1.1     71.7   10.3    62.6    20 156361\n 6 Colorado          2541   4884        0.7     72.1    6.8    63.9   166 103766\n 7 Connecticut       3100   5348        1.1     72.5    3.1    56     139   4862\n 8 Delaware           579   4809        0.9     70.1    6.2    54.6   103   1982\n 9 Florida           8277   4815        1.3     70.7   10.7    52.6    11  54090\n10 Georgia           4931   4091        2       68.5   13.9    40.6    60  58073\n# ℹ 40 more rows\n\n\nWe basically repeat what we’ve done previously:\n\nUSAstate_pear &lt;-USAstate %&gt;% \n  select(-state) %&gt;% \n  cor(method = \"pearson\")\n\nNext, we can plot the data:\n\nheatmap(USAstate_pear, symm = TRUE, Rowv = NA)\n\n\n\n\n\n\n\n\n\n\nFirst, we load the data:\n\nUSAstate_py = pd.read_csv(\"data/CS3-statedata.csv\")\n\nUSAstate_py.head()\n\n        state  population  income  illiteracy  ...  murder  hs_grad  frost    area\n0     Alabama        3615    3624         2.1  ...    15.1     41.3     20   50708\n1      Alaska         365    6315         1.5  ...    11.3     66.7    152  566432\n2     Arizona        2212    4530         1.8  ...     7.8     58.1     15  113417\n3    Arkansas        2110    3378         1.9  ...    10.1     39.9     65   51945\n4  California       21198    5114         1.1  ...    10.3     62.6     20  156361\n\n[5 rows x 9 columns]\n\n\n\n# create correlation matrix\nUSAstate_cor_py = USAstate_py.corr()\n# put the row names into a column\nUSAstate_cor_py = USAstate_cor_py.rename_axis(\"var1\").reset_index()\n\nUSAstate_cor_py.head()\n\n         var1  population    income  ...   hs_grad     frost      area\n0  population    1.000000  0.208228  ... -0.098490 -0.332152  0.022544\n1      income    0.208228  1.000000  ...  0.619932  0.226282  0.363315\n2  illiteracy    0.107622 -0.437075  ... -0.657189 -0.671947  0.077261\n3    life_exp   -0.068052  0.340255  ...  0.582216  0.262068 -0.107332\n4      murder    0.343643 -0.230078  ... -0.487971 -0.538883  0.228390\n\n[5 rows x 9 columns]\n\n\nNow that we have the correlation matrix in a workable format, we need to restructure it so that we can plot the data. For this, we need to create a “long” format, using the melt() function. Note that we’re not setting the values_var argument. If not set, then it uses all but the id_vars column (which in our case is a good thing, since we don’t want to manually specify lots of column names).\n\nUSAstate_pear_py = pd.melt(USAstate_cor_py,\n        id_vars=['var1'],\n        var_name='var2',\n        value_name='cor').round(3)\n\nHave a look at the structure:\n\nUSArrests_pear_py.head()\n\n        var1     var2    cor\n0     murder   murder  1.000\n1    assault   murder  0.802\n2  urban_pop   murder  0.070\n3    robbery   murder  0.564\n4     murder  assault  0.802\n\n\n\n(ggplot(USAstate_pear_py,\n        aes(x = \"var1\", y = \"var2\", fill = \"cor\")) +\n     geom_tile() +\n     geom_text(aes(label = \"cor\"),\n               colour = \"white\",\n               size = 10))\n\n\n\n\n\n\n\n\n\n\n\nIt looks like:\n\nilliteracy and murder are the most positively correlated pair\nlife_exp and murder are the most negatively correlated pair\npopulation and area are the least correlated pair\n\nWe can explore that numerically, by doing the following:\n\nRPython\n\n\nFirst, we need to create the pairwise comparisons, with the relevant Pearson’s \\(r\\) values:\n\n# build a contingency table with as.table()\n# and create a dataframe with as.data.frame()\nUSAstate_pear_cont &lt;- as.data.frame(as.table(USAstate_pear))\n    \n# and have a look\nhead(USAstate_pear_cont)\n\n        Var1       Var2        Freq\n1 population population  1.00000000\n2     income population  0.20822756\n3 illiteracy population  0.10762237\n4   life_exp population -0.06805195\n5     murder population  0.34364275\n6    hs_grad population -0.09848975\n\n\nIs this method obvious? No! Some creative Googling led to Stackoverflow and here we are. But, it does give us what we need.\nNow that we have the paired comparisons, we can extract the relevant data:\n\n# first we remove the same-pair correlations\nUSAstate_pear_cont &lt;- USAstate_pear_cont %&gt;% \n  filter(Freq != 1)\n\n# most positively correlated pair\nUSAstate_pear_cont %&gt;% \n  filter(Freq == max(Freq))\n\n        Var1       Var2      Freq\n1     murder illiteracy 0.7029752\n2 illiteracy     murder 0.7029752\n\n# most negatively correlated pair\nUSAstate_pear_cont %&gt;% \n  filter(Freq == min(Freq))\n\n      Var1     Var2       Freq\n1   murder life_exp -0.7808458\n2 life_exp   murder -0.7808458\n\n# least correlated pair\nUSAstate_pear_cont %&gt;% \n  filter(Freq == min(abs(Freq)))\n\n        Var1       Var2       Freq\n1       area population 0.02254384\n2 population       area 0.02254384\n\n\nNote that we use the minimum absolute value (with the abs() function) to find the least correlated pair.\n\n\nWe take the correlation matrix in the long format:\n\nUSAstate_pear_py.head()\n\n         var1        var2    cor\n0  population  population  1.000\n1      income  population  0.208\n2  illiteracy  population  0.108\n3    life_exp  population -0.068\n4      murder  population  0.344\n\n\nand use it to extract the relevant values:\n\n# filter out self-pairs\ndf_cor = USAstate_pear_py.query(\"cor != 1\")\n\n# filter for the maximum correlation value\ndf_cor[df_cor.cor == df_cor.cor.max()]\n\n          var1        var2    cor\n20      murder  illiteracy  0.703\n34  illiteracy      murder  0.703\n\n# filter for the minimum correlation value\ndf_cor[df_cor.cor == df_cor.cor.min()]\n\n        var1      var2    cor\n28    murder  life_exp -0.781\n35  life_exp    murder -0.781\n\n# filter for the least correlated value\n# create a column containing absolute values\ndf_cor[\"abs_cor\"] = df_cor[\"cor\"].abs()\ndf_cor[df_cor.abs_cor == df_cor.abs_cor.min()]\n\n          var1        var2    cor  abs_cor\n7         area  population  0.023    0.023\n56  population        area  0.023    0.023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n13.8.1 Spearman’s correlation\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLevel: \nCalculate Spearman’s correlation coefficient for the data/CS3-statedata.csv data set.\nWhich variable’s correlations are affected most by the use of the Spearman’s rank compared with Pearson’s r? Hint: think of a way to address this question programmatically.\nThinking about the variables, can you explain why this might this be?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n13.9 Answer\nIn order to determine which variables are most affected by the choice of Spearman vs Pearson you could just plot both matrices out side by side and try to spot what was going on, but one of the reasons we’re using programming languages is that we can be a bit more programmatic about these things. Also, our eyes aren’t that good at processing and parsing this sort of information display. A better way would be to somehow visualise the data.\n\nRPython\n\n\nFirst, calculate the Pearson and Spearman correlation matrices (technically, we’ve done the Pearson one already, but we’re doing it again for clarity here).\n\ncor_pear &lt;- USAstate %&gt;% \n    select(-state) %&gt;% \n    cor(method = \"pearson\")\n\ncor_spear &lt;- USAstate %&gt;% \n    select(-state) %&gt;% \n    cor(method = \"spearman\")\n\nWe can calculate the difference between two matrices by subtracting them.\n\ncor_diff &lt;- cor_pear - cor_spear\n\nAgain, we could now just look at a grid of 64 numbers and see if we can spot the biggest differences, but our eyes aren’t that good at processing and parsing this sort of information display. A better way would be to visualise the data.\n\nheatmap(abs(cor_diff), symm = TRUE, Rowv = NA)\n\n\n\n\n\n\n\n\nThe abs() function calculates the absolute value (i.e. just the magnitude) of the matrix values. This is just because I only care about situations where the two correlation coefficients are different from each other but I don’t care which is the larger. The symm argument tells the function that we have a symmetric matrix and in conjunction with the Rowv = NA argument stops the plot from reordering the rows and columns. The Rowv = NA argument also stops the function from adding dendrograms to the margins of the plot.\n\n\nFirst, calculate the Pearson and Spearman correlation matrices (technically, we’ve done the Pearson one already, but we’re doing it again for clarity here).\n\ncor_pear_py = USAstate_py.corr(method = \"pearson\")\ncor_spea_py = USAstate_py.corr(method = \"spearman\")\n\nWe can calculate the difference between two matrices by subtracting them.\n\ncor_dif_py = cor_pear_py - cor_spea_py\n\nAgain, we could now just look at a grid of 64 numbers and see if we can spot the biggest differences, but our eyes aren’t that good at processing and parsing this sort of information display. A better way would be to visualise the data.\n\n# get the row names in a column\ncor_dif_py = cor_dif_py.rename_axis(\"var1\").reset_index()\n\n# reformat the data into a long format\n# and round the values\ncor_dif_py = pd.melt(cor_dif_py,\n        id_vars=['var1'],\n        var_name='var2',\n        value_name='cor').round(3)\n        \n# create a column with absolute correlation difference values\ncor_dif_py[\"abs_cor\"] = cor_dif_py[\"cor\"].abs()\n\n# have a look at the final data frame\ncor_dif_py.head()\n\n         var1        var2    cor  abs_cor\n0  population  population  0.000    0.000\n1      income  population  0.084    0.084\n2  illiteracy  population -0.205    0.205\n3    life_exp  population  0.036    0.036\n4      murder  population -0.002    0.002\n\n\nNow we can plot the data:\n\n(ggplot(cor_dif_py,\n        aes(x = \"var1\", y = \"var2\", fill = \"abs_cor\")) +\n     geom_tile() +\n     geom_text(aes(label = \"abs_cor\"),\n               colour = \"white\",\n               size = 10))\n\n\n\n\n\n\n\n\n\n\n\nAll in all there is not a huge difference in correlation coefficients, since the values are all quite small. Most of the changes occur along the area variable. One possible explanation could be that certain states with a large area have a relatively large effect on the Pearson’s r coefficient. For example, Alaska has an area that is over twice as big as the next state - Texas.\nIf, for example, we’d look a bit closer then we would find for area and income that Pearson gives a value of 0.36, a slight positive correlation, whereas Spearman gives a value of 0.057, basically uncorrelated.\nThis means that this is basically ignored by Spearman.\nWell done, Mr. Spearman.",
    "crumbs": [
      "Continuous predictors",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_correlations.html#answer",
    "href": "materials/cs3_practical_correlations.html#answer",
    "title": "13  Correlations",
    "section": "13.8 Answer",
    "text": "13.8 Answer\nVisually determining the most negative/positively and uncorrelated pairs of variables:\n\nRPython\n\n\n\nUSAstate &lt;- read_csv(\"data/CS3-statedata.csv\")\n\n# have a look at the data\nUSAstate\n\n# A tibble: 50 × 9\n   state       population income illiteracy life_exp murder hs_grad frost   area\n   &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 Alabama           3615   3624        2.1     69.0   15.1    41.3    20  50708\n 2 Alaska             365   6315        1.5     69.3   11.3    66.7   152 566432\n 3 Arizona           2212   4530        1.8     70.6    7.8    58.1    15 113417\n 4 Arkansas          2110   3378        1.9     70.7   10.1    39.9    65  51945\n 5 California       21198   5114        1.1     71.7   10.3    62.6    20 156361\n 6 Colorado          2541   4884        0.7     72.1    6.8    63.9   166 103766\n 7 Connecticut       3100   5348        1.1     72.5    3.1    56     139   4862\n 8 Delaware           579   4809        0.9     70.1    6.2    54.6   103   1982\n 9 Florida           8277   4815        1.3     70.7   10.7    52.6    11  54090\n10 Georgia           4931   4091        2       68.5   13.9    40.6    60  58073\n# ℹ 40 more rows\n\n\nWe basically repeat what we’ve done previously:\n\nUSAstate_pear &lt;-USAstate %&gt;% \n  select(-state) %&gt;% \n  cor(method = \"pearson\")\n\nNext, we can plot the data:\n\nheatmap(USAstate_pear, symm = TRUE, Rowv = NA)\n\n\n\n\n\n\n\n\n\n\nFirst, we load the data:\n\nUSAstate_py = pd.read_csv(\"data/CS3-statedata.csv\")\n\nUSAstate_py.head()\n\n        state  population  income  illiteracy  ...  murder  hs_grad  frost    area\n0     Alabama        3615    3624         2.1  ...    15.1     41.3     20   50708\n1      Alaska         365    6315         1.5  ...    11.3     66.7    152  566432\n2     Arizona        2212    4530         1.8  ...     7.8     58.1     15  113417\n3    Arkansas        2110    3378         1.9  ...    10.1     39.9     65   51945\n4  California       21198    5114         1.1  ...    10.3     62.6     20  156361\n\n[5 rows x 9 columns]\n\n\n\n# create correlation matrix\nUSAstate_cor_py = USAstate_py.corr()\n# put the row names into a column\nUSAstate_cor_py = USAstate_cor_py.rename_axis(\"var1\").reset_index()\n\nUSAstate_cor_py.head()\n\n         var1  population    income  ...   hs_grad     frost      area\n0  population    1.000000  0.208228  ... -0.098490 -0.332152  0.022544\n1      income    0.208228  1.000000  ...  0.619932  0.226282  0.363315\n2  illiteracy    0.107622 -0.437075  ... -0.657189 -0.671947  0.077261\n3    life_exp   -0.068052  0.340255  ...  0.582216  0.262068 -0.107332\n4      murder    0.343643 -0.230078  ... -0.487971 -0.538883  0.228390\n\n[5 rows x 9 columns]\n\n\nNow that we have the correlation matrix in a workable format, we need to restructure it so that we can plot the data. For this, we need to create a “long” format, using the melt() function. Note that we’re not setting the values_var argument. If not set, then it uses all but the id_vars column (which in our case is a good thing, since we don’t want to manually specify lots of column names).\n\nUSAstate_pear_py = pd.melt(USAstate_cor_py,\n        id_vars=['var1'],\n        var_name='var2',\n        value_name='cor').round(3)\n\nHave a look at the structure:\n\nUSArrests_pear_py.head()\n\n        var1     var2    cor\n0     murder   murder  1.000\n1    assault   murder  0.802\n2  urban_pop   murder  0.070\n3    robbery   murder  0.564\n4     murder  assault  0.802\n\n\n\n(ggplot(USAstate_pear_py,\n        aes(x = \"var1\", y = \"var2\", fill = \"cor\")) +\n     geom_tile() +\n     geom_text(aes(label = \"cor\"),\n               colour = \"white\",\n               size = 10))\n\n\n\n\n\n\n\n\n\n\n\nIt looks like:\n\nilliteracy and murder are the most positively correlated pair\nlife_exp and murder are the most negatively correlated pair\npopulation and area are the least correlated pair\n\nWe can explore that numerically, by doing the following:\n\nRPython\n\n\nFirst, we need to create the pairwise comparisons, with the relevant Pearson’s \\(r\\) values:\n\n# build a contingency table with as.table()\n# and create a dataframe with as.data.frame()\nUSAstate_pear_cont &lt;- as.data.frame(as.table(USAstate_pear))\n    \n# and have a look\nhead(USAstate_pear_cont)\n\n        Var1       Var2        Freq\n1 population population  1.00000000\n2     income population  0.20822756\n3 illiteracy population  0.10762237\n4   life_exp population -0.06805195\n5     murder population  0.34364275\n6    hs_grad population -0.09848975\n\n\nIs this method obvious? No! Some creative Googling led to Stackoverflow and here we are. But, it does give us what we need.\nNow that we have the paired comparisons, we can extract the relevant data:\n\n# first we remove the same-pair correlations\nUSAstate_pear_cont &lt;- USAstate_pear_cont %&gt;% \n  filter(Freq != 1)\n\n# most positively correlated pair\nUSAstate_pear_cont %&gt;% \n  filter(Freq == max(Freq))\n\n        Var1       Var2      Freq\n1     murder illiteracy 0.7029752\n2 illiteracy     murder 0.7029752\n\n# most negatively correlated pair\nUSAstate_pear_cont %&gt;% \n  filter(Freq == min(Freq))\n\n      Var1     Var2       Freq\n1   murder life_exp -0.7808458\n2 life_exp   murder -0.7808458\n\n# least correlated pair\nUSAstate_pear_cont %&gt;% \n  filter(Freq == min(abs(Freq)))\n\n        Var1       Var2       Freq\n1       area population 0.02254384\n2 population       area 0.02254384\n\n\nNote that we use the minimum absolute value (with the abs() function) to find the least correlated pair.\n\n\nWe take the correlation matrix in the long format:\n\nUSAstate_pear_py.head()\n\n         var1        var2    cor\n0  population  population  1.000\n1      income  population  0.208\n2  illiteracy  population  0.108\n3    life_exp  population -0.068\n4      murder  population  0.344\n\n\nand use it to extract the relevant values:\n\n# filter out self-pairs\ndf_cor = USAstate_pear_py.query(\"cor != 1\")\n\n# filter for the maximum correlation value\ndf_cor[df_cor.cor == df_cor.cor.max()]\n\n          var1        var2    cor\n20      murder  illiteracy  0.703\n34  illiteracy      murder  0.703\n\n# filter for the minimum correlation value\ndf_cor[df_cor.cor == df_cor.cor.min()]\n\n        var1      var2    cor\n28    murder  life_exp -0.781\n35  life_exp    murder -0.781\n\n# filter for the least correlated value\n# create a column containing absolute values\ndf_cor[\"abs_cor\"] = df_cor[\"cor\"].abs()\ndf_cor[df_cor.abs_cor == df_cor.abs_cor.min()]\n\n          var1        var2    cor  abs_cor\n7         area  population  0.023    0.023\n56  population        area  0.023    0.023",
    "crumbs": [
      "Continuous predictors",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_correlations.html#answer-1",
    "href": "materials/cs3_practical_correlations.html#answer-1",
    "title": "13  Correlations",
    "section": "13.9 Answer",
    "text": "13.9 Answer\nIn order to determine which variables are most affected by the choice of Spearman vs Pearson you could just plot both matrices out side by side and try to spot what was going on, but one of the reasons we’re using programming languages is that we can be a bit more programmatic about these things. Also, our eyes aren’t that good at processing and parsing this sort of information display. A better way would be to somehow visualise the data.\n\nRPython\n\n\nFirst, calculate the Pearson and Spearman correlation matrices (technically, we’ve done the Pearson one already, but we’re doing it again for clarity here).\n\ncor_pear &lt;- USAstate %&gt;% \n    select(-state) %&gt;% \n    cor(method = \"pearson\")\n\ncor_spear &lt;- USAstate %&gt;% \n    select(-state) %&gt;% \n    cor(method = \"spearman\")\n\nWe can calculate the difference between two matrices by subtracting them.\n\ncor_diff &lt;- cor_pear - cor_spear\n\nAgain, we could now just look at a grid of 64 numbers and see if we can spot the biggest differences, but our eyes aren’t that good at processing and parsing this sort of information display. A better way would be to visualise the data.\n\nheatmap(abs(cor_diff), symm = TRUE, Rowv = NA)\n\n\n\n\n\n\n\n\nThe abs() function calculates the absolute value (i.e. just the magnitude) of the matrix values. This is just because I only care about situations where the two correlation coefficients are different from each other but I don’t care which is the larger. The symm argument tells the function that we have a symmetric matrix and in conjunction with the Rowv = NA argument stops the plot from reordering the rows and columns. The Rowv = NA argument also stops the function from adding dendrograms to the margins of the plot.\n\n\nFirst, calculate the Pearson and Spearman correlation matrices (technically, we’ve done the Pearson one already, but we’re doing it again for clarity here).\n\ncor_pear_py = USAstate_py.corr(method = \"pearson\")\ncor_spea_py = USAstate_py.corr(method = \"spearman\")\n\nWe can calculate the difference between two matrices by subtracting them.\n\ncor_dif_py = cor_pear_py - cor_spea_py\n\nAgain, we could now just look at a grid of 64 numbers and see if we can spot the biggest differences, but our eyes aren’t that good at processing and parsing this sort of information display. A better way would be to visualise the data.\n\n# get the row names in a column\ncor_dif_py = cor_dif_py.rename_axis(\"var1\").reset_index()\n\n# reformat the data into a long format\n# and round the values\ncor_dif_py = pd.melt(cor_dif_py,\n        id_vars=['var1'],\n        var_name='var2',\n        value_name='cor').round(3)\n        \n# create a column with absolute correlation difference values\ncor_dif_py[\"abs_cor\"] = cor_dif_py[\"cor\"].abs()\n\n# have a look at the final data frame\ncor_dif_py.head()\n\n         var1        var2    cor  abs_cor\n0  population  population  0.000    0.000\n1      income  population  0.084    0.084\n2  illiteracy  population -0.205    0.205\n3    life_exp  population  0.036    0.036\n4      murder  population -0.002    0.002\n\n\nNow we can plot the data:\n\n(ggplot(cor_dif_py,\n        aes(x = \"var1\", y = \"var2\", fill = \"abs_cor\")) +\n     geom_tile() +\n     geom_text(aes(label = \"abs_cor\"),\n               colour = \"white\",\n               size = 10))\n\n\n\n\n\n\n\n\n\n\n\nAll in all there is not a huge difference in correlation coefficients, since the values are all quite small. Most of the changes occur along the area variable. One possible explanation could be that certain states with a large area have a relatively large effect on the Pearson’s r coefficient. For example, Alaska has an area that is over twice as big as the next state - Texas.\nIf, for example, we’d look a bit closer then we would find for area and income that Pearson gives a value of 0.36, a slight positive correlation, whereas Spearman gives a value of 0.057, basically uncorrelated.\nThis means that this is basically ignored by Spearman.\nWell done, Mr. Spearman.",
    "crumbs": [
      "Continuous predictors",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "materials/cs3_practical_correlations.html#summary",
    "href": "materials/cs3_practical_correlations.html#summary",
    "title": "13  Correlations",
    "section": "13.10 Summary",
    "text": "13.10 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nCorrelation is the degree to which two variables are linearly related\nCorrelation does not imply causation\nWe can visualise correlations by plotting variables against each other or creating heatmap-type plots of the correlation coefficients\nTwo main correlation coefficients are Pearson’s r and Spearman’s rank, with Spearman’s rank being less sensitive to outliers",
    "crumbs": [
      "Continuous predictors",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Correlations</span>"
    ]
  },
  {
    "objectID": "materials/cs4_practical_two-way-anova.html",
    "href": "materials/cs4_practical_two-way-anova.html",
    "title": "14  Two-way ANOVA",
    "section": "",
    "text": "14.1 Libraries and functions",
    "crumbs": [
      "Two predictors",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Two-way ANOVA</span>"
    ]
  },
  {
    "objectID": "materials/cs4_practical_two-way-anova.html#libraries-and-functions",
    "href": "materials/cs4_practical_two-way-anova.html#libraries-and-functions",
    "title": "14  Two-way ANOVA",
    "section": "",
    "text": "Click to expand\n\n\n\n\n\n\nRPython\n\n\n\n14.1.1 Libraries\n\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\n# Converts stats functions to a tidyverse-friendly format\nlibrary(rstatix)\n\n# Creates diagnostic plots using ggplot2\nlibrary(ggResidpanel)\n\n\n\n14.1.2 Functions\n\n# Creates diagnostic plots\nggResidpanel::resid_panel()\n\n# Creates a linear model\nstats::lm()\n\n# Creates an ANOVA table for a linear model\nstats::anova()\n\n\n\n\n\n14.1.3 Libraries\n\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# Statistical models, conducting tests and statistical data exploration\nimport statsmodels.api as sm\n\n# Convenience interface for specifying models using formula strings and DataFrames\nimport statsmodels.formula.api as smf\n\n\n\n14.1.4 Functions\n\n# Summary statistics\npandas.DataFrame.describe()\n\n# Plots the first few rows of a DataFrame\npandas.DataFrame.head()\n\n# Reads in a .csv file\npandas.read_csv()\n\n# Creates a model from a formula and data frame\nstatsmodels.formula.api.ols()\n\n# Creates an ANOVA table for one or more fitted linear models\nstatsmodels.stats.anova.anova_lm()",
    "crumbs": [
      "Two predictors",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Two-way ANOVA</span>"
    ]
  },
  {
    "objectID": "materials/cs4_practical_two-way-anova.html#purpose-and-aim",
    "href": "materials/cs4_practical_two-way-anova.html#purpose-and-aim",
    "title": "14  Two-way ANOVA",
    "section": "14.2 Purpose and aim",
    "text": "14.2 Purpose and aim\nA two-way analysis of variance is used when we have two categorical predictor variables (or factors) and a single continuous response variable. For example, when we are looking at how body weight (continuous response variable in kilograms) is affected by sex (categorical variable, male or female) and exercise type (categorical variable, control or runner).\n\n\n\n\n\n\n\n\n\nWhen analysing these type of data there are two things we want to know:\n\nDoes either of the predictor variables have an effect on the response variable i.e. does sex affect body weight? Or does being a runner affect body weight?\nIs there any interaction between the two predictor variables? An interaction would mean that the effect that exercise has on your weight depends on whether you are male or female rather than being independent of your sex. For example if being male means that runners weigh more than non-runners, but being female means that runners weight less than non-runners then we would say that there was an interaction.\n\nWe will first consider how to visualise the data before then carrying out an appropriate statistical test.",
    "crumbs": [
      "Two predictors",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Two-way ANOVA</span>"
    ]
  },
  {
    "objectID": "materials/cs4_practical_two-way-anova.html#data-and-hypotheses",
    "href": "materials/cs4_practical_two-way-anova.html#data-and-hypotheses",
    "title": "14  Two-way ANOVA",
    "section": "14.3 Data and hypotheses",
    "text": "14.3 Data and hypotheses\nWe will recreate the example analysis used in the lecture. The data are stored as a .csv file called data/CS4-exercise.csv.",
    "crumbs": [
      "Two predictors",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Two-way ANOVA</span>"
    ]
  },
  {
    "objectID": "materials/cs4_practical_two-way-anova.html#summarise-and-visualise",
    "href": "materials/cs4_practical_two-way-anova.html#summarise-and-visualise",
    "title": "14  Two-way ANOVA",
    "section": "14.4 Summarise and visualise",
    "text": "14.4 Summarise and visualise\nexercise is a data frame with three variables; weight, sex and exercise. weight is the continuous response variable, whereas sex and exercise are the categorical predictor variables.\n\nRPython\n\n\nFirst, we read in the data:\n\nexercise &lt;- read_csv(\"data/CS4-exercise.csv\")\n\nYou can visualise the data with:\n\n# visualise the data, sex vs weight\nggplot(exercise,\n       aes(x = sex, y = weight)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n# visualise the data, exercise vs weight\nggplot(exercise,\n       aes(x = exercise, y = weight)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\nFirst, we read in the data:\n\nexercise_py = pd.read_csv(\"data/CS4-exercise.csv\")\n\nYou can visualise the data with:\n\n# visualise the data, sex vs weight\n(ggplot(exercise_py,\n        aes(x = \"sex\", y = \"weight\")) +\n  geom_boxplot())\n\n\n\n\n\n\n\n# visualise the data, exercise vs weight\n(ggplot(exercise_py,\n        aes(x = \"exercise\", y = \"weight\")) +\n  geom_boxplot())\n\n\n\n\n\n\n\n\n\n\n\nThese produce box plots showing the response variable (weight) only in terms of one of the predictor variables. The values of the other predictor variable in each case aren’t taken into account.\nA better way would be to visualise both variables at the same time. We can do this as follows:\n\nRPython\n\n\n\nggplot(exercise,\n       aes(x = sex, y = weight, fill = exercise)) +\n  geom_boxplot() +\n  scale_fill_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n\nThis produces box plots for all (four) combinations of the predictor variables. We are plotting sex on the x-axis; weight on the y-axis and filling the box plot by exercise regime.\nHere I’ve also changed the default colouring scheme, by using scale_fill_brewer(palette = \"Dark2\"). This uses a colour-blind friendly colour palette (more about the Brewer colour pallete here).\n\n\n\n(ggplot(exercise_py,\n        aes(x = \"sex\",\n            y = \"weight\", fill = \"exercise\")) +\n     geom_boxplot() +\n     scale_fill_brewer(type = \"qual\", palette = \"Dark2\"))\n\n\n\n\n\n\n\n\nThis produces box plots for all (four) combinations of the predictor variables. We are plotting sex on the x-axis; weight on the y-axis and filling the box plot by exercise regime.\nHere I’ve also changed the default colouring scheme, by using scale_fill_brewer(type = \"qual\", palette = \"Dark2\"). This uses a colour-blind friendly colour palette (more about the Brewer colour pallete here).\n\n\n\nIn this example there are only four box plots and so it is relatively easy to compare them and look for any interactions between variables, but if there were more than two groups per categorical variable, it would become harder to spot what was going on.\nTo compare categorical variables more easily we can just plot the group means which aids our ability to look for interactions and the main effects of each predictor variable. This is called an interaction plot.\nCreate an interaction plot:\n\nRPython\n\n\nWe’re adding a bit of jitter to the data, to avoid too much overlap between the data points. We can do this with geom_jitter().\n\nggplot(exercise,\n       aes(x = sex, y = weight,\n           colour = exercise, group = exercise)) +\n  geom_jitter(width = 0.05) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  scale_colour_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n\nHere we plot weight on the y-axis, by sex on the x-axis.\n\nwe colour the data by exercise regime and group the data by exercise to work out the mean values of each group\ngeom_jitter(width = 0.05) displays the data, with a tiny bit of random noise, to separate the data points a bit for visualisation\nstat_summary(fun = mean)calculates the mean for each group\nscale_colour_brewer() lets us define the colour palette\n\nThe choice of which categorical factor is plotted on the horizontal axis and which is plotted as different lines is completely arbitrary. Looking at the data both ways shouldn’t add anything but often you’ll find that you prefer one plot to another.\nPlot the interaction plot the other way round:\n\nggplot(exercise,\n       aes(x = exercise, y = weight,\n           colour = sex, group = sex)) +\n  geom_jitter(width = 0.05) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  scale_colour_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n\n\n\nWe’re adding a bit of jitter to the data, to avoid too much overlap between the data points. We can do this with geom_jitter().\n\n(ggplot(exercise_py,\n        aes(x = \"sex\", y = \"weight\",\n            colour = \"exercise\", group = \"exercise\")) +\n     geom_jitter(width = 0.05) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"point\", size = 3) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"line\") +\n     scale_colour_brewer(type = \"qual\", palette = \"Dark2\"))\n\n\n\n\n\n\n\n\nHere we plot weight on the y-axis, by sex on the x-axis.\n\nwe colour the data by exercise regime and group the data by exercise to work out the mean values of each group\ngeom_jitter(width = 0.05) displays the data, with a tiny bit of random noise, to separate the data points a bit for visualisation\nstat_summary(fun_data = \"mean_cl_boot\")calculates the mean for each group\nscale_colour_brewer() lets us define the colour palette\n\nThe choice of which categorical factor is plotted on the horizontal axis and which is plotted as different lines is completely arbitrary. Looking at the data both ways shouldn’t add anything but often you’ll find that you prefer one plot to another.\nPlot the interaction plot the other way round:\n\n(ggplot(exercise_py,\n        aes(x = \"exercise\", y = \"weight\",\n            colour = \"sex\", group = \"sex\")) +\n     geom_jitter(width = 0.05) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"point\", size = 3) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"line\") +\n  scale_colour_brewer(type = \"qual\", palette = \"Dark2\"))\n\n\n\n\n\n\n\n\n\n\n\nBy now you should have a good feeling for the data and could already provide some guesses to the following three questions:\n\nDoes there appear to be any interaction between the two categorical variables?\nIf not:\n\nDoes exercise have an effect on weight?\nDoes sex have an effect on weight?\n\n\nWe can now attempt to answer these three questions more formally using an ANOVA test. We have to test for three things: the interaction, the effect of exercise and the effect of sex.",
    "crumbs": [
      "Two predictors",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Two-way ANOVA</span>"
    ]
  },
  {
    "objectID": "materials/cs4_practical_two-way-anova.html#assumptions",
    "href": "materials/cs4_practical_two-way-anova.html#assumptions",
    "title": "14  Two-way ANOVA",
    "section": "14.5 Assumptions",
    "text": "14.5 Assumptions\nBefore we can formally test these things we first need to define the model and check the underlying assumptions. We use the following code to define the model:\n\nRPython\n\n\n\n# define the linear model\nlm_exercise &lt;- lm(weight ~ sex + exercise + sex:exercise,\n                  data = exercise)\n\nThe sex:exercise term is how R represents the concept of an interaction between these two variables.\n\n\n\n# create a linear model\nmodel = smf.ols(formula = \"weight ~ exercise * sex\", data = exercise_py)\n# and get the fitted parameters of the model\nlm_exercise_py = model.fit()\n\nThe formula weight ~ exercise * sex can be read as “weight depends on exercise and sex and the interaction between exercise and sex.\n\n\n\nAs the two-way ANOVA is a type of linear model we need to satisfy pretty much the same assumptions as we did for a simple linear regression or a one-way ANOVA:\n\nThe data must not have any systematic pattern to it\nThe residuals must be normally distributed\nThe residuals must have homogeneity of variance\nThe fit should not depend overly much on a single point (no point should have high leverage).\n\nAgain, we will check these assumptions visually by producing four key diagnostic plots.\n\nRPython\n\n\n\nresid_panel(lm_exercise,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\n\n\nThe Residual plot shows the residuals against the predicted values. There is no systematic pattern here and this plot is pretty good.\nThe Q-Q plot allows a visual inspection of normality. Again, this looks OK (not perfect but OK).\nThe Location-Scale plot allows us to investigate whether there is homogeneity of variance. This plot is fine (not perfect but fine).\nThe Cook’s D plot shows that no individual point has a high influence on the model (all values are well below 0.5)\n\n\nThere is a shorthand way of writing:\nweight ~ sex + exercise + sex:exercise\nIf you use the following syntax:\nweight ~ sex * exercise\nThen R interprets it exactly the same way as writing all three terms. You can see this if you compare the output of the following two commands:\n\nanova(lm(weight ~ sex + exercise + sex:exercise,\n         data = exercise))\n\nAnalysis of Variance Table\n\nResponse: weight\n              Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nsex            1 4510.1  4510.1 366.911 &lt; 2.2e-16 ***\nexercise       1 1312.0  1312.0 106.733 &lt; 2.2e-16 ***\nsex:exercise   1  404.4   404.4  32.902 4.889e-08 ***\nResiduals    156 1917.6    12.3                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(lm(weight ~ sex * exercise,\n         data = exercise))\n\nAnalysis of Variance Table\n\nResponse: weight\n              Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nsex            1 4510.1  4510.1 366.911 &lt; 2.2e-16 ***\nexercise       1 1312.0  1312.0 106.733 &lt; 2.2e-16 ***\nsex:exercise   1  404.4   404.4  32.902 4.889e-08 ***\nResiduals    156 1917.6    12.3                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\ndgplots(lm_exercise_py)",
    "crumbs": [
      "Two predictors",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Two-way ANOVA</span>"
    ]
  },
  {
    "objectID": "materials/cs4_practical_two-way-anova.html#implement-and-interpret-test",
    "href": "materials/cs4_practical_two-way-anova.html#implement-and-interpret-test",
    "title": "14  Two-way ANOVA",
    "section": "14.6 Implement and interpret test",
    "text": "14.6 Implement and interpret test\nThe assumptions appear to be met well enough, meaning we can implement the ANOVA. We do this as follows (this is probably the easiest bit!):\n\nRPython\n\n\n\n# perform the ANOVA\nanova(lm_exercise)\n\nAnalysis of Variance Table\n\nResponse: weight\n              Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nsex            1 4510.1  4510.1 366.911 &lt; 2.2e-16 ***\nexercise       1 1312.0  1312.0 106.733 &lt; 2.2e-16 ***\nsex:exercise   1  404.4   404.4  32.902 4.889e-08 ***\nResiduals    156 1917.6    12.3                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe have a row in the table for each of the different effects that we’ve asked R to consider. The last column is the important one as this contains the p-values. We need to look at the interaction row first.\n\n\n\nsm.stats.anova_lm(lm_exercise_py, typ = 2)\n\n                   sum_sq     df           F        PR(&gt;F)\nexercise      1311.970522    1.0  106.733448  2.177106e-19\nsex           4636.450232    1.0  377.191645  1.760076e-43\nexercise:sex   404.434414    1.0   32.902172  4.889216e-08\nResidual      1917.556353  156.0         NaN           NaN\n\n\nWe have a row in the table for each of the different effects that we’ve asked Python to consider. The last column is the important one as this contains the p-values. We need to look at the interaction row first.\n\n\n\nsex:exercise has a p-value of about 4.89e-08 (which is smaller than 0.05) and so we can conclude that the interaction between sex and exercise is significant.\nThis is where we must stop.\nThe top two lines (corresponding to the effects of sex and exercise) are meaningless now. This is because the interaction means that we cannot interpret the main effects independently.\nIn this case, weight depends on and the sex and the exercise regime. This means the effect of sex on weight is dependent on exercise (and vice-versa).\nWe would report this as follows:\n\nA two-way ANOVA test showed that there was a significant interaction between the effects of sex and exercise on weight (p = 4.89e-08). Exercise was associated with a small loss of weight in males but a larger loss of weight in females.",
    "crumbs": [
      "Two predictors",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Two-way ANOVA</span>"
    ]
  },
  {
    "objectID": "materials/cs4_practical_two-way-anova.html#exercises",
    "href": "materials/cs4_practical_two-way-anova.html#exercises",
    "title": "14  Two-way ANOVA",
    "section": "14.7 Exercises",
    "text": "14.7 Exercises\n\n14.7.1 Auxin response\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLevel: \nPlant height responses to auxin in different genotypes\nThese data/CS4-auxin.csv data are from a simulated experiment that looks at the effect of the plant hormone auxin on plant height.\nThe experiment consists of two genotypes: a wild type control and a mutant (genotype). The plants are treated with auxin at different concentrations: none, low and high, which are stored in the concentration column.\nThe response variable plant height (plant_height) is then measured at the end of their life cycle, in centimeters.\nQuestions to answer:\n\nVisualise the data using boxplots and interaction plots.\nDoes there appear to be any interaction between genotype and concentration?\nCarry out a two-way ANOVA test.\nCheck the assumptions.\nWhat can you conclude? (Write a sentence to summarise).\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n14.8 Answer\n\nRPython\n\n\n\nLoad the data\n\n# read in the data\nauxin_response &lt;- read_csv(\"data/CS4-auxin.csv\")\n\n# let's have a peek at the data\nhead(auxin_response)\n\n# A tibble: 6 × 3\n  genotype concentration plant_height\n  &lt;chr&gt;    &lt;chr&gt;                &lt;dbl&gt;\n1 control  high                  33.7\n2 control  high                  27.1\n3 control  high                  22.9\n4 control  high                  28.7\n5 control  high                  30.7\n6 control  high                  26.9\n\n\n\n\nVisualise the data\n\nggplot(auxin_response,\n       aes(x = genotype, y = plant_height)) +\n  geom_boxplot()\n\n\n\n\n\n\n\nggplot(auxin_response,\n       aes(x = concentration, y = plant_height)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nLet’s look at the interaction plots. We’re only plotting the mean values here, but feel free to explore the data itself by adding another geom_.\n\n# by genotype\nggplot(auxin_response,\n       aes(x = concentration,\n          y = plant_height,\n          colour = genotype, group = genotype)) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  geom_jitter(alpha = 0.3, width = 0.1) +\n  scale_colour_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n# by concentration\nggplot(auxin_response,\n       aes(x = genotype,\n           y = plant_height,\n           colour = concentration, group = concentration)) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  geom_jitter(alpha = 0.3, width = 0.1) +\n  scale_colour_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n\nWe’ve constructed both box plots and two interaction plots. We only needed to do one interaction plot but I find it can be quite useful to look at the data from different angles.\nThe interaction plots show the mean values for each group, so I prefer to overlay this with the actual data. Both interaction plots suggest that there is an interaction here as the lines in the plots aren’t parallel. Looking at the interaction plot with concentration on the x-axis, it appears that there is non-difference between genotypes when the concentration is low, but that there is a difference between genotypes when the concentration is none or high.\n\n\nAssumptions\nFirst we need to define the model:\n\n# define the linear model, with interaction term\nlm_auxin &lt;- lm(plant_height ~ concentration * genotype,\n               data = auxin_response)\n\nNext, we check the assumptions:\n\nresid_panel(lm_auxin,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\n\nSo, these actually all look pretty good, with the data looking normally distributed (Q-Q plot), linearity OK (Residual plot), homogeneity of variance looking sharp (Location-scale plot) and no clear influential points (Cook’s D plot).\n\n\nImplement the test\nLet’s carry out a two-way ANOVA:\n\n# perform the ANOVA\nanova(lm_auxin)\n\nAnalysis of Variance Table\n\nResponse: plant_height\n                        Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nconcentration            2 4708.2 2354.12 316.333 &lt; 2.2e-16 ***\ngenotype                 1   83.8   83.84  11.266 0.0009033 ***\nconcentration:genotype   2 1034.9  517.45  69.531 &lt; 2.2e-16 ***\nResiduals              269 2001.9    7.44                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nInterpret the output and report the results\nThere is a significant interaction between concentration and genotype.\n\n\n\n\nLoad the data\n\n# read in the data\nauxin_response_py = pd.read_csv(\"data/CS4-auxin.csv\")\n\n# let's have a peek at the data\nauxin_response_py.head()\n\n  genotype concentration  plant_height\n0  control          high          33.7\n1  control          high          27.1\n2  control          high          22.9\n3  control          high          28.7\n4  control          high          30.7\n\n\n\n\nVisualise the data\n\n(ggplot(auxin_response_py,\n       aes(x = \"genotype\", y = \"plant_height\")) +\n  geom_boxplot())\n\n\n\n\n\n\n\n(ggplot(auxin_response_py,\n       aes(x = \"concentration\", y = \"plant_height\")) +\n  geom_boxplot())\n\n\n\n\n\n\n\n\nLet’s look at the interaction plots. We’re also including the data itself here with geom_jitter().\n\n# by genotype\n(ggplot(auxin_response_py,\n        aes(x = \"concentration\",\n            y = \"plant_height\",\n            colour = \"genotype\", group = \"genotype\")) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"point\", size = 3) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"line\") +\n     geom_jitter(alpha = 0.3, width = 0.1) +\n     scale_colour_brewer(type = \"qual\", palette = \"Dark2\"))\n\n\n\n\n\n\n\n# by concentration\n(ggplot(auxin_response_py,\n        aes(x = \"genotype\",\n            y = \"plant_height\",\n            colour = \"concentration\", group = \"concentration\")) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"point\", size = 3) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"line\") +\n     geom_jitter(alpha = 0.3, width = 0.1) +\n     scale_colour_brewer(type = \"qual\", palette = \"Dark2\"))\n\n\n\n\n\n\n\n\nWe’ve constructed both box plots and two interaction plots. We only needed to do one interaction plot but I find it can be quite useful to look at the data from different angles.\nThe interaction plots show the mean values for each group, so I prefer to overlay this with the actual data. Both interaction plots suggest that there is an interaction here as the lines in the plots aren’t parallel. Looking at the interaction plot with concentration on the x-axis, it appears that there is non-difference between genotypes when the concentration is low, but that there is a difference between genotypes when the concentration is none or high.\n\n\nAssumptions\nFirst we need to define the model:\n\n# create a linear model\nmodel = smf.ols(formula= \"plant_height ~ C(genotype) * concentration\", data = auxin_response_py)\n# and get the fitted parameters of the model\nlm_auxin_py = model.fit()\n\nNext, we check the assumptions:\n\ndgplots(lm_auxin_py)\n\n\n\n\n\n\n\n\n\n\nSo, these actually all look pretty good, with the data looking normally distributed (Q-Q plot), linearity OK (Residual plot), homogeneity of variance looking sharp (Location-scale plot) and no clear influential points (Influential points plot).\n\n\nImplement the test\nLet’s carry out a two-way ANOVA:\n\nsm.stats.anova_lm(lm_auxin_py, typ = 2)\n\n                                sum_sq     df           F        PR(&gt;F)\nC(genotype)                  83.839560    1.0   11.265874  9.033241e-04\nconcentration              4578.890410    2.0  307.642376  3.055198e-70\nC(genotype):concentration  1034.894263    2.0   69.531546  4.558874e-25\nResidual                   2001.872330  269.0         NaN           NaN\n\n\n\n\nInterpret the output and report the results\nThere is definitely a significant interaction between concentration and genotype.\n\n\n\n\nSo, we can conclude the following:\n\nA two-way ANOVA showed that there is a significant interaction between genotype and auxin concentration on plant height (p = 4.56e-25). Increasing auxin concentration appears to result in a reduction of plant height in both wild type and mutant genotypes. The response in the mutant genotype seems to be less pronounced than in wild type.\n\n\n\n\n\n\n\n\n\n\n\n\n\n14.8.1 Tulips\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLevel: \nBlooms and growing conditions\nWe’re sticking with the plant theme and using the data/CS4-tulip.csv data set, which contains information on an experiment to determine the best conditions for growing tulips (well someone has to care about these sorts of things!). The average number of flower heads (blooms) were recorded for 27 different plots. Each plot experienced one of three different watering regimes and one of three different shade regimes.\n\nInvestigate how the number of blooms is affected by different growing conditions.\n\nNote: have a look at the data and make sure that they are in the correct format!\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nRPython\n\n\n\nLoad the data\n\n# read in the data\ntulip &lt;- read_csv(\"data/CS4-tulip.csv\")\n\nRows: 27 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): water, shade, blooms\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# have a quick look at the data\ntulip\n\n# A tibble: 27 × 3\n   water shade blooms\n   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1     1     1    0  \n 2     1     2    0  \n 3     1     3  111. \n 4     2     1  183. \n 5     2     2   59.2\n 6     2     3   76.8\n 7     3     1  225. \n 8     3     2   83.8\n 9     3     3  135. \n10     1     1   80.1\n# ℹ 17 more rows\n\n\nIn this data set the watering regime (water) and shading regime (shade) are encoded with numerical values. However, these numbers are actually categories, representing the amount of water/shade.\nAs such, we don’t want to treat these as numbers but as factors. At the moment they are numbers, which we can tell with &lt;dbl&gt;, which stands for double.\nWe can convert the columns using the as_factor() function. Because we’d like to keep referring to these columns as factors, we will update our existing data set.\n\n# convert watering and shade regimes to factor\ntulip &lt;- tulip %&gt;% \n  mutate(water = as_factor(water),\n         shade = as_factor(shade))\n\nThis data set has three variables; blooms (which is the response variable) and water and shade (which are the two potential predictor variables).\n\n\nVisualise the data\nAs always we’ll visualise the data first:\n\n# by watering regime\nggplot(tulip,\n       aes(x = water, y = blooms)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n# by shading regime\nggplot(tulip,\n       aes(x = shade, y = blooms)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n# interaction plot by watering regime\nggplot(tulip,\n       aes(x = shade,\n           y = blooms,\n           colour = water, group = water)) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  geom_jitter(alpha = 0.3, width = 0.1) +\n  scale_colour_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n# interaction plot by shade regime\nggplot(tulip,\n       aes(x = water,\n           y = blooms,\n           colour = shade, group = shade)) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  geom_jitter(alpha = 0.3, width = 0.1) +\n  scale_colour_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n\nAgain, both interaction plots suggest that there might be an interaction here. Digging in a little deeper from a descriptive perspective, it looks as though that water regime 1 is behaving differently to water regimes 2 and 3 under different shade conditions.\n\n\nAssumptions\nFirst we need to define the model:\n\n# define the linear model\nlm_tulip &lt;- lm(blooms ~ water * shade,\n               data = tulip)\n\nNext, we check the assumptions:\n\nresid_panel(lm_tulip,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\n\nThese are actually all OK. A two-way ANOVA analysis is on the cards.\n\n\nImplement the test\nLet’s carry out the two-way ANOVA.\n\n# perform the ANOVA\nanova(lm_tulip)\n\nAnalysis of Variance Table\n\nResponse: blooms\n            Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nwater        2 103626   51813 22.0542 1.442e-05 ***\nshade        2  36376   18188  7.7417   0.00375 ** \nwater:shade  4  41058   10265  4.3691   0.01211 *  \nResiduals   18  42288    2349                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nInterpret the output and report results\nSo we do appear to have a significant interaction between water and shade as expected.\n\n\n\n\nLoad the data\n\n# read in the data\ntulip_py = pd.read_csv(\"data/CS4-tulip.csv\")\n\n# have a quick look at the data\ntulip_py.head()\n\n   water  shade  blooms\n0      1      1    0.00\n1      1      2    0.00\n2      1      3  111.04\n3      2      1  183.47\n4      2      2   59.16\n\n\nIn this data set the watering regime (water) and shading regime (shade) are encoded with numerical values. However, these numbers are actually categories, representing the amount of water/shade.\nAs such, we don’t want to treat these as numbers but as factors. We can convert the columns using astype(object). Because we’d like to keep referring to these columns as factors, we will update our existing data set.\n\n# convert watering and shade regimes to factor\ntulip_py['water'] = tulip_py['water'].astype(object)\ntulip_py['shade'] = tulip_py['shade'].astype(object)\n\nThis data set has three variables; blooms (which is the response variable) and water and shade (which are the two potential predictor variables).\n\n\nVisualise the data\nAs always we’ll visualise the data first:\n\n# by watering regime\n(ggplot(tulip_py,\n        aes(x = \"water\", y = \"blooms\")) +\n     geom_boxplot())\n\n\n\n\n\n\n\n  \n# by shading regime\n(ggplot(tulip_py,\n        aes(x = \"shade\", y = \"blooms\")) +\n     geom_boxplot())\n\n\n\n\n\n\n\n# interaction plot by watering regime\n(ggplot(tulip_py,\n        aes(x = \"shade\", y = \"blooms\",\n            colour = \"water\", group = \"water\")) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"point\", size = 3) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"line\") +\n     geom_jitter(alpha = 0.3, width = 0.1) +\n     scale_colour_brewer(type = \"qual\", palette = \"Dark2\"))\n\n\n\n\n\n\n\n# interaction plot by shade regime\n(ggplot(tulip_py,\n        aes(x = \"water\", y = \"blooms\",\n            colour = \"shade\", group = \"shade\")) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"point\", size = 3) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"line\") +\n     geom_jitter(alpha = 0.3, width = 0.1) +\n     scale_colour_brewer(type = \"qual\", palette = \"Dark2\"))\n\n\n\n\n\n\n\n\nAgain, both interaction plots suggest that there might be an interaction here. Digging in a little deeper from a descriptive perspective, it looks as though that water regime 1 is behaving differently to water regimes 2 and 3 under different shade conditions.\n\n\nAssumptions\nFirst we need to define the model:\n\n# create a linear model\nmodel = smf.ols(formula= \"blooms ~ water * shade\", data = tulip_py)\n# and get the fitted parameters of the model\nlm_tulip_py = model.fit()\n\nNext, we check the assumptions:\n\ndgplots(lm_tulip_py)\n\n\n\n\n\n\n\n\n\n\nThese are actually all OK. A two-way ANOVA analysis is on the cards.\n\n\nImplement the test\nLet’s carry out the two-way ANOVA.\n\nsm.stats.anova_lm(lm_tulip_py, typ = 2)\n\n                    sum_sq    df          F    PR(&gt;F)\nwater        103625.786985   2.0  22.054200  0.000014\nshade         36375.936807   2.0   7.741723  0.003750\nwater:shade   41058.139437   4.0   4.369108  0.012108\nResidual      42288.185200  18.0        NaN       NaN\n\n\n\n\nInterpret the output and report results\nSo we do appear to have a significant interaction between water and shade as expected.\n\n\n\n\n\nA two-way ANOVA showed that there is a significant interaction between watering and shading regimes on number of blooms (p = 1.21e-02).",
    "crumbs": [
      "Two predictors",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Two-way ANOVA</span>"
    ]
  },
  {
    "objectID": "materials/cs4_practical_two-way-anova.html#answer",
    "href": "materials/cs4_practical_two-way-anova.html#answer",
    "title": "14  Two-way ANOVA",
    "section": "14.8 Answer",
    "text": "14.8 Answer\n\nRPython\n\n\n\nLoad the data\n\n# read in the data\nauxin_response &lt;- read_csv(\"data/CS4-auxin.csv\")\n\n# let's have a peek at the data\nhead(auxin_response)\n\n# A tibble: 6 × 3\n  genotype concentration plant_height\n  &lt;chr&gt;    &lt;chr&gt;                &lt;dbl&gt;\n1 control  high                  33.7\n2 control  high                  27.1\n3 control  high                  22.9\n4 control  high                  28.7\n5 control  high                  30.7\n6 control  high                  26.9\n\n\n\n\nVisualise the data\n\nggplot(auxin_response,\n       aes(x = genotype, y = plant_height)) +\n  geom_boxplot()\n\n\n\n\n\n\n\nggplot(auxin_response,\n       aes(x = concentration, y = plant_height)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nLet’s look at the interaction plots. We’re only plotting the mean values here, but feel free to explore the data itself by adding another geom_.\n\n# by genotype\nggplot(auxin_response,\n       aes(x = concentration,\n          y = plant_height,\n          colour = genotype, group = genotype)) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  geom_jitter(alpha = 0.3, width = 0.1) +\n  scale_colour_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n# by concentration\nggplot(auxin_response,\n       aes(x = genotype,\n           y = plant_height,\n           colour = concentration, group = concentration)) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  geom_jitter(alpha = 0.3, width = 0.1) +\n  scale_colour_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n\nWe’ve constructed both box plots and two interaction plots. We only needed to do one interaction plot but I find it can be quite useful to look at the data from different angles.\nThe interaction plots show the mean values for each group, so I prefer to overlay this with the actual data. Both interaction plots suggest that there is an interaction here as the lines in the plots aren’t parallel. Looking at the interaction plot with concentration on the x-axis, it appears that there is non-difference between genotypes when the concentration is low, but that there is a difference between genotypes when the concentration is none or high.\n\n\nAssumptions\nFirst we need to define the model:\n\n# define the linear model, with interaction term\nlm_auxin &lt;- lm(plant_height ~ concentration * genotype,\n               data = auxin_response)\n\nNext, we check the assumptions:\n\nresid_panel(lm_auxin,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\n\nSo, these actually all look pretty good, with the data looking normally distributed (Q-Q plot), linearity OK (Residual plot), homogeneity of variance looking sharp (Location-scale plot) and no clear influential points (Cook’s D plot).\n\n\nImplement the test\nLet’s carry out a two-way ANOVA:\n\n# perform the ANOVA\nanova(lm_auxin)\n\nAnalysis of Variance Table\n\nResponse: plant_height\n                        Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nconcentration            2 4708.2 2354.12 316.333 &lt; 2.2e-16 ***\ngenotype                 1   83.8   83.84  11.266 0.0009033 ***\nconcentration:genotype   2 1034.9  517.45  69.531 &lt; 2.2e-16 ***\nResiduals              269 2001.9    7.44                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nInterpret the output and report the results\nThere is a significant interaction between concentration and genotype.\n\n\n\n\nLoad the data\n\n# read in the data\nauxin_response_py = pd.read_csv(\"data/CS4-auxin.csv\")\n\n# let's have a peek at the data\nauxin_response_py.head()\n\n  genotype concentration  plant_height\n0  control          high          33.7\n1  control          high          27.1\n2  control          high          22.9\n3  control          high          28.7\n4  control          high          30.7\n\n\n\n\nVisualise the data\n\n(ggplot(auxin_response_py,\n       aes(x = \"genotype\", y = \"plant_height\")) +\n  geom_boxplot())\n\n\n\n\n\n\n\n(ggplot(auxin_response_py,\n       aes(x = \"concentration\", y = \"plant_height\")) +\n  geom_boxplot())\n\n\n\n\n\n\n\n\nLet’s look at the interaction plots. We’re also including the data itself here with geom_jitter().\n\n# by genotype\n(ggplot(auxin_response_py,\n        aes(x = \"concentration\",\n            y = \"plant_height\",\n            colour = \"genotype\", group = \"genotype\")) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"point\", size = 3) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"line\") +\n     geom_jitter(alpha = 0.3, width = 0.1) +\n     scale_colour_brewer(type = \"qual\", palette = \"Dark2\"))\n\n\n\n\n\n\n\n# by concentration\n(ggplot(auxin_response_py,\n        aes(x = \"genotype\",\n            y = \"plant_height\",\n            colour = \"concentration\", group = \"concentration\")) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"point\", size = 3) +\n     stat_summary(fun_data = \"mean_cl_boot\",\n                  geom = \"line\") +\n     geom_jitter(alpha = 0.3, width = 0.1) +\n     scale_colour_brewer(type = \"qual\", palette = \"Dark2\"))\n\n\n\n\n\n\n\n\nWe’ve constructed both box plots and two interaction plots. We only needed to do one interaction plot but I find it can be quite useful to look at the data from different angles.\nThe interaction plots show the mean values for each group, so I prefer to overlay this with the actual data. Both interaction plots suggest that there is an interaction here as the lines in the plots aren’t parallel. Looking at the interaction plot with concentration on the x-axis, it appears that there is non-difference between genotypes when the concentration is low, but that there is a difference between genotypes when the concentration is none or high.\n\n\nAssumptions\nFirst we need to define the model:\n\n# create a linear model\nmodel = smf.ols(formula= \"plant_height ~ C(genotype) * concentration\", data = auxin_response_py)\n# and get the fitted parameters of the model\nlm_auxin_py = model.fit()\n\nNext, we check the assumptions:\n\ndgplots(lm_auxin_py)\n\n\n\n\n\n\n\n\n\n\nSo, these actually all look pretty good, with the data looking normally distributed (Q-Q plot), linearity OK (Residual plot), homogeneity of variance looking sharp (Location-scale plot) and no clear influential points (Influential points plot).\n\n\nImplement the test\nLet’s carry out a two-way ANOVA:\n\nsm.stats.anova_lm(lm_auxin_py, typ = 2)\n\n                                sum_sq     df           F        PR(&gt;F)\nC(genotype)                  83.839560    1.0   11.265874  9.033241e-04\nconcentration              4578.890410    2.0  307.642376  3.055198e-70\nC(genotype):concentration  1034.894263    2.0   69.531546  4.558874e-25\nResidual                   2001.872330  269.0         NaN           NaN\n\n\n\n\nInterpret the output and report the results\nThere is definitely a significant interaction between concentration and genotype.\n\n\n\n\nSo, we can conclude the following:\n\nA two-way ANOVA showed that there is a significant interaction between genotype and auxin concentration on plant height (p = 4.56e-25). Increasing auxin concentration appears to result in a reduction of plant height in both wild type and mutant genotypes. The response in the mutant genotype seems to be less pronounced than in wild type.",
    "crumbs": [
      "Two predictors",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Two-way ANOVA</span>"
    ]
  },
  {
    "objectID": "materials/cs4_practical_two-way-anova.html#summary",
    "href": "materials/cs4_practical_two-way-anova.html#summary",
    "title": "14  Two-way ANOVA",
    "section": "14.9 Summary",
    "text": "14.9 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nA two-way ANOVA is used when there are two categorical variables and a single continuous variable\nWe can visually check for interactions between the categorical variables by using interaction plots\nThe two-way ANOVA is a type of linear model and assumes the following:\n\nthe data have no systematic pattern\nthe residuals are normally distributed\nthe residuals have homogeneity of variance\nthe fit does not depend on a single point (no single point has high leverage)",
    "crumbs": [
      "Two predictors",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Two-way ANOVA</span>"
    ]
  },
  {
    "objectID": "materials/cs4_practical_linear-regression-grouped-data.html",
    "href": "materials/cs4_practical_linear-regression-grouped-data.html",
    "title": "15  Linear regression with grouped data",
    "section": "",
    "text": "15.1 Libraries and functions",
    "crumbs": [
      "Two predictors",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Linear regression with grouped data</span>"
    ]
  },
  {
    "objectID": "materials/cs4_practical_linear-regression-grouped-data.html#libraries-and-functions",
    "href": "materials/cs4_practical_linear-regression-grouped-data.html#libraries-and-functions",
    "title": "15  Linear regression with grouped data",
    "section": "",
    "text": "Click to expand\n\n\n\n\n\n\nRPython\n\n\n\n15.1.1 Libraries\n\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\n# Converts stats functions to a tidyverse-friendly format\nlibrary(rstatix)\n\n# Creates diagnostic plots using ggplot2\nlibrary(ggResidpanel)\n\n# Helper functions for tidying data\nlibrary(broom)\n\n\n\n15.1.2 Functions\n\n# Gets underlying data out of model object\nbroom::augment()\n\n# Creates diagnostic plots\nggResidpanel::resid_panel()\n\n# Performs an analysis of variance\nstats::anova()\n\n# Creates a linear model\nstats::lm()\n\n\n\n\n\n15.1.3 Libraries\n\n# A fundamental package for scientific computing in Python\nimport numpy as np\n\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Simple yet exhaustive stats functions.\nimport pingouin as pg\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# Statistical models, conducting tests and statistical data exploration\nimport statsmodels.api as sm\n\n# Convenience interface for specifying models using formula strings and DataFrames\nimport statsmodels.formula.api as smf\n\n\n\n15.1.4 Functions\n\n# Computes natural logarithm of value\nnumpy.log()\n\n# Plots the first few rows of a DataFrame\npandas.DataFrame.head()\n\n# Reads in a .csv file\npandas.read_csv()\n\n# Creates a model from a formula and data frame\nstatsmodels.formula.api.ols()",
    "crumbs": [
      "Two predictors",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Linear regression with grouped data</span>"
    ]
  },
  {
    "objectID": "materials/cs4_practical_linear-regression-grouped-data.html#purpose-and-aim",
    "href": "materials/cs4_practical_linear-regression-grouped-data.html#purpose-and-aim",
    "title": "15  Linear regression with grouped data",
    "section": "15.2 Purpose and aim",
    "text": "15.2 Purpose and aim\nA linear regression analysis with grouped data is used when we have one categorical predictor variable (or factor), and one continuous predictor variable. The response variable must still be continuous however.\nFor example in an experiment that looks at light intensity in woodland, how is light intensity (continuous: lux) affected by the height at which the measurement is taken, recorded as depth measured from the top of the canopy (continuous: meters) and by the type of woodland (categorical: Conifer or Broad leaf).\n\n\n\n\n\n\n\n\n\nWhen analysing these type of data we want to know:\n\nIs there a difference between the groups?\nDoes the continuous predictor variable affect the continuous response variable (does canopy depth affect measured light intensity?)\nIs there any interaction between the two predictor variables? Here an interaction would display itself as a difference in the slopes of the regression lines for each group, so for example perhaps the conifer data set has a significantly steeper line than the broad leaf woodland data set.\n\nIn this case, no interaction means that the regression lines will have the same slope. Essentially the analysis is identical to two-way ANOVA.\n\nWe will plot the data and visually inspect it.\nWe will test for an interaction and if it doesn’t exist then:\n\nWe can test to see if either predictor variable has an effect (i.e. do the regression lines have different intercepts? and is the common gradient significantly different from zero?)\n\n\nWe will first consider how to visualise the data before then carrying out an appropriate statistical test.",
    "crumbs": [
      "Two predictors",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Linear regression with grouped data</span>"
    ]
  },
  {
    "objectID": "materials/cs4_practical_linear-regression-grouped-data.html#data-and-hypotheses",
    "href": "materials/cs4_practical_linear-regression-grouped-data.html#data-and-hypotheses",
    "title": "15  Linear regression with grouped data",
    "section": "15.3 Data and hypotheses",
    "text": "15.3 Data and hypotheses\nThe data are stored in data/CS4-treelight.csv. This is a data frame with four variables; id, light, depth and species. light is the continuous response variable, depth is the continuous predictor variable and species is the categorical predictor variable.\nRead in the data and inspect them:\n\nRPython\n\n\n\n# read in the data\ntreelight &lt;- read_csv(\"data/CS4-treelight.csv\")\n\n# inspect the data\ntreelight\n\n# A tibble: 23 × 4\n      id light depth species\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  \n 1     1 4106.  1    Conifer\n 2     2 4934.  1.75 Conifer\n 3     3 4417.  2.5  Conifer\n 4     4 4529.  3.25 Conifer\n 5     5 3443.  4    Conifer\n 6     6 4640.  4.75 Conifer\n 7     7 3082.  5.5  Conifer\n 8     8 2368.  6.25 Conifer\n 9     9 2777.  7    Conifer\n10    10 2419.  7.75 Conifer\n# ℹ 13 more rows\n\n\n\n\n\n# load the data\ntreelight_py = pd.read_csv(\"data/CS4-treelight.csv\")\n\n# and have a look\ntreelight_py.head()\n\n   id        light  depth  species\n0   1  4105.646110   1.00  Conifer\n1   2  4933.925144   1.75  Conifer\n2   3  4416.527443   2.50  Conifer\n3   4  4528.618186   3.25  Conifer\n4   5  3442.610306   4.00  Conifer",
    "crumbs": [
      "Two predictors",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Linear regression with grouped data</span>"
    ]
  },
  {
    "objectID": "materials/cs4_practical_linear-regression-grouped-data.html#summarise-and-visualise",
    "href": "materials/cs4_practical_linear-regression-grouped-data.html#summarise-and-visualise",
    "title": "15  Linear regression with grouped data",
    "section": "15.4 Summarise and visualise",
    "text": "15.4 Summarise and visualise\n\nRPython\n\n\n\n# plot the data\nggplot(treelight,\n       aes(x = depth, y = light, colour = species)) +\n  geom_point() +\n  scale_color_brewer(palette = \"Dark2\") +\n  labs(x = \"Depth (m)\",\n       y = \"Light intensity (lux)\")\n\n\n\n\n\n\n\n\n\n\n\n# plot the data\n(ggplot(treelight_py,\n        aes(x = \"depth\",\n            y = \"light\",\n            colour = \"species\")) +\n     geom_point() +\n     scale_color_brewer(type = \"qual\", palette = \"Dark2\") +\n     labs(x = \"Depth (m)\",\n         y = \"Light intensity (lux)\")\n)\n\n\n\n\n\n\n\n\n\n\n\nIt looks like there is a slight negative correlation between depth and light intensity, with light intensity reducing as the canopy depth increases. It would be useful to plot the regression lines in this plot.\n\nRPython\n\n\n\n# plot the data\nggplot(treelight,\n       aes(x = depth, y = light, colour = species)) +\n  geom_point() +\n  # add regression lines\n  geom_smooth(method = \"lm\", se = FALSE) +\n  scale_color_brewer(palette = \"Dark2\") +\n  labs(x = \"Depth (m)\",\n       y = \"Light intensity (lux)\")\n\n\n\n\n\n\n\n\n\n\n\n# plot the data\n(ggplot(treelight_py,\n        aes(x = \"depth\",\n            y = \"light\",\n            colour = \"species\")) +\n     geom_point() +\n     # add regression lines\n     geom_smooth(method = \"lm\", se = False) +\n     scale_color_brewer(type = \"qual\", palette = \"Dark2\") +\n     labs(x = \"Depth (m)\",\n          y = \"Light intensity (lux)\"))\n\n\n\n\n\n\n\n\n\n\n\nLooking at this plot, there doesn’t appear to be any significant interaction between the woodland type (Broadleaf and Conifer) and the depth at which light measurements were taken (depth) on the amount of light intensity getting through the canopy as the gradients of the two lines appear to be very similar. There does appear to be a noticeable slope to both lines and both lines look as though they have very different intercepts. All of this suggests that there isn’t any interaction but that both depth and species have a significant effect on light independently.",
    "crumbs": [
      "Two predictors",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Linear regression with grouped data</span>"
    ]
  },
  {
    "objectID": "materials/cs4_practical_linear-regression-grouped-data.html#implement-and-interpret-the-test",
    "href": "materials/cs4_practical_linear-regression-grouped-data.html#implement-and-interpret-the-test",
    "title": "15  Linear regression with grouped data",
    "section": "15.5 Implement and interpret the test",
    "text": "15.5 Implement and interpret the test\nIn this case we’re going to implement the test before checking the assumptions (I know, let’s live a little!). You’ll find out why soon…\nWe can test for a possible interaction more formally:\n\nRPython\n\n\n\nanova(lm(light ~ depth * species,\n         data = treelight))\n\nAnalysis of Variance Table\n\nResponse: light\n              Df   Sum Sq  Mean Sq  F value    Pr(&gt;F)    \ndepth          1 30812910 30812910 107.8154 2.861e-09 ***\nspecies        1 51029543 51029543 178.5541 4.128e-11 ***\ndepth:species  1   218138   218138   0.7633    0.3932    \nResiduals     19  5430069   285793                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nRemember that depth * species is a shorthand way of writing the full set of depth + species + depth:species terms in R i.e. both main effects and the interaction effect.\n\n\nUnfortunately there is no clear way of defining interaction models in pingouin. So we’re resorting back to statsmodels, just like we had to when we performed the Shapiro-Wilk test on the residuals.\nIf you haven’t loaded statsmodels yet, run the following:\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nNext, we create a linear model and get the .fit():\n\n# create a linear model\nmodel = smf.ols(formula = \"light ~ depth * C(species)\",\n                data = treelight_py)\n\n# and get the fitted parameters of the model\nlm_treelight_py = model.fit()\n\nTo get the relevant values, we can print the summary of the model fit. This gives us a rather huge table. Don’t be too daunted by it - there is a logic to the madness and for now we’re mainly interested in the P&gt;|t| column.\n\nprint(lm_treelight_py.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  light   R-squared:                       0.938\nModel:                            OLS   Adj. R-squared:                  0.928\nMethod:                 Least Squares   F-statistic:                     95.71\nDate:                Tue, 16 Apr 2024   Prob (F-statistic):           1.19e-11\nTime:                        07:43:49   Log-Likelihood:                -174.91\nNo. Observations:                  23   AIC:                             357.8\nDf Residuals:                      19   BIC:                             362.4\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n===============================================================================================\n                                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------------------\nIntercept                    7798.5655    298.623     26.115      0.000    7173.541    8423.590\nC(species)[T.Conifer]       -2784.5833    442.274     -6.296      0.000   -3710.274   -1858.893\ndepth                        -221.1256     61.802     -3.578      0.002    -350.478     -91.773\ndepth:C(species)[T.Conifer]   -71.0357     81.309     -0.874      0.393    -241.217      99.145\n==============================================================================\nOmnibus:                        1.435   Durbin-Watson:                   2.176\nProb(Omnibus):                  0.488   Jarque-Bera (JB):                1.269\nSkew:                           0.444   Prob(JB):                        0.530\nKurtosis:                       2.267   Cond. No.                         31.8\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nAs with two-way ANOVA we have a row in the table for each of the different effects. At this point we are particularly interested in the p-values. We need to look at the interaction first.\nThe interaction term between depth and species has a p-value of 0.393 (which is bigger than 0.05) and so we can conclude that the interaction between depth and species isn’t significant. As such we can now consider whether each of the predictor variables independently has an effect.\nBoth depth and species have very small p-values (2.86x10-9 and 4.13x10 -11) and so we can conclude that they do have a significant effect on light.\nThis means that the two regression lines should have the same non-zero slope, but different intercepts. We would now like to know what those values are.\n\n15.5.1 Finding intercept values\nFinding the intercept values is not entirely straightforward and there is some deciphering required to get this right.\nFor a simple straight line such as the linear regression for the conifer data by itself, the output is relatively straightforward:\n\nRPython\n\n\nI’m being a bit lazy here. Since I don’t want to save the filtered data in a separate object, I’m using the pipe to send the Conifer data to the lm() function. However, the first argument in the lm() function is not data, so we need to tell it specifically that the data is coming from the pipe. We do this with the . notation:\n\n# filter the Conifer data and fit a linear model\ntreelight %&gt;% \n  filter(species == \"Conifer\") %&gt;% \n  lm(light ~ depth, data = .)\n\n\nCall:\nlm(formula = light ~ depth, data = .)\n\nCoefficients:\n(Intercept)        depth  \n     5014.0       -292.2  \n\n\n\n\nWe have two options to obtain the intercept for conifers only. We could subset our data, keeping only the conifer values. We could then create a linear model of those data, and obtain the relevant intercept.\nHowever, since we already created a model for the entire data set (including the interaction term) and printed the summary of that, we can actually derive the intercept value with the information that we’ve got.\nIn the coef table of the summary there are several values:\nIntercept                      7798.5655\nC(species)[T.Conifer]         -2784.5833\ndepth                         -221.1256\ndepth:C(species)[T.Conifer]   -71.0357\nThis tells us that the overall intercept value for the model with the interaction term is 7798.5655. The C(species)[T.Conifer] term means that, to go from this overall intercept value to the intercept for conifer, we need to add -2784.5833.\nDoing the maths gives us an intercept of \\(7798.5655 + (-2784.5833) = 5014\\) if we round this.\nEqually, if we want to get the coefficient for depth, then we take the reference value of -221.1256 and add the value next to depth:C(species)[T.Conifer] to it. This gives us \\(-221.1256 + (-71.0357) = -292.2\\) if we round it.\n\n\n\nWe can interpret this as meaning that the intercept of the regression line is 5014 and the coefficient of the depth variable (the number in front of it in the equation) is -292.2.\nSo, the equation of the regression line for the conifer data is given by:\n\\[\\begin{equation}\nlight = 5014 + -292.2 \\times depth\n\\end{equation}\\]\nThis means that for every extra 1 m of depth of forest canopy we lose 292.2 lux of light.\nWhen we looked at the full data set, we found that interaction wasn’t important. This means that we will have a model with two distinct intercepts but only a single slope (that’s what you get for a linear regression without any interaction), so we need to calculate that specific combination. We do this is as follows:\n\nRPython\n\n\n\nlm(light ~ depth + species,\n   data = treelight)\n\n\nCall:\nlm(formula = light ~ depth + species, data = treelight)\n\nCoefficients:\n   (Intercept)           depth  speciesConifer  \n        7962.0          -262.2         -3113.0  \n\n\nNotice the + symbol in the argument, as opposed to the * symbol used earlier. This means that we are explicitly not including an interaction term in this fit, and consequently we are forcing R to calculate the equation of lines which have the same gradient.\nIdeally we would like R to give us two equations, one for each forest type, so four parameters in total. Unfortunately, R is parsimonious and doesn’t do that. Instead R gives you three coefficients, and these require a bit of interpretation.\nThe first two numbers that R returns (underneath Intercept and depth) are the exact intercept and slope coefficients for one of the lines (in this case they correspond to the data for Broadleaf woodlands).\nFor the coefficients belonging to the other line, R uses these first two coefficients as baseline values and expresses the other coefficients relative to these ones. R also doesn’t tell you explicitly which group it is using as its baseline reference group! (Did I mention that R can be very helpful at times 😉?)\nSo, how to decipher the above output?\nFirst, I need to work out which group has been used as the baseline.\n\nIt will be the group that comes first alphabetically, so it should be Broadleaf\nThe other way to check would be to look and see which group is not mentioned in the above table. Conifer is mentioned (in the SpeciesConifer heading) and so again the baseline group is Broadleaf.\n\nThis means that the intercept value and depth coefficient correspond to the Broadleaf group and as a result I know what the equation of one of my lines is:\nBroadleaf:\n\\[\\begin{equation}\nlight = 7962 + -262.2 \\times depth\n\\end{equation}\\]\nIn this example we know that the gradient is the same for both lines (because we explicitly asked to exclude an interaction), so all I need to do is find the intercept value for the Conifer group. Unfortunately, the final value given underneath SpeciesConifer does not give me the intercept for Conifer, instead it tells me the difference between the Conifer group intercept and the baseline intercept i.e. the equation for the regression line for conifer woodland is given by:\n\\[\\begin{equation}\nlight = (7962 + -3113) + -262.2 \\times depth\n\\end{equation}\\]\n\\[\\begin{equation}\nlight = 4829 + -262.2 \\times depth\n\\end{equation}\\]\n\n\nThe way we obtain the values for the model without the interaction is very similar to what we did for the conifer data. We need to update our model first, to remove the interaction:\n\n# create a linear model\nmodel = smf.ols(formula = \"light ~ depth + C(species)\",\n                data = treelight_py)\n\n# and get the fitted parameters of the model\nlm_treelight_add_py = model.fit()\n\nNotice the + symbol in the argument, as opposed to the * symbol used earlier. This means that we are explicitly not including an interaction term in this fit, and consequently we are forcing Python to calculate the equation of lines which have the same gradient.\nWe can get the relevant coefficients as follows:\n\nprint(lm_treelight_add_py.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  light   R-squared:                       0.935\nModel:                            OLS   Adj. R-squared:                  0.929\nMethod:                 Least Squares   F-statistic:                     144.9\nDate:                Tue, 16 Apr 2024   Prob (F-statistic):           1.26e-12\nTime:                        07:43:50   Log-Likelihood:                -175.37\nNo. Observations:                  23   AIC:                             356.7\nDf Residuals:                      20   BIC:                             360.1\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n=========================================================================================\n                            coef    std err          t      P&gt;|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------------\nIntercept              7962.0316    231.356     34.415      0.000    7479.431    8444.633\nC(species)[T.Conifer] -3113.0265    231.586    -13.442      0.000   -3596.106   -2629.947\ndepth                  -262.1656     39.922     -6.567      0.000    -345.441    -178.891\n==============================================================================\nOmnibus:                        2.068   Durbin-Watson:                   2.272\nProb(Omnibus):                  0.356   Jarque-Bera (JB):                1.677\nSkew:                           0.633   Prob(JB):                        0.432\nKurtosis:                       2.618   Cond. No.                         13.9\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nAgain, I need to work out which group has been used as the baseline.\n\nIt will be the group that comes first alphabetically, so it should be Broadleaf\nThe other way to check would be to look and see which group is not mentioned in the above table. Conifer is mentioned (in the C(species)[T.Conifer] heading) and so again the baseline group is Broadleaf.\n\nThis means that the intercept value and depth coefficient correspond to the Broadleaf group and as a result I know what the equation of one of my lines is:\nBroadleaf:\n\\[\\begin{equation}\nlight = 7962 + -262.2 \\times depth\n\\end{equation}\\]\nIn this example we know that the gradient is the same for both lines (because we explicitly asked to exclude an interaction), so all I need to do is find the intercept value for the Conifer group. Unfortunately, the final value given in C(species)[T.Conifer] does not give me the intercept for Conifer, instead it tells me the difference between the Conifer group intercept and the baseline intercept i.e. the equation for the regression line for conifer woodland is given by:\n\\[\\begin{equation}\nlight = (7962 + -3113) + -262.2 \\times depth\n\\end{equation}\\]\n\\[\\begin{equation}\nlight = 4829 + -262.2 \\times depth\n\\end{equation}\\]\n\n\n\n\n\n15.5.2 Adding custom regression lines\nIn the example above we determined that the interaction term species:depth was not significant. It would be good to visualise the model without the interaction term.\n\nRPython\n\n\nThis is relatively straightforward if we understand the output of the model a bit better.\nFirst of all, we load the broom library. This is part of tidyverse, so you don’t have to install it. It is not loaded by default, hence us loading it. What broom does it changes the format of many common base R outputs into a more tidy format, so we can work with the output in our analyses more easily.\nThe function we use here is called augment(). What this does is take a model object and a dataset and adds information about each observation in the dataset.\n\n# define the model without interaction term\nlm_additive &lt;- lm(light ~ species + depth,\n                  data = treelight)\n\n# load the broom package\nlibrary(broom)\n\n# augment the model\nlm_additive %&gt;% augment()\n\n# A tibble: 23 × 9\n   light species depth .fitted .resid   .hat .sigma .cooksd .std.resid\n   &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1 4106. Conifer  1      4587.  -481. 0.191    531. 0.0799      -1.01 \n 2 4934. Conifer  1.75   4390.   544. 0.156    528. 0.0766       1.11 \n 3 4417. Conifer  2.5    4194.   223. 0.128    542. 0.00985      0.449\n 4 4529. Conifer  3.25   3997.   532. 0.105    530. 0.0440       1.06 \n 5 3443. Conifer  4      3800.  -358. 0.0896   538. 0.0163      -0.706\n 6 4640. Conifer  4.75   3604.  1037. 0.0801   486. 0.120        2.03 \n 7 3082. Conifer  5.5    3407.  -325. 0.0769   540. 0.0113      -0.637\n 8 2368. Conifer  6.25   3210.  -842. 0.0801   507. 0.0793      -1.65 \n 9 2777. Conifer  7      3014.  -237. 0.0896   542. 0.00719     -0.468\n10 2419. Conifer  7.75   2817.  -398. 0.105    537. 0.0247      -0.792\n# ℹ 13 more rows\n\n\nThe output shows us lots of data. Our original light values are in the light column and it’s the same for species and depth. What has been added is information about the fitted (or predicted) values based on the light ~ depth + species model we defined.\nThe fitted or predicted values are in the .fitted column, with corresponding residuals in the .resid column. Remember, your data = predicted values + error, so if you would add .fitted + resid then you would end up with your original data again.\nUsing this information we can now plot the regression lines by species:\n\n# plot the regression lines by species\nlm_additive %&gt;%\n  augment() %&gt;% \n  ggplot(aes(x = depth, y = .fitted, colour = species)) +\n  geom_line() +\n  scale_color_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n\nLastly, if we would want to plot the data and regression lines together, we could change the code as follows:\n\n# plot the regression lines\nlm_additive %&gt;%\n  augment() %&gt;% \n  ggplot(aes(x = depth, y = .fitted, colour = species)) +\n  # add the original data points\n  geom_point(data = treelight,\n             aes(x = depth, y = light, colour = species)) +\n  geom_line() +\n  scale_color_brewer(palette = \"Dark2\") +\n  labs(x = \"Depth (m)\",\n       y = \"Light intensity (lux)\")\n\n\n\n\n\n\n\n\n\n\nTo do this, we need to do the following:\n\ncreate a linear model without the interaction term (we did this previously)\nextract the predicted values of the model\nplot these against the original data\n\n\n# get predicted values\nlm_treelight_add_py.predict()\n\narray([4586.83958789, 4390.21542165, 4193.59125541, 3996.96708918,\n       3800.34292294, 3603.7187567 , 3407.09459046, 3210.47042422,\n       3013.84625799, 2817.22209175, 2620.59792551, 2423.97375927,\n       2227.34959303, 7322.87199901, 7047.59816627, 5781.86286681,\n       7543.35323075, 6319.56442008, 7267.03073579, 7773.27242247,\n       5822.76069339, 7766.98044915, 6532.70501628])\n\n\nWe can’t easily use the predicted values in this kind of format, so we’re adding them to the existing data, in a column called .fitted:\n\n# add predicted values to data set\ntreelight_py['.fitted'] = lm_treelight_add_py.predict()\n\n# have a peek at the data\ntreelight_py.head()\n\n   id        light  depth  species      .fitted\n0   1  4105.646110   1.00  Conifer  4586.839588\n1   2  4933.925144   1.75  Conifer  4390.215422\n2   3  4416.527443   2.50  Conifer  4193.591255\n3   4  4528.618186   3.25  Conifer  3996.967089\n4   5  3442.610306   4.00  Conifer  3800.342923\n\n\nNow we can simply plot the data:\n\n# plot the data\n(ggplot(treelight_py,\n        aes(x = \"depth\",\n            y = \"light\",\n            colour = \"species\")) +\n     geom_point() +\n     # add regression lines\n     geom_line(aes(x = \"depth\",\n                   y = \".fitted\",\n                   colour = \"species\")) +\n     scale_color_brewer(type = \"qual\", palette = \"Dark2\") +\n     labs(x = \"Depth (m)\",\n          y = \"Light intensity (lux)\"))",
    "crumbs": [
      "Two predictors",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Linear regression with grouped data</span>"
    ]
  },
  {
    "objectID": "materials/cs4_practical_linear-regression-grouped-data.html#assumptions",
    "href": "materials/cs4_practical_linear-regression-grouped-data.html#assumptions",
    "title": "15  Linear regression with grouped data",
    "section": "15.6 Assumptions",
    "text": "15.6 Assumptions\nIn this case we first wanted to check if the interaction was significant, prior to checking the assumptions. If we would have checked the assumptions first, then we would have done that one the full model (with the interaction), then done the ANOVA if everything was OK. We would have then found out that the interaction was not significant, meaning we’d have to re-check the assumptions with the new model. In what order you do it is a bit less important here. The main thing is that you check the assumptions and report on it!\nAnyway, hopefully you’ve got the gist of checking assumptions for linear models by now: diagnostic plots!\n\nRPython\n\n\n\nresid_panel(lm_additive,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\n\n\nThe Residuals plot looks OK, no systematic pattern.\nThe Q-Q plot isn’t perfect, but I’m happy with the normality assumption.\nThe Location-Scale plot is OK, some very slight suggestion of heterogeneity of variance, but nothing to be too worried about.\nThe Cook’s D plot shows that all of the points are OK\n\n\n\n\ndgplots(lm_treelight_add_py)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Residuals plot looks OK, no systematic pattern.\nThe Q-Q plot isn’t perfect, but I’m happy with the normality assumption.\nThe Location-Scale plot is OK, some very slight suggestion of heterogeneity of variance, but nothing to be too worried about.\nThe Influential points plot shows that all of the points are OK\n\n\n\n\nWoohoo!",
    "crumbs": [
      "Two predictors",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Linear regression with grouped data</span>"
    ]
  },
  {
    "objectID": "materials/cs4_practical_linear-regression-grouped-data.html#dealing-with-interaction",
    "href": "materials/cs4_practical_linear-regression-grouped-data.html#dealing-with-interaction",
    "title": "15  Linear regression with grouped data",
    "section": "15.7 Dealing with interaction",
    "text": "15.7 Dealing with interaction\nIf there had been a significant interaction between the two predictor variables (for example, if light intensity had dropped off significantly faster in conifer woods than in broad leaf woods, in addition to being lower overall, then we would again be looking for two equations for the linear regression, but this time the gradients vary as well. In this case interaction is important and so we need the output from a linear regression that explicitly includes the interaction term:\n\nRPython\n\n\n\nlm(light ~ depth + species + depth:species,\n   data = treelight)\n\nor written using the short-hand:\n\nlm(light ~ depth * species,\n   data = treelight)\n\nThere really is absolutely no difference in the end result. Either way this gives us the following output:\n\n\n\nCall:\nlm(formula = light ~ depth * species, data = treelight)\n\nCoefficients:\n         (Intercept)                 depth        speciesConifer  \n             7798.57               -221.13              -2784.58  \ndepth:speciesConifer  \n              -71.04  \n\n\nAs before the Broadleaf line is used as the baseline regression and we can read off the values for its intercept and slope directly:\nBroadleaf: \\[\\begin{equation}\nlight = 7798.57 + -221.13 \\times depth\n\\end{equation}\\]\nNote that this is different from the previous section, by allowing for an interaction all fitted values will change.\nFor the conifer line we will have a different intercept value and a different gradient value. As before the value underneath speciesConifer gives us the difference between the intercept of the conifer line and the broad leaf line. The new, additional term depth:speciesConifer tells us how the coefficient of depth varies for the conifer line i.e. how the gradient is different. Putting these two together gives us the following equation for the regression line conifer woodland:\nConifer: \\[\\begin{equation}\nlight = (7798.57 + -2784.58) + (-221.13 + -71.04) \\times depth\n\\end{equation}\\]\n\\[\\begin{equation}\nlight = 5014 + -292.2 \\times depth\n\\end{equation}\\]\n\n\nWe’ve actually created this model before, but for clarity we’ll define it here again.\n\n# create a linear model\nmodel = smf.ols(formula = \"light ~ depth * C(species)\",\n                data = treelight_py)\n\n# and get the fitted parameters of the model\nlm_treelight_py = model.fit()\n\nWe get the model parameters as follows:\n\nprint(lm_treelight_py.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  light   R-squared:                       0.938\nModel:                            OLS   Adj. R-squared:                  0.928\nMethod:                 Least Squares   F-statistic:                     95.71\nDate:                Tue, 16 Apr 2024   Prob (F-statistic):           1.19e-11\nTime:                        07:43:54   Log-Likelihood:                -174.91\nNo. Observations:                  23   AIC:                             357.8\nDf Residuals:                      19   BIC:                             362.4\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n===============================================================================================\n                                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------------------\nIntercept                    7798.5655    298.623     26.115      0.000    7173.541    8423.590\nC(species)[T.Conifer]       -2784.5833    442.274     -6.296      0.000   -3710.274   -1858.893\ndepth                        -221.1256     61.802     -3.578      0.002    -350.478     -91.773\ndepth:C(species)[T.Conifer]   -71.0357     81.309     -0.874      0.393    -241.217      99.145\n==============================================================================\nOmnibus:                        1.435   Durbin-Watson:                   2.176\nProb(Omnibus):                  0.488   Jarque-Bera (JB):                1.269\nSkew:                           0.444   Prob(JB):                        0.530\nKurtosis:                       2.267   Cond. No.                         31.8\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nAs before the Broadleaf line is used as the baseline regression and we can read off the values for its intercept and slope directly:\nBroadleaf: \\[\\begin{equation}\nlight = 7798.57 + -221.13 \\times depth\n\\end{equation}\\]\nNote that this is different from the previous section, by allowing for an interaction all fitted values will change.\nFor the conifer line we will have a different intercept value and a different gradient value. As before the value next to C(species)[T.Conifer] gives us the difference between the intercept of the conifer line and the broad leaf line. The interaction term depth:C(species)[T.Conifer] tells us how the coefficient of depth varies for the conifer line i.e. how the gradient is different. Putting these two together gives us the following equation for the regression line conifer woodland:\nConifer: \\[\\begin{equation}\nlight = (7798.57 + -2784.58) + (-221.13 + -71.04) \\times depth\n\\end{equation}\\]\n\\[\\begin{equation}\nlight = 5014 + -292.2 \\times depth\n\\end{equation}\\]\n\n\n\nThese also happen to be exactly the regression lines that you would get by calculating a linear regression on each group’s data separately.",
    "crumbs": [
      "Two predictors",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Linear regression with grouped data</span>"
    ]
  },
  {
    "objectID": "materials/cs4_practical_linear-regression-grouped-data.html#exercises",
    "href": "materials/cs4_practical_linear-regression-grouped-data.html#exercises",
    "title": "15  Linear regression with grouped data",
    "section": "15.8 Exercises",
    "text": "15.8 Exercises\n\n15.8.1 Clover and yarrow\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLevel: \nClover and yarrow field trials\nThe data/CS4-clover.csv data set contains information on field trials at three different farms (A, B and C). Each farm recorded the yield of clover in each of ten fields along with the density of yarrow stalks in each field.\n\nInvestigate how clover yield is affected by yarrow stalk density. Is there evidence of competition between the two species?\nIs there a difference between farms?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nRPython\n\n\n\nclover &lt;- read_csv(\"data/CS4-clover.csv\")\n\nThis data set has three variables; yield (which is the response variable), yarrow (which is a continuous predictor variable) and farm (which is the categorical predictor variables). As always we’ll visualise the data first:\n\n# plot the data\nggplot(clover,\n       aes(x = yarrow, y = yield,\n           colour = farm)) +\n  geom_point() +\n  scale_color_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n\nLooking at this plot as it stands, it’s pretty clear that yarrow density has a significant effect on yield, but it’s pretty hard to see from the plot whether there is any effect of farm, or whether there is any interaction. In order to work that out we’ll want to add the regression lines for each farm separately.\n\n# plot the data\nggplot(clover,\n       aes(x = yarrow, y = yield,\n           colour = farm, group = farm)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  scale_color_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n\nThe regression lines are very close together. Although the line for Farm C crosses the other two, it doesn’t look like we’re going to see a very strong interaction - if there is one. There doesn’t seem to be an effect of farm in general, since the lines are not separate from each other.\nLet’s carry out the analysis:\n\nlm_clover &lt;- lm(yield ~ yarrow * farm,\n                data = clover)\n\nanova(lm_clover)\n\nAnalysis of Variance Table\n\nResponse: yield\n            Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nyarrow       1 8538.3  8538.3 28.3143 1.847e-05 ***\nfarm         2    3.8     1.9  0.0063    0.9937    \nyarrow:farm  2  374.7   187.4  0.6213    0.5457    \nResiduals   24 7237.3   301.6                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis confirms our suspicions from looking at the plot. There isn’t any interaction between yarrow and farm. yarrow density has a statistically significant effect on yield but there isn’t any difference between the different farms on the yields of clover.\nLet’s check the assumptions:\n\nresid_panel(lm_clover,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\n\nThis is a borderline case.\n\nNormality is fine (Q-Q plot)\nThere aren’t any highly influential points (Cook’s D plot)\nThere is a strong suggestion of heterogeneity of variance (Location-Scale plot). Most of the points are relatively close to the regression lines, but the there is a much great spread of points low yarrow density (which corresponds to high yield values, which is what the predicted values correspond to).\nFinally, there is a slight suggestion that the data might not be linear, that it might curve slightly (Residual plot).\n\n\n\n\nclover_py = pd.read_csv(\"data/CS4-clover.csv\")\n\nThis data set has three variables; yield (which is the response variable), yarrow (which is a continuous predictor variable) and farm (which is the categorical predictor variables). As always we’ll visualise the data first:\n\n# plot the data\n(ggplot(clover_py,\n        aes(x = \"yarrow\",\n            y = \"yield\", colour = \"farm\")) +\n     geom_point() +\n     scale_color_brewer(type = \"qual\", palette = \"Dark2\"))\n\n\n\n\n\n\n\n\nLooking at this plot as it stands, it’s pretty clear that yarrow density has a significant effect on yield, but it’s pretty hard to see from the plot whether there is any effect of farm, or whether there is any interaction. In order to work that out we’ll want to add the regression lines for each farm separately.\n\n# plot the data\n(ggplot(clover_py,\n        aes(x = \"yarrow\",\n            y = \"yield\", colour = \"farm\")) +\n     geom_point() +\n     geom_smooth(method = \"lm\", se = False) +\n     scale_color_brewer(type = \"qual\", palette = \"Dark2\"))\n\n\n\n\n\n\n\n\nThe regression lines are very close together. Although the line for Farm C crosses the other two, it doesn’t look like we’re going to see a very strong interaction - if there is one. There doesn’t seem to be an effect of farm in general, since the lines are not separate from each other.\nLet’s carry out the analysis. First, we need to change the name of the yield column, because yield is a keyword in Python, and we can’t use it inside the model formula. How annoying.\n\n# rename yield column\nclover_py = clover_py.rename(columns = {\"yield\": \"clover_yield\"})\n\n# create a linear model\nmodel = smf.ols(formula = \"clover_yield ~ yarrow * C(farm)\",\n                data = clover_py)\n\n# and get the fitted parameters of the model\nlm_clover_py = model.fit()\n\nPerform an ANOVA on the model:\n\nsm.stats.anova_lm(lm_clover_py, typ = 2)\n\n                     sum_sq    df          F    PR(&gt;F)\nC(farm)            3.808918   2.0   0.006315  0.993706\nyarrow          8540.833898   1.0  28.322674  0.000018\nyarrow:C(farm)   374.718749   2.0   0.621312  0.545659\nResidual        7237.311354  24.0        NaN       NaN\n\n\nThis confirms our suspicions from looking at the plot. There isn’t any interaction between yarrow and farm. yarrow density has a statistically significant effect on yield but there isn’t any difference between the different farms on the yields of clover.\nLet’s check the assumptions:\n\ndgplots(lm_clover_py)\n\n\n\n\n\n\n\n\n\n\nThis is a borderline case.\n\nNormality is fine (Q-Q plot)\nThere aren’t any highly influential points (Influential points plot)\nThere is a strong suggestion of heterogeneity of variance (Location-Scale plot). Most of the points are relatively close to the regression lines, but the there is a much great spread of points low yarrow density (which corresponds to high yield values, which is what the predicted values correspond to).\nFinally, there is a slight suggestion that the data might not be linear, that it might curve slightly (Residuals plot).\n\n\n\n\nWe have two options; both of which are arguably OK to do in real life.\n\nWe can claim that these assumptions are well enough met and just report the analysis that we’ve just done.\nWe can decide that the analysis is not appropriate and look for other options.\n\nWe can try to transform the data by taking logs of yield. This might fix both of our problems: taking logs of the response variable has the effect of improving heterogeneity of variance when the Residuals plot is more spread out on the right vs. the left (like ours). It also is appropriate if we think the true relationship between the response and predictor variables is exponential rather than linear (which we might have). We do have the capabilities to try this option.\nWe could try a permutation based approach (beyond the remit of this course, and actually a bit tricky in this situation). This wouldn’t address the non-linearity but it would deal with the variance assumption.\nWe could come up with a specific functional, mechanistic relationship between yarrow density and clover yield based upon other aspects of their biology. For example there might be a threshold effect such that for yarrow densities below a particular value, clover yields are unaffected, but as soon as yarrow values get above that threshold the clover yield decreases (maybe even linearly). This would require a much better understanding of clover-yarrow dynamics (of which I personally know very little).\n\n\nLet’s do a quick little transformation of the data, and repeat our analysis see if our assumptions are better met this time (just for the hell of it):\n\nRPython\n\n\n\n# plot log-transformed data\nggplot(clover,\n       aes(x = yarrow, y = log(yield), colour = farm)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  scale_color_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n\n\n\nTo log-transform our data, we require numpy.\n\n# import numpy\nimport numpy as np\n\nFirst, we create a new column containing the log-transformed data:\n\nclover_py[\"log_yield\"] = np.log(clover_py[\"clover_yield\"])\n\nThen we can plot them:\n\n# plot log-transformed data\n(ggplot(clover_py,\n         aes(x = \"yarrow\",\n             y = \"log_yield\", colour = \"farm\")) +\n     geom_point() +\n     geom_smooth(method = \"lm\", se = False) +\n     scale_color_brewer(type = \"qual\", palette = \"Dark2\"))\n\n\n\n\n\n\n\n\n\n\n\nAgain, this looks plausible. There’s a noticeable outlier from Farm B (data point at the bottom of the plot) but otherwise we see that: there probably isn’t an interaction; there is likely to be an effect of yarrow on log_yield; and there probably isn’t any difference between the farms.\nLet’s do the analysis:\n\nRPython\n\n\n\n# define linear model\nlm_log_clover &lt;- lm(log(yield) ~ yarrow * farm,\n                    data = clover)\n\nanova(lm_log_clover)\n\nAnalysis of Variance Table\n\nResponse: log(yield)\n            Df  Sum Sq Mean Sq F value   Pr(&gt;F)    \nyarrow       1 10.6815 10.6815 27.3233 2.34e-05 ***\nfarm         2  0.0862  0.0431  0.1103   0.8960    \nyarrow:farm  2  0.8397  0.4199  1.0740   0.3575    \nResiduals   24  9.3823  0.3909                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWoop. All good so far. We have the same conclusions as before in terms of what is significant and what isn’t. Now we just need to check the assumptions:\n\nresid_panel(lm_log_clover,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n# create a linear model\nmodel = smf.ols(formula = \"log_yield ~ yarrow * C(farm)\",\n                data = clover_py)\n\n# and get the fitted parameters of the model\nlm_log_clover_py = model.fit()\n\nPerform an ANOVA on the model:\n\nsm.stats.anova_lm(lm_clover_py, typ = 2)\n\n                     sum_sq    df          F    PR(&gt;F)\nC(farm)            3.808918   2.0   0.006315  0.993706\nyarrow          8540.833898   1.0  28.322674  0.000018\nyarrow:C(farm)   374.718749   2.0   0.621312  0.545659\nResidual        7237.311354  24.0        NaN       NaN\n\n\nWoop. All good so far. We have the same conclusions as before in terms of what is significant and what isn’t. Now we just need to check the assumptions:\n\ndgplots(lm_clover_py)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWell, this is actually a better set of diagnostic plots. Whilst one data point (for example in the Q-Q plot) is a clear outlier, if we ignore that point then all of the other plots do look better.\nSo now we know that yarrow is a significant predictor of yield and we’re happy that the assumptions have been met.",
    "crumbs": [
      "Two predictors",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Linear regression with grouped data</span>"
    ]
  },
  {
    "objectID": "materials/cs4_practical_linear-regression-grouped-data.html#summary",
    "href": "materials/cs4_practical_linear-regression-grouped-data.html#summary",
    "title": "15  Linear regression with grouped data",
    "section": "15.9 Summary",
    "text": "15.9 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nA linear regression analysis with grouped data is used when we have one categorical and one continuous predictor variable, together with one continuous response variable\nWe can visualise the data by plotting a regression line together with the original data\nWhen performing an ANOVA, we need to check for interaction terms\nWe check the underlying assumptions using diagnostic plots\nWe can create an equation for the regression line for each group in the data using the parameter from the linear model output",
    "crumbs": [
      "Two predictors",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Linear regression with grouped data</span>"
    ]
  },
  {
    "objectID": "materials/cs5_practical_multiple-linear-regression.html",
    "href": "materials/cs5_practical_multiple-linear-regression.html",
    "title": "16  Multiple linear regression",
    "section": "",
    "text": "16.1 Libraries and functions",
    "crumbs": [
      "Multiple predictors",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Multiple linear regression</span>"
    ]
  },
  {
    "objectID": "materials/cs5_practical_multiple-linear-regression.html#libraries-and-functions",
    "href": "materials/cs5_practical_multiple-linear-regression.html#libraries-and-functions",
    "title": "16  Multiple linear regression",
    "section": "",
    "text": "Click to expand\n\n\n\n\n\n\nRPython\n\n\n\n16.1.1 Libraries\n\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\n# Converts stats functions to a tidyverse-friendly format\nlibrary(rstatix)\n\n# Creates diagnostic plots using ggplot2\nlibrary(ggResidpanel)\n\n# Helper functions for tidying data\nlibrary(broom)\n\n\n\n16.1.2 Functions\n\n# Gets underlying data out of model object\nbroom::augment()\n\n# Creates diagnostic plots\nggResidpanel::resid_panel()\n\n# Performs an analysis of variance\nstats::anova()\n\n# Creates a linear model\nstats::lm()\n\n\n\n\n\n16.1.3 Libraries\n\n# A fundamental package for scientific computing in Python\nimport numpy as np\n\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Simple yet exhaustive stats functions.\nimport pingouin as pg\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# Statistical models, conducting tests and statistical data exploration\nimport statsmodels.api as sm\n\n# Convenience interface for specifying models using formula strings and DataFrames\nimport statsmodels.formula.api as smf\n\n\n\n16.1.4 Functions\n\n# Reads in a .csv file\npandas.read_csv()\n\n# Creates a model from a formula and data frame\nstatsmodels.formula.api.ols()",
    "crumbs": [
      "Multiple predictors",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Multiple linear regression</span>"
    ]
  },
  {
    "objectID": "materials/cs5_practical_multiple-linear-regression.html#purpose-and-aim",
    "href": "materials/cs5_practical_multiple-linear-regression.html#purpose-and-aim",
    "title": "16  Multiple linear regression",
    "section": "16.2 Purpose and aim",
    "text": "16.2 Purpose and aim\nRevisiting the linear model framework and expanding to systems with three predictor variables.",
    "crumbs": [
      "Multiple predictors",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Multiple linear regression</span>"
    ]
  },
  {
    "objectID": "materials/cs5_practical_multiple-linear-regression.html#data-and-hypotheses",
    "href": "materials/cs5_practical_multiple-linear-regression.html#data-and-hypotheses",
    "title": "16  Multiple linear regression",
    "section": "16.3 Data and hypotheses",
    "text": "16.3 Data and hypotheses\nThe data set we’ll be using is located in data/CS5-pm2_5.csv. It contains data on air pollution levels measured in London, in 2019. It also contains several meteorological measurements. Each variable was recorded on a daily basis.\nNote: some of the variables are based on simulations.\nIt contains the following variables:\n\n\n\nvariable\nexplanation\n\n\n\n\navg_temp\naverage daily temperature (\\(^\\circ C\\))\n\n\ndate\ndate of record\n\n\nlocation\nlocation in London (inner or outer)\n\n\npm2_5\nconcentration of PM2.5 (\\(\\mu g / m^3\\))\n\n\nrain_mm\ndaily rainfall in mm (same across both locations)\n\n\nwind_m_s\nwind speed in \\(m/s\\)",
    "crumbs": [
      "Multiple predictors",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Multiple linear regression</span>"
    ]
  },
  {
    "objectID": "materials/cs5_practical_multiple-linear-regression.html#summarise-and-visualise",
    "href": "materials/cs5_practical_multiple-linear-regression.html#summarise-and-visualise",
    "title": "16  Multiple linear regression",
    "section": "16.4 Summarise and visualise",
    "text": "16.4 Summarise and visualise\n\nRPython\n\n\nLet’s first load the data:\n\npm2_5 &lt;- read_csv(\"data/CS5-pm2_5.csv\")\n\nRows: 730 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): date, location\ndbl (4): avg_temp, pm2_5, rain_mm, wind_m_s\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(pm2_5)\n\n# A tibble: 6 × 6\n  avg_temp date       location pm2_5 rain_mm wind_m_s\n     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1      4.5 01/01/2019 inner     17.1     2.3     3.87\n2      4.9 01/01/2019 outer     10.8     2.3     5.84\n3      4.3 02/01/2019 inner     14.9     2.3     3.76\n4      4.8 02/01/2019 outer     11.4     2.3     6   \n5      4   03/01/2019 inner     18.5     1.4     2.13\n6      4.5 03/01/2019 outer     15.0     1.4     2.57\n\n\nIt’s the pm2_5 response variable we’re interested in here. Let’s start by checking if there might be a difference between PM2.5 level in inner and outer London:\n\nggplot(pm2_5,\n       aes(x = location, y = pm2_5)) +\n    geom_boxplot() +\n    geom_jitter(width = 0.1, alpha = 0.3)\n\n\n\n\n\n\n\n\nI’ve added the (jittered) data to the plot, with some transparency (alpha = 0.3). It’s always good to look at the actual data and not just summary statistics (which is what the box plot is).\nThere seems to be quite a difference between the PM2.5 levels in the two London areas, with the levels in inner London being markedly higher. I’m not surprised by this! So when we do our statistical testing, I would expect to find a clear difference between the locations.\nApart from the location, there are quite a few numerical descriptor variables. We could plot them one-by-one, but that’s a bit tedious. So instead we use the pairs() function again. This only works on numerical data, so we select all the columns that are numeric with select_if(is.numeric):\n\npm2_5 %&gt;% \n    select_if(is.numeric) %&gt;% \n    pairs(lower.panel = NULL)\n\n\n\n\n\n\n\n\nWe can see that there is not much of a correlation between pm2_5 and avg_temp or rain_mm, whereas there might be something going on in relation to wind_m_s.\nOther notable things include that rainfall seems completely independent of wind speed (rain fall seems pretty constant). Nor does the average temperature seem in any way related to wind speed (it looks like a random collection of data points!).\nWe can visualise the relationship between pm2_5 and wind_m_s in a bit more detail, by plotting the data against each other and colouring by location:\n\nggplot(pm2_5,\n       aes(x = wind_m_s, y = pm2_5,\n           colour = location)) +\n    geom_point()\n\n\n\n\n\n\n\n\nThis seems to show that there might be some linear relationship between PM2.5 levels and wind speed.\nAnother way of looking at this would be to create a correlation matrix, like we did before in the correlations chapter:\n\npm2_5 %&gt;% \n    select_if(is.numeric) %&gt;% \n    cor()\n\n            avg_temp       pm2_5     rain_mm    wind_m_s\navg_temp  1.00000000  0.03349457  0.03149221 -0.01107855\npm2_5     0.03349457  1.00000000 -0.02184951 -0.41733945\nrain_mm   0.03149221 -0.02184951  1.00000000  0.04882097\nwind_m_s -0.01107855 -0.41733945  0.04882097  1.00000000\n\n\nThis confirms what we saw in the plots, there aren’t any very strong correlations between the different (numerical) variables, apart from a negative correlation between pm2_5 and wind_m_s, which has a Pearson’s r of \\(r\\) = -0.42.\n\n\nLet’s first load the data:\n\npm2_5_py = pd.read_csv(\"data/CS5-pm2_5.csv\")\n\npm2_5_py.head()\n\n   avg_temp        date location   pm2_5  rain_mm  wind_m_s\n0       4.5  01/01/2019    inner  17.126      2.3      3.87\n1       4.9  01/01/2019    outer  10.821      2.3      5.84\n2       4.3  02/01/2019    inner  14.884      2.3      3.76\n3       4.8  02/01/2019    outer  11.416      2.3      6.00\n4       4.0  03/01/2019    inner  18.471      1.4      2.13\n\n\nIt’s the pm2_5 response variable we’re interested in here. Let’s start by checking if there might be a difference between PM2.5 level in inner and outer London:\n\n(ggplot(pm2_5_py, aes(x = \"location\", y = \"pm2_5\")) +\n    geom_boxplot() +\n    geom_jitter(width = 0.1, alpha = 0.7))\n\n\n\n\n\n\n\n\nI’ve added the (jittered) data to the plot, with some transparency (alpha = 0.7). It’s always good to look at the actual data and not just summary statistics (which is what the box plot is).\nThere seems to be quite a difference between the PM2.5 levels in the two London areas, with the levels in inner London being markedly higher. I’m not surprised by this! So when we do our statistical testing, I would expect to find a clear difference between the locations.\nApart from the location, there are quite a few numerical descriptor variables. At this point I should probably bite the bullet and install seaborn, so I can use the pairplot() function.\nBut I’m not going to ;-)\nI’ll just tell you that there is not much of a correlation between pm2_5 and avg_temp or rain_mm, whereas there might be something going on in relation to wind_m_s. So I plot that instead and colour it by location:\n\n(ggplot(pm2_5_py,\n        aes(x = \"wind_m_s\",\n            y = \"pm2_5\",\n            colour = \"location\")) +\n     geom_point())\n\n\n\n\n\n\n\n\nThis seems to show that there might be some linear relationship between PM2.5 levels and wind speed.\nIf I would plot all the other variables against each other, then I would spot that rainfall seems completely independent of wind speed (rain fall seems pretty constant). Nor does the average temperature seem in any way related to wind speed (it looks like a random collection of data points!). You can check this yourself!\nAnother way of looking at this would be to create a correlation matrix, like we did before in the correlations chapter:\n\npm2_5_py.corr()\n\n          avg_temp     pm2_5   rain_mm  wind_m_s\navg_temp  1.000000  0.033495  0.031492 -0.011079\npm2_5     0.033495  1.000000 -0.021850 -0.417339\nrain_mm   0.031492 -0.021850  1.000000  0.048821\nwind_m_s -0.011079 -0.417339  0.048821  1.000000\n\n\nThis confirms what we saw in the plots, there aren’t any very strong correlations between the different (numerical) variables, apart from a negative correlation between pm2_5 and wind_m_s, which has a Pearson’s r of \\(r\\) = -0.42.",
    "crumbs": [
      "Multiple predictors",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Multiple linear regression</span>"
    ]
  },
  {
    "objectID": "materials/cs5_practical_multiple-linear-regression.html#implement-and-interpret-the-test",
    "href": "materials/cs5_practical_multiple-linear-regression.html#implement-and-interpret-the-test",
    "title": "16  Multiple linear regression",
    "section": "16.5 Implement and interpret the test",
    "text": "16.5 Implement and interpret the test\nFrom our initial observations we derived that there might be some relationship between PM2.5 levels and wind speed. We also noticed that this is likely to be different between inner and outer London.\nIf we would want to test for every variable and interaction, then we would end up with a rather huge model, which would even include 3-way and a 4-way interaction! To illustrate the point that the process of model testing applies to as many variables as you like, we’re adding the avg_temp and rain_mm variables to our model.\nSo in this case we create a model that takes into account all of the main effects (avg_temp, location, rain_mm, wind_m_s). We also include a potential two-way interaction (location:wind_m_s). The two-way interaction may be of interest since the PM2.5 levels in response to wind speed seem to differ between the two locations.\nOur model is then as follows:\npm2_5 ~ avg_temp + location + rain_mm + wind_m_s + wind_m_s:location\nSo let’s define and explore it!\n\nRPython\n\n\nWe write the model as follows:\n\nlm_pm2_5_full &lt;- lm(pm2_5 ~ avg_temp + location +\n                            rain_mm + wind_m_s +\n                            wind_m_s:location,\n                    data = pm2_5)\n\nLet’s look at the coefficients:\n\nlm_pm2_5_full\n\n\nCall:\nlm(formula = pm2_5 ~ avg_temp + location + rain_mm + wind_m_s + \n    wind_m_s:location, data = pm2_5)\n\nCoefficients:\n           (Intercept)                avg_temp           locationouter  \n              18.18286                 0.01045                -2.07084  \n               rain_mm                wind_m_s  locationouter:wind_m_s  \n              -0.02788                -0.28545                -0.42945  \n\n\n\n\n\n\n\n\nExtracting coefficients with tidy()\n\n\n\n\n\nThis will give us quite a few coefficients, so instead of just calling the lm object, I’m restructuring the output using the tidy() function from the broom package. It’s installed with tidyverse but you have to load it separately using library(broom).\n\nlm_pm2_5_full %&gt;%\n    tidy() %&gt;% \n    select(term, estimate)\n\n# A tibble: 6 × 2\n  term                   estimate\n  &lt;chr&gt;                     &lt;dbl&gt;\n1 (Intercept)             18.2   \n2 avg_temp                 0.0105\n3 locationouter           -2.07  \n4 rain_mm                 -0.0279\n5 wind_m_s                -0.285 \n6 locationouter:wind_m_s  -0.429 \n\n\n\n\n\nThe question is, are all of these terms statistically significant? To find out we perform an ANOVA:\n\nanova(lm_pm2_5_full)\n\nAnalysis of Variance Table\n\nResponse: pm2_5\n                   Df  Sum Sq Mean Sq   F value  Pr(&gt;F)    \navg_temp            1    5.29    5.29    5.0422 0.02504 *  \nlocation            1 3085.37 3085.37 2940.5300 &lt; 2e-16 ***\nrain_mm             1    2.48    2.48    2.3644 0.12457    \nwind_m_s            1  728.13  728.13  693.9481 &lt; 2e-16 ***\nlocation:wind_m_s   1  134.82  134.82  128.4912 &lt; 2e-16 ***\nResiduals         724  759.66    1.05                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFrom this we can see that the interaction between location and wind_m_s is statistically significant. Which means that we can’t just talk about the effect of location or wind_m_s on PM2.5 levels, without taking the other variable into account!\nThe p-value for the avg_temp is significant, whereas the rain_mm main effect is not. This means that rain fall is not contributing much to model’s ability to explain our data. This matches what we already saw when we visualised the data.\nWhat to do? We’ll explore this in more detail in the chapter on model comparisons, but for now the most sensible option would be to redefine the model, but exclude the rain_mm variable. Here I have rewritten the model and named it lm_pm2_5_red to indicate it is a reduced model (with fewer variables than our original full model):\n\nlm_pm2_5_red &lt;- lm(pm2_5 ~ avg_temp + location + wind_m_s + location:wind_m_s, data = pm2_5)\n\nLet’s look at the new model coefficients:\n\nlm_pm2_5_red\n\n\nCall:\nlm(formula = pm2_5 ~ avg_temp + location + wind_m_s + location:wind_m_s, \n    data = pm2_5)\n\nCoefficients:\n           (Intercept)                avg_temp           locationouter  \n              18.13930                 0.01031                -2.07380  \n              wind_m_s  locationouter:wind_m_s  \n              -0.28631                -0.42880  \n\n\nAs we did in the linear regression on grouped data, we end up with two linear equations, one for inner London and one for outer London.\nOur reference group is inner (remember, it takes a reference group in alphabetical order and we can see outer in the output).\nSo we end up with:\n\\(PM2.5_{inner} = 18.14 + 0.01 \\times avg\\_temp - 0.29 \\times wind\\_m\\_s\\)\n\\(PM2.5_{outer} = (18.14 - 2.07) + 0.01 \\times avg\\_temp + (-0.29 - 0.43) \\times wind\\_m\\_s\\)\nwhich gives\n\\(PM2.5_{outer} = 16.07 + 0.01 \\times avg\\_temp - 0.72 \\times wind\\_m\\_s\\)\nWe still need to check the assumptions of the model:\n\nresid_panel(lm_pm2_5_red,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThey all look pretty good, with the only weird thing being a small empty zone of predicted values just under 16. Nothing that is getting me worried though.\nIt’d be useful to visualise the model. We can take the model and use the augment() function to extract the fitted values (.fitted). These are the values for pm2_5 that the model is predicting. We can then plot these against the wind_m_s measurements, colouring by location:\n\nlm_pm2_5_red %&gt;% \n  augment() %&gt;% \n  ggplot(aes(x = wind_m_s,\n             y = pm2_5, colour = location)) +\n  geom_point() +\n  geom_smooth(aes(y = .fitted))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nWe write the model as follows:\n\n# create a linear model\nmodel = smf.ols(formula = \"pm2_5 ~ avg_temp + C(location) + rain_mm + wind_m_s + wind_m_s:location\", data = pm2_5_py)\n# and get the fitted parameters of the model\nlm_pm2_5_full_py = model.fit()\n\nThis will give us quite a few coefficients, so instead of just printing the entire summary table, we’re extracting the parameters with .params:\n\nlm_pm2_5_full_py.params\n\nIntercept                     18.182858\nC(location)[T.outer]          -2.070843\navg_temp                       0.010451\nrain_mm                       -0.027880\nwind_m_s                      -0.285450\nwind_m_s:location[T.outer]    -0.429455\ndtype: float64\n\n\nThe question is, are all of these terms statistically significant? To find out we perform an ANOVA:\n\nsm.stats.anova_lm(lm_pm2_5_full_py, typ = 2)\n\n                       sum_sq     df           F         PR(&gt;F)\nC(location)        123.803830    1.0  117.991919   1.441933e-25\navg_temp             1.803685    1.0    1.719012   1.902360e-01\nrain_mm              0.350639    1.0    0.334179   5.633886e-01\nwind_m_s           728.129799    1.0  693.948101  8.928636e-108\nwind_m_s:location  134.820234    1.0  128.491164   1.567268e-27\nResidual           759.661960  724.0         NaN            NaN\n\n\nFrom this we can see that the interaction between location and wind_m_s is statistically significant. Which means that we can’t just talk about the effect of location or wind_m_s on PM2.5 levels, without taking the other variable into account!\nThe p-value for the avg_temp is significant, whereas the rain_mm main effect is not. This means that rain fall is not contributing much to model’s ability to explain our data. This matches what we already saw when we visualised the data.\nWhat to do? We’ll explore this in more detail in the chapter on model comparisons, but for now the most sensible option would be to redefine the model, but exclude the rain_mm variable. Here I have rewritten the model and named it lm_pm2_5_red to indicate it is a reduced model (with fewer variables than our original full model):\n\n# create a linear model\nmodel = smf.ols(formula = \"pm2_5 ~ avg_temp + C(location) * wind_m_s\", data = pm2_5_py)\n# and get the fitted parameters of the model\nlm_pm2_5_red_py = model.fit()\n\nLet’s look at the new model coefficients:\n\nlm_pm2_5_red_py.params\n\nIntercept                        18.139300\nC(location)[T.outer]             -2.073802\navg_temp                          0.010312\nwind_m_s                         -0.286313\nC(location)[T.outer]:wind_m_s    -0.428800\ndtype: float64\n\n\nAs we did in the linear regression on grouped data, we end up with two linear equations, one for inner London and one for outer London.\nOur reference group is inner (remember, it takes a reference group in alphabetical order and we can see outer in the output).\nSo we end up with:\n\\(PM2.5_{inner} = 18.14 + 0.01 \\times avg\\_temp - 0.29 \\times wind\\_m\\_s\\)\n\\(PM2.5_{outer} = (18.14 - 2.07) + 0.01 \\times avg\\_temp + (-0.29 - 0.43) \\times wind\\_m\\_s\\)\nwhich gives\n\\(PM2.5_{outer} = 16.07 + 0.01 \\times avg\\_temp - 0.72 \\times wind\\_m\\_s\\)\nWe still need to check the assumptions of the model:\n\ndgplots(lm_pm2_5_red_py)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThey all look pretty good, with the only weird thing being a small empty zone of predicted values just under 16. Nothing that is getting me worried though.\nIt’d be useful to visualise the model. We can take the model and extract the fitted values (.fittedvalues). These are the pm2_5 that the model is predicting. We can then plot these against the wind_m_s measurements, colouring by location. We’re also adding the original values to the plot with geom_point():\n\n(ggplot(pm2_5_py, aes(x = \"wind_m_s\",\n                     y = \"pm2_5\",\n                     colour = \"location\")) +\n    geom_point() +\n    geom_smooth(aes(y = lm_pm2_5_red_py.fittedvalues)))",
    "crumbs": [
      "Multiple predictors",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Multiple linear regression</span>"
    ]
  },
  {
    "objectID": "materials/cs5_practical_multiple-linear-regression.html#exploring-models",
    "href": "materials/cs5_practical_multiple-linear-regression.html#exploring-models",
    "title": "16  Multiple linear regression",
    "section": "16.6 Exploring models",
    "text": "16.6 Exploring models\nRather than stop here however, we will use the concept of the linear model to its full potential and show that we can construct and analyse any possible combination of predictor variables for this data set. Namely we will consider the following four extra models, where reduce the complexity to the model, step-by-step:\n\n\n\nModel\nDescription\n\n\n\n\n1. pm2_5 ~ wind_m_s + location\nAn additive model\n\n\n2. pm2_5 ~ wind_m_s\nEquivalent to a simple linear regression\n\n\n3. pm2_5 ~ location\nEquivalent to a one-way ANOVA\n\n\n4. pm2_5 ~ 1\nThe null model, where we have no predictors\n\n\n\n\n16.6.1 Additive model\nTo create the additive model, we drop the interaction term (keep in mind, this is to demonstrate the process - we would normally not do this because the interaction term is significant!).\n\nRPython\n\n\nFirst, we define the model:\n\nlm_pm2_5_add &lt;- lm(pm2_5 ~ avg_temp + location + wind_m_s,\n                   data = pm2_5)\n\nWe can visualise this as follows:\n\nlm_pm2_5_add %&gt;% \n    augment() %&gt;% \n    ggplot(aes(x = wind_m_s, y = pm2_5,\n               colour = location)) +\n    geom_point() +\n    geom_smooth(aes(y = .fitted))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nNext, we extract the coefficient estimates:\n\nlm_pm2_5_add\n\n\nCall:\nlm(formula = pm2_5 ~ avg_temp + location + wind_m_s, data = pm2_5)\n\nCoefficients:\n  (Intercept)       avg_temp  locationouter       wind_m_s  \n     19.04867        0.01587       -4.05339       -0.49868  \n\n\n\n\nFirst, we define the model\n\n# create a linear model\nmodel = smf.ols(formula = \"pm2_5 ~ avg_temp + C(location) + wind_m_s\",\n                data = pm2_5_py)\n                \n# and get the fitted parameters of the model\nlm_pm2_5_add_py = model.fit()\n\nWe can visualise this as follows:\n\n(ggplot(pm2_5_py, aes(x = \"wind_m_s\",\n                      y = \"pm2_5\",\n                      colour = \"location\")) +\n    geom_point() +\n    geom_smooth(aes(y = lm_pm2_5_add_py.fittedvalues)))\n\n\n\n\n\n\n\n\nNext, we extract the coefficient estimates:\n\nlm_pm2_5_add_py.params\n\nIntercept               19.048672\nC(location)[T.outer]    -4.053394\navg_temp                 0.015872\nwind_m_s                -0.498683\ndtype: float64\n\n\n\n\n\nSo our two equations would be as follows:\n\\(PM2.5_{inner} = 19.04 + 0.016 \\times avg\\_temp - 0.50 \\times wind\\_m\\_s\\)\n\\(PM2.5_{outer} = (19.22 - 4.05) + 0.016 \\times avg\\_temp - 0.50 \\times wind\\_m\\_s\\)\ngives\n\\(PM2.5_{outer} = 15.17 + 0.016 \\times avg\\_temp - 0.50 \\times wind\\_m\\_s\\)\n\n\n16.6.2 Revisiting linear regression\n\nRPython\n\n\nFirst, we define the model:\n\nlm_pm2_5_wind &lt;- lm(pm2_5 ~ wind_m_s,\n                   data = pm2_5)\n\nWe can visualise this as follows:\n\nlm_pm2_5_wind %&gt;% \n    augment() %&gt;% \n    ggplot(aes(x = wind_m_s, y = pm2_5)) +\n    geom_point() +\n    geom_smooth(aes(y = .fitted))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlternative using geom_smooth()\n\n\n\n\n\n\nggplot(pm2_5, aes(x = wind_m_s, y = pm2_5)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\nNext, we extract the coefficient estimates:\n\nlm_pm2_5_wind\n\n\nCall:\nlm(formula = pm2_5 ~ wind_m_s, data = pm2_5)\n\nCoefficients:\n(Intercept)     wind_m_s  \n    17.3267      -0.5285  \n\n\n\n\nFirst, we define the model\n\n# create a linear model\nmodel = smf.ols(formula = \"pm2_5 ~ wind_m_s\",\n                data = pm2_5_py)\n                \n# and get the fitted parameters of the model\nlm_pm2_5_wind_py = model.fit()\n\nWe can visualise this as follows:\n\n(ggplot(pm2_5_py, aes(x = \"wind_m_s\",\n                      y = \"pm2_5\")) +\n    geom_point() +\n    geom_smooth(aes(y = lm_pm2_5_wind_py.fittedvalues), colour = \"blue\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlternative using geom_smooth()\n\n\n\n\n\n\n(ggplot(pm2_5_py, aes(x = \"wind_m_s\", y = \"pm2_5\")) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = False, colour = \"blue\"))\n\n\n\n\n\n\n\n\n\n\n\nNext, we extract the coefficient estimates:\n\nlm_pm2_5_wind_py.params\n\nIntercept    17.326708\nwind_m_s     -0.528510\ndtype: float64\n\n\n\n\n\nThis gives us the following equation:\n\\(PM2.5 = 17.33 - 0.53 \\times wind\\_m\\_s\\)\n\n\n16.6.3 Revisiting ANOVA\nIf we’re just looking at the effect of location, then we’re essentially doing a one-way ANOVA.\n\nRPython\n\n\nFirst, we define the model:\n\nlm_pm2_5_loc &lt;- lm(pm2_5 ~ location,\n                   data = pm2_5)\n\nWe can visualise this as follows:\n\nlm_pm2_5_loc %&gt;% \n    augment() %&gt;% \n    ggplot(aes(x = location, y = pm2_5)) +\n    geom_jitter(alpha = 0.3, width = 0.1) +\n    geom_point(aes(y = .fitted), colour = \"blue\", size = 3)\n\n\n\n\n\n\n\n\nOK, what’s going on here? I’ve plotted the .fitted values (the values predicted by the model) in blue and overlaid the original (with a little bit of jitter to avoid overplotting). However, there are only two predicted values!\nWe can check this and see that each unique fitted value occurs 365 times:\n\nlm_pm2_5_loc %&gt;% \n    augment() %&gt;% \n    count(location, .fitted)\n\n# A tibble: 2 × 3\n  location .fitted     n\n  &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;\n1 inner       16.9   365\n2 outer       12.8   365\n\n\nThis makes sense if we think back to our original ANOVA exploration. There we established that an ANOVA is just a special case of a linear model, where the fitted values are equal to the mean of each group.\nWe could even check this:\n\npm2_5 %&gt;% \n    group_by(location) %&gt;% \n    summarise(mean_pm2_5 = mean(pm2_5))\n\n# A tibble: 2 × 2\n  location mean_pm2_5\n  &lt;chr&gt;         &lt;dbl&gt;\n1 inner          16.9\n2 outer          12.8\n\n\nSo, that matches. We move on and extract the coefficient estimates:\n\nlm_pm2_5_loc\n\n\nCall:\nlm(formula = pm2_5 ~ location, data = pm2_5)\n\nCoefficients:\n  (Intercept)  locationouter  \n       16.943         -4.112  \n\n\nThese values match up exactly with the predicted values for each individual location.\n\n\nFirst, we define the model:\n\n# create a linear model\nmodel = smf.ols(formula = \"pm2_5 ~ C(location)\", data = pm2_5_py)\n# and get the fitted parameters of the model\nlm_pm2_5_loc_py = model.fit()\n\nWe can visualise this as follows:\n\n(ggplot(pm2_5_py, aes(x = \"location\",\n                     y = \"pm2_5\")) +\n    geom_jitter(alpha = 0.3, width = 0.1) +\n    geom_point(aes(y = lm_pm2_5_loc_py.fittedvalues), colour = \"blue\", size = 3))\n\n\n\n\n\n\n\n\nOK, what’s going on here? I’ve plotted the fittedvalues (the values predicted by the model) in blue and overlaid the original (with a little bit of jitter to avoid overplotting). However, there are only two predicted values!\nWe can check this and see that each unique fitted value occurs 365 times, using the value_counts() function on the fitted values:\n\nlm_pm2_5_loc_py.fittedvalues.value_counts()\n\n16.942926    365\n12.831356    365\ndtype: int64\n\n\nThis makes sense if we think back to our original ANOVA exploration. There we established that an ANOVA is just a special case of a linear model, where the fitted values are equal to the mean of each group.\nWe could even check this:\n\npm2_5_py.groupby(\"location\")[\"pm2_5\"].mean()\n\nlocation\ninner    16.942926\nouter    12.831356\nName: pm2_5, dtype: float64\n\n\nSo, that matches. We move on and extract the coefficient estimates:\n\nlm_pm2_5_loc_py.params\n\nIntercept               16.942926\nC(location)[T.outer]    -4.111570\ndtype: float64\n\n\nThese values match up exactly with the predicted values for each individual location.\n\n\n\nThis gives us the following equation:\n\\(\\bar{PM2.5_{inner}} = 16.94\\)\n\\(\\bar{PM2.5_{outer}} = 16.94 - 4.11 = 12.83\\)\n\n\n16.6.4 The null model\nThe null model by itself is rarely analysed for its own sake but is instead used a reference point for more sophisticated model selection techniques. It represents your data as an overal average value.\n\nRPython\n\n\nWe define the null model as follows:\n\nlm_pm2_5_null &lt;- lm(pm2_5 ~ 1, data = pm2_5)\n\nWe can just view the model:\n\nlm_pm2_5_null\n\n\nCall:\nlm(formula = pm2_5 ~ 1, data = pm2_5)\n\nCoefficients:\n(Intercept)  \n      14.89  \n\n\n\n\nWe define the null model as follows:\n\n# create a linear model\nmodel = smf.ols(formula = \"pm2_5 ~ 1\", data = pm2_5_py)\n# and get the fitted parameters of the model\nlm_pm2_5_null_py = model.fit()\n\nWe can just view the model parameters:\n\nlm_pm2_5_null_py.params\n\nIntercept    14.887141\ndtype: float64\n\n\n\n\n\nThis shows us that there is just one value: 14.89. This is the average across all the PM2.5 values in the data set.\nHere we’d predict the PM2.5 values as follows:\n\\(PM2.5 = 14.89\\)",
    "crumbs": [
      "Multiple predictors",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Multiple linear regression</span>"
    ]
  },
  {
    "objectID": "materials/cs5_practical_multiple-linear-regression.html#exercises",
    "href": "materials/cs5_practical_multiple-linear-regression.html#exercises",
    "title": "16  Multiple linear regression",
    "section": "16.7 Exercises",
    "text": "16.7 Exercises\n\n16.7.1 Trees\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLevel: \nTrees: an example with only continuous variables\nUse the data/CS5-trees.csv data set. This is a data frame with 31 observations of 3 continuous variables. The variables are the height height, diameter girth and timber volume volume of 31 felled black cherry trees.\nInvestigate the relationship between volume (as a dependent variable) and height and girth (as predictor variables).\n\nHere all variables are continuous and so there isn’t a way of producing a 2D plot of all three variables for visualisation purposes using R’s standard plotting functions.\nconstruct four linear models\n\nAssume volume depends on height, girth and an interaction between girth and height\nAssume volume depends on height and girth but that there isn’t any interaction between them.\nAssume volume only depends on girth (plot the result, with the regression line).\nAssume volume only depends on height (plot the result, with the regression line).\n\nFor each linear model write down the algebraic equation that the linear model produces that relates volume to the two continuous predictor variables.\nCheck the assumptions of each model. Do you have any concerns?\n\nNB: For two continuous predictors, the interaction term is simply the two values multiplied together (so girth:height means girth x height)\n\nUse the equations to calculate the predicted volume of a tree that has a diameter of 20 inches and a height of 67 feet in each case.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nLet’s construct the four linear models in turn.\n\nFull model\n\nRPython\n\n\nFirst, we read in the data:\n\ntrees &lt;- read_csv(\"data/CS5-trees.csv\")\n\n\n# define the model\nlm_trees_full &lt;- lm(volume ~ height * girth,\n                   data = trees)\n\n# view the model\nlm_trees_full\n\n\nCall:\nlm(formula = volume ~ height * girth, data = trees)\n\nCoefficients:\n (Intercept)        height         girth  height:girth  \n     69.3963       -1.2971       -5.8558        0.1347  \n\n\n\n\nFirst, we read in the data:\n\ntrees_py = pd.read_csv(\"data/CS5-trees.csv\")\n\n\n# create a linear model\nmodel = smf.ols(formula = \"volume ~ height * girth\",\n                data = trees_py)\n# and get the fitted parameters of the model\nlm_trees_full_py = model.fit()\n\nExtract the parameters:\n\nlm_trees_full_py.params\n\nIntercept       69.396316\nheight          -1.297083\ngirth           -5.855848\nheight:girth     0.134654\ndtype: float64\n\n\n\n\n\nWe can use this output to get the following equation:\nvolume = 69.40 + -1.30 \\(\\times\\) height + -5.86 \\(\\times\\) girth + 0.13 \\(\\times\\) height \\(\\times\\) girth\nIf we stick the numbers in (girth = 20 and height = 67) we get the following equation:\nvolume = 69.40 + -1.30 \\(\\times\\) 67 + -5.86 \\(\\times\\) 20 + 0.13 \\(\\times\\) 67 \\(\\times\\) 20\nvolume = 45.81\nHere we note that the interaction term just requires us to multiple the three numbers together (we haven’t looked at continuous predictors before in the examples and this exercise was included as a check to see if this whole process was making sense).\nIf we look at the diagnostic plots for the model using the following commands we get:\n\nRPython\n\n\n\nresid_panel(lm_trees_full,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\n\n\n\n\ndgplots(lm_trees_full_py)\n\n\n\n\n\n\n\n\n\n\n\n\n\nAll assumptions are OK.\n\nThere is some suggestion of heterogeneity of variance (with the variance being lower for small and large fitted (i.e. predicted volume) values), but that can be attributed to there only being a small number of data points at the edges, so I’m not overly concerned.\nSimilarly, there is a suggestion of snaking in the Q-Q plot (suggesting some lack of normality) but this is mainly due to the inclusion of one data point and overall the plot looks acceptable.\nThere are no highly influential points\n\n\n\nAdditive model\n\nRPython\n\n\n\n# define the model\nlm_trees_add &lt;- lm(volume ~ height + girth,\n                   data = trees)\n\n# view the model\nlm_trees_add\n\n\nCall:\nlm(formula = volume ~ height + girth, data = trees)\n\nCoefficients:\n(Intercept)       height        girth  \n   -57.9877       0.3393       4.7082  \n\n\n\n\n\n# create a linear model\nmodel = smf.ols(formula = \"volume ~ height + girth\",\n                data = trees_py)\n# and get the fitted parameters of the model\nlm_trees_add_py = model.fit()\n\nExtract the parameters:\n\nlm_trees_add_py.params\n\nIntercept   -57.987659\nheight        0.339251\ngirth         4.708161\ndtype: float64\n\n\n\n\n\nWe can use this output to get the following equation:\nvolume = -57.99 + 0.34 \\(\\times\\) height + 4.71 \\(\\times\\) girth\nIf we stick the numbers in (girth = 20 and height = 67) we get the following equation:\nvolume = -57.99 + 0.34 \\(\\times\\) 67 + 4.71 \\(\\times\\) 20\nvolume = 58.91\nIf we look at the diagnostic plots for the model using the following commands we get the following:\n\nRPython\n\n\n\nresid_panel(lm_trees_add,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\n\n\n\n\ndgplots(lm_trees_add_py)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis model isn’t great.\n\nThere is a worrying lack of linearity exhibited in the Residuals plot suggesting that this linear model isn’t appropriate.\nAssumptions of Normality seem OK\nEquality of variance is harder to interpret. Given the lack of linearity in the data it isn’t really sensible to interpret the Location-Scale plot as it stands (since the plot is generated assuming that we’ve fitted a straight line through the data), but for the sake of practising interpretation we’ll have a go. There is definitely suggestions of heterogeneity of variance here with a cluster of points with fitted values of around 20 having noticeably lower variance than the rest of the dataset.\nOne point is influential and if there weren’t issues with the linearity of the model I would remove this point and repeat the analysis. As it stands there isn’t much point.\n\n\n\nHeight-only model\n\nRPython\n\n\n\n# define the model\nlm_height &lt;- lm(volume ~ height,\n              data = trees)\n\n# view the model\nlm_height\n\n\nCall:\nlm(formula = volume ~ height, data = trees)\n\nCoefficients:\n(Intercept)       height  \n    -87.124        1.543  \n\n\n\n\n\n# create a linear model\nmodel = smf.ols(formula = \"volume ~ height\",\n                data = trees_py)\n# and get the fitted parameters of the model\nlm_height_py = model.fit()\n\nExtract the parameters:\n\nlm_height_py.params\n\nIntercept   -87.123614\nheight        1.543350\ndtype: float64\n\n\n\n\n\nWe can use this output to get the following equation:\nvolume = -87.12 + 1.54 \\(\\times\\) height\nIf we stick the numbers in (girth = 20 and height = 67) we get the following equation:\nvolume = -87.12 + 1.54 \\(\\times\\) 67\nvolume = 16.28\nIf we look at the diagnostic plots for the model using the following commands we get the following:\n\nRPython\n\n\n\nresid_panel(lm_height,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\n\n\n\n\ndgplots(lm_height_py)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis model also isn’t great.\n\nThe main issue here is the clear heterogeneity of variance. For trees with bigger volumes the data are much more spread out than for trees with smaller volumes (as can be seen clearly from the Location-Scale plot).\nApart from that, the assumption of Normality seems OK\nAnd there aren’t any hugely influential points in this model\n\n\n\nGirth-only model\n\nRPython\n\n\n\n# define the model\nlm_girth &lt;- lm(volume ~ girth,\n               data = trees)\n\n# view the model\nlm_girth\n\n\nCall:\nlm(formula = volume ~ girth, data = trees)\n\nCoefficients:\n(Intercept)        girth  \n    -36.943        5.066  \n\n\n\n\n\n# create a linear model\nmodel = smf.ols(formula = \"volume ~ girth\",\n                data = trees_py)\n# and get the fitted parameters of the model\nlm_girth_py = model.fit()\n\nExtract the parameters:\n\nlm_girth_py.params\n\nIntercept   -36.943459\ngirth         5.065856\ndtype: float64\n\n\n\n\n\nWe can use this output to get the following equation:\nvolume = -36.94 + 5.07 \\(\\times\\) girth\nIf we stick the numbers in (girth = 20 and height = 67) we get the following equation:\nvolume = -36.94 + 5.07 \\(\\times\\) 20\nvolume = 64.37\nIf we look at the diagnostic plots for the model using the following commands we get the following:\n\nRPython\n\n\n\nresid_panel(lm_girth,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\n\n\n\n\ndgplots(lm_girth_py)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe diagnostic plots here look rather similar to the ones we generated for the additive model and we have the same issue with a lack of linearity, heterogeneity of variance and one of the data points being influential.",
    "crumbs": [
      "Multiple predictors",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Multiple linear regression</span>"
    ]
  },
  {
    "objectID": "materials/cs5_practical_multiple-linear-regression.html#summary",
    "href": "materials/cs5_practical_multiple-linear-regression.html#summary",
    "title": "16  Multiple linear regression",
    "section": "16.8 Summary",
    "text": "16.8 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nWe can define a linear model with any combination of categorical and continuous predictor variables\nUsing the coefficients of the model we can construct the linear model equation\nThe underlying assumptions of a linear model with three (or more) predictor variables are the same as those of a two-way ANOVA",
    "crumbs": [
      "Multiple predictors",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Multiple linear regression</span>"
    ]
  },
  {
    "objectID": "materials/cs5_practical_model-comparisons.html",
    "href": "materials/cs5_practical_model-comparisons.html",
    "title": "17  Model comparisons",
    "section": "",
    "text": "17.1 Libraries and functions",
    "crumbs": [
      "Multiple predictors",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Model comparisons</span>"
    ]
  },
  {
    "objectID": "materials/cs5_practical_model-comparisons.html#libraries-and-functions",
    "href": "materials/cs5_practical_model-comparisons.html#libraries-and-functions",
    "title": "17  Model comparisons",
    "section": "",
    "text": "Click to expand\n\n\n\n\n\n\nRPython\n\n\n\n17.1.1 Libraries\n\n\n17.1.2 Functions\n\n\n\n\n17.1.3 Libraries\n\n\n17.1.4 Functions",
    "crumbs": [
      "Multiple predictors",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Model comparisons</span>"
    ]
  },
  {
    "objectID": "materials/cs5_practical_model-comparisons.html#purpose-and-aim",
    "href": "materials/cs5_practical_model-comparisons.html#purpose-and-aim",
    "title": "17  Model comparisons",
    "section": "17.2 Purpose and aim",
    "text": "17.2 Purpose and aim\nIn the previous example we used a single data set and fitted five linear models to it depending on which predictor variables we used. Whilst this was fun (seriously, what else would you be doing right now?) it seems that there should be a “better way”. Well, thankfully there is! In fact there a several methods that can be used to compare different models in order to help identify “the best” model. More specifically, we can determine if a full model (which uses all available predictor variables and interactions) is necessary to appropriately describe the dependent variable, or whether we can throw away some of the terms (e.g. an interaction term) because they don’t offer any useful predictive power.\nHere we will use the Akaike Information Criterion in order to compare different models.",
    "crumbs": [
      "Multiple predictors",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Model comparisons</span>"
    ]
  },
  {
    "objectID": "materials/cs5_practical_model-comparisons.html#data-and-hypotheses",
    "href": "materials/cs5_practical_model-comparisons.html#data-and-hypotheses",
    "title": "17  Model comparisons",
    "section": "17.3 Data and hypotheses",
    "text": "17.3 Data and hypotheses\nThis section uses the data/CS5-ladybird.csv data set. This data set comprises of 20 observations of three variables (one dependent and two predictor). This records the clutch size (eggs) in a species of ladybird, alongside two potential predictor variables; the mass of the female (weight), and the colour of the male (male) which is a categorical variable.",
    "crumbs": [
      "Multiple predictors",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Model comparisons</span>"
    ]
  },
  {
    "objectID": "materials/cs5_practical_model-comparisons.html#backwards-stepwise-elimination",
    "href": "materials/cs5_practical_model-comparisons.html#backwards-stepwise-elimination",
    "title": "17  Model comparisons",
    "section": "17.4 Backwards Stepwise Elimination",
    "text": "17.4 Backwards Stepwise Elimination\nFirst, we load the data and visualise it:\n\nRPython\n\n\n\nladybird &lt;- read_csv(\"data/CS5-ladybird.csv\")\n\nhead(ladybird)\n\n# A tibble: 6 × 4\n     id male  weight  eggs\n  &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1     1 Wild    10.6    27\n2     2 Wild    10.6    26\n3     3 Wild    12.5    28\n4     4 Wild    11.4    24\n5     5 Wild    14.9    30\n6     6 Wild    10.4    21\n\n\nWe can visualise the data by male, so we can see if the eggs clutch size differs a lot between the two groups:\n\nggplot(ladybird,\n       aes(x = male, y = eggs)) +\n    geom_boxplot() +\n    geom_jitter(width = 0.05)\n\n\n\n\n\n\n\n\nWe can also plot the egg clutch size against the weight, again for each male colour group:\n\n# visualise the data\nggplot(ladybird,\n       aes(x = weight, y = eggs,\n           colour = male)) +\n    geom_point() +\n    scale_color_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n\n\n\n\nladybird_py = pd.read_csv(\"data/CS5-ladybird.csv\")\n\nWe can visualise the data by male, so we can see if the eggs clutch size differs a lot between the two groups:\n\n(ggplot(ladybird_py,\n        aes(x = \"male\", y = \"eggs\")) +\n        geom_boxplot() +\n        geom_jitter(width = 0.05))\n\n\n\n\n\n\n\n\nWe can also plot the egg clutch size against the weight, again for each male colour group:\n\n(ggplot(ladybird_py,\n        aes(x = \"weight\", y = \"eggs\",\n            colour = \"male\")) +\n        geom_point() +\n        scale_color_brewer(type = \"qual\",\n                           palette = \"Dark2\"))\n\n\n\n\n\n\n\n\n\n\n\nWe can see a few things already:\n\nThere aren’t a huge number of data points in each group, so we need to be a bit cautious with drawing any firm conclusions.\nThere is quite some spread in the egg clutch sizes, with two observations in the Melanic group being rather low.\nFrom the box plot, there does not seem to be much difference in the egg clutch size between the two male colour groups.\nThe scatter plot suggests that egg clutch size seems to increase somewhat linearly as the weight of the female goes up. There does not seem to be much difference between the two male colour groups in this respect.\n\n\n17.4.1 Comparing models with AIC (step 1)\nWe start with the complete or full model, that takes into account any possible interaction between weight and male.\nNext, we define the reduced model. This is the next, most simple model. In this case we’re removing the interaction and constructing an additive model.\n\nRPython\n\n\n\n# define the full model\nlm_full &lt;- lm(eggs ~ weight * male,\n              data = ladybird)\n\n\n# define the additive model\nlm_add &lt;- lm(eggs ~ weight + male,\n             data = ladybird)\n\nWe then extract the AIC values for each of the models:\n\nAIC(lm_full)\n\n[1] 100.0421\n\nAIC(lm_add)\n\n[1] 99.19573\n\n\n\n\n\n# create the model\nmodel = smf.ols(formula= \"eggs ~ weight * C(male)\", data = ladybird_py)\n# and get the fitted parameters of the model\nlm_full_py = model.fit()\n\n\n# create the additive linear model\nmodel = smf.ols(formula= \"eggs ~ weight + C(male)\", data = ladybird_py)\n# and get the fitted parameters of the model\nlm_add_py = model.fit()\n\nWe then extract the AIC values for each of the models:\n\nlm_full_py.aic\n\n98.04206256815984\n\nlm_add_py.aic\n\n97.19572958073036\n\n\n\n\n\nEach line tells you the AIC score for that model. The full model has 4 parameters (the intercept, the coefficient for the continuous variable weight, the coefficient for the categorical variable male and a coefficient for the interaction term weight:male). The additive model has a lower AIC score with only 3 parameters (since we’ve dropped the interaction term). There are different ways of interpreting AIC scores but the most widely used interpretation says that:\n\nif the difference between two AIC scores is greater than 2, then the model with the smallest AIC score is more supported than the model with the higher AIC score\nif the difference between the two models’ AIC scores is less than 2 then both models are equally well supported\n\nThis choice of language (supported vs significant) is deliberate and there are areas of statistics where AIC scores are used differently from the way we are going to use them here (ask if you want a bit of philosophical ramble from me). However, in this situation we will use the AIC scores to decide whether our reduced model is at least as good as the full model. Here since the difference in AIC scores is less than 2, we can say that dropping the interaction term has left us with a model that is both simpler (fewer terms) and as least as good (AIC score) as the full model. As such our additive model eggs ~ weight + male is designated our current working minimal model.\n\n\n17.4.2 Comparing models with AIC (step 2)\nNext, we see which of the remaining terms can be dropped. We will look at the models where we have dropped both male and weight (i.e. eggs ~ weight and eggs ~ male) and compare their AIC values with the AIC of our current minimal model (eggs ~ weight + male). If the AIC values of at least one of our new reduced models is lower (or at least no more than 2 greater) than the AIC of our current minimal model, then we can drop the relevant term and get ourselves a new minimal model. If we find ourselves in a situation where we can drop more than one term we will drop the term that gives us the model with the lowest AIC.\n\nRPython\n\n\nDrop the variable weight and examine the AIC:\n\n# define the model\nlm_male &lt;- lm(eggs ~ male,\n              data = ladybird)\n\n# extract the AIC\nAIC(lm_male)\n\n[1] 118.7093\n\n\nDrop the variable male and examine the AIC:\n\n# define the model\nlm_weight &lt;- lm(eggs ~ weight,\n                data = ladybird)\n\n# extract the AIC\nAIC(lm_weight)\n\n[1] 97.52601\n\n\n\n\nDrop the variable weight and examine the AIC:\n\n# create the model\nmodel = smf.ols(formula= \"eggs ~ C(male)\", data = ladybird_py)\n# and get the fitted parameters of the model\nlm_male_py = model.fit()\n\n# extract the AIC\nlm_male_py.aic\n\n116.7092646564482\n\n\nDrop the variable male and examine the AIC:\n\n# create the model\nmodel = smf.ols(formula= \"eggs ~ weight\", data = ladybird_py)\n# and get the fitted parameters of the model\nlm_weight_py = model.fit()\n\n# extract the AIC\nlm_weight_py.aic\n\n95.52601286248304\n\n\n\n\n\nConsidering both outputs together and comparing with the AIC of our current minimal model, we can see that dropping male has decreased the AIC further, whereas dropping weight has actually increased the AIC and thus worsened the model quality.\nHence we can drop male and our new minimal model is eggs ~ weight.\n\n\n17.4.3 Comparing models with AIC (step 3)\nOur final comparison is to drop the variable weight and compare this simple model with a null model (eggs ~ 1), which assumes that the clutch size is constant across all parameters.\nDrop the variable weight and see if that has an effect:\n\nRPython\n\n\n\n# define the model\nlm_null &lt;- lm(eggs ~ 1,\n              data = ladybird)\n\n# extract the AIC\nAIC(lm_null)\n\n[1] 117.2178\n\n\n\n\n\n# create the model\nmodel = smf.ols(formula= \"eggs ~ 1\", data = ladybird_py)\n# and get the fitted parameters of the model\nlm_null_py = model.fit()\n\n# extract the AIC\nlm_null_py.aic\n\n115.21783038624153\n\n\n\n\n\nThe AIC of our null model is quite a bit larger than that of our current minimal model eggs ~ weight and so we conclude that weight is important. As such our minimal model is eggs ~ weight.\nSo, in summary, we could conclude that:\n\nFemale size is a useful predictor of clutch size, but male type is not so important.\n\nAt this point we can then continue analysing this minimal model, by checking the diagnostic plots and checking the assumptions. If they all pan out, then we can continue with an ANOVA.",
    "crumbs": [
      "Multiple predictors",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Model comparisons</span>"
    ]
  },
  {
    "objectID": "materials/cs5_practical_model-comparisons.html#notes-on-backwards-stepwise-elimination",
    "href": "materials/cs5_practical_model-comparisons.html#notes-on-backwards-stepwise-elimination",
    "title": "17  Model comparisons",
    "section": "17.5 Notes on Backwards Stepwise Elimination",
    "text": "17.5 Notes on Backwards Stepwise Elimination\nThis method of finding a minimal model by starting with a full model and removing variables is called backward stepwise elimination. Although regularly practised in data analysis, there is increasing criticism of this approach, with calls for it to be avoided entirely.\nWhy have we made you work through this procedure then? Given their prevalence in academic papers, it is very useful to be aware of these procedures and to know that there are issues with them. In other situations, using AIC for model comparisons are justified and you will come across them regularly. Additionally, there may be situations where you feel there are good reasons to drop a parameter from your model – using this technique you can justify that this doesn’t affect the model fit. Taken together, using backwards stepwise elimination for model comparison is still a useful technique.\n\n\n\n\n\n\nAutomatic BSE in R (but not Python)\n\n\n\nPerforming backwards stepwise elimination manually can be quite tedious. Thankfully R acknowledges this and there is a single inbuilt function called step() that can perform all of the necessary steps for you using AIC.\n\n# define the full model\nlm_full &lt;- lm(eggs ~ weight * male,\n              data = ladybird)\n\n# perform backwards stepwise elimination\nstep(lm_full)\n\nThis will perform a full backwards stepwise elimination process and will find the minimal model for you.\nYes, I could have told you this earlier, but where’s the fun in that? (it is also useful for you to understand the steps behind the technique I suppose…)\nWhen doing this in Python, you are a bit stuck. There does not seem to be an equivalent function. If you want to cobble something together yourself, then use this link as a starting point.",
    "crumbs": [
      "Multiple predictors",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Model comparisons</span>"
    ]
  },
  {
    "objectID": "materials/cs5_practical_model-comparisons.html#exercises",
    "href": "materials/cs5_practical_model-comparisons.html#exercises",
    "title": "17  Model comparisons",
    "section": "17.6 Exercises",
    "text": "17.6 Exercises\nWe are going to practice the backwards stepwise elimination technique on some of the data sets we analysed previously.\nFor each of the following data sets I would like you to:\n\nDefine the response variable\nDefine the relevant predictor variables\nDefine relevant interactions\nPerform a backwards stepwise elimination and discover the minimal model using AIC\n\nNB: if an interaction term is significant then any main factor that is part of the interaction term cannot be dropped from the model.\nPerform a BSE on the following data sets:\n\ndata/CS5-trees.csv\ndata/CS5-pm2_5.csv\n\n\n17.6.1 BSE: Trees\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLevel: \nBSE on trees:\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n17.7 Answer\nLet’s start by reading in the data and checking which variables are in the data set.\n\nRPython\n\n\n\ntrees &lt;- read_csv(\"data/CS5-trees.csv\")\n\nhead(trees)\n\n# A tibble: 6 × 3\n  girth height volume\n  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1   8.3     70   10.3\n2   8.6     65   10.3\n3   8.8     63   10.2\n4  10.5     72   16.4\n5  10.7     81   18.8\n6  10.8     83   19.7\n\n\n\n\nFirst, we read in the data (if needed) and have a look at the variables:\n\ntrees_py = pd.read_csv(\"data/CS5-trees.csv\")\n\ntrees_py.head()\n\n   girth  height  volume\n0    8.3      70    10.3\n1    8.6      65    10.3\n2    8.8      63    10.2\n3   10.5      72    16.4\n4   10.7      81    18.8\n\n\n\n\n\n\nThe response variable is volume\nThe predictor variables are girth and height\nThe only possible interaction term is girth:height\nWe perform a BSE on the model using the step() function\n\nThe full model is volume ~ girth * height.\nWe perform the BSE as follows:\n\nRPython\n\n\nDefine the model and use the step() function:\n\n# define the full model\nlm_trees &lt;- lm(volume ~ girth * height,\n               data = trees)\n\n# perform BSE\nstep(lm_trees)\n\nStart:  AIC=65.49\nvolume ~ girth * height\n\n               Df Sum of Sq    RSS    AIC\n&lt;none&gt;                      198.08 65.495\n- girth:height  1    223.84 421.92 86.936\n\n\n\nCall:\nlm(formula = volume ~ girth * height, data = trees)\n\nCoefficients:\n (Intercept)         girth        height  girth:height  \n     69.3963       -5.8558       -1.2971        0.1347  \n\n\n\n\nWe first define the full model, then the model without the interaction (model_1).\nWe extract the AICs for both models. Because we do not have an automated way of performing the BSE, we’re stringing together a few operations to make the code a bit more concise (getting the .fit() of the model and immediately extracting the aic value):\n\n# define the models\nmodel_full = smf.ols(formula= \"volume ~ girth * height\",\n                     data = trees_py)\n\nmodel_1 = smf.ols(formula= \"volume ~ girth + height\",\n                  data = trees_py)\n\n# get the AIC of the model\nmodel_full.fit().aic\n\n153.46916240903528\n\nmodel_1.fit().aic\n\n174.9099729872703\n\n\n\n\n\nThis BSE approach only gets as far as the first step (trying to drop the interaction term). We see immediately that dropping the interaction term makes the model worse. This means that the best model is still the full model.\n\n\n\n\n\n\n\n\n\n\n\n\n17.7.1 BSE: Air pollution\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLevel: \nPerform a BSE on pm2_5. Let’s start by reading in the data and checking which variables are in the data set.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n17.8 Answer\n\nRPython\n\n\n\npm2_5 &lt;- read_csv(\"data/CS5-pm2_5.csv\")\n\nhead(pm2_5)\n\n# A tibble: 6 × 6\n  avg_temp date       location pm2_5 rain_mm wind_m_s\n     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1      4.5 01/01/2019 inner     17.1     2.3     3.87\n2      4.9 01/01/2019 outer     10.8     2.3     5.84\n3      4.3 02/01/2019 inner     14.9     2.3     3.76\n4      4.8 02/01/2019 outer     11.4     2.3     6   \n5      4   03/01/2019 inner     18.5     1.4     2.13\n6      4.5 03/01/2019 outer     15.0     1.4     2.57\n\n\n\n\n\npm2_5_py = pd.read_csv(\"data/CS5-pm2_5.csv\")\n\npm2_5_py.head()\n\n   avg_temp        date location   pm2_5  rain_mm  wind_m_s\n0       4.5  01/01/2019    inner  17.126      2.3      3.87\n1       4.9  01/01/2019    outer  10.821      2.3      5.84\n2       4.3  02/01/2019    inner  14.884      2.3      3.76\n3       4.8  02/01/2019    outer  11.416      2.3      6.00\n4       4.0  03/01/2019    inner  18.471      1.4      2.13\n\n\n\n\n\n\nThe response variable is pm2_5\nThe predictor variables are all the variables, apart from date and pm2_5. It would be strange to try and create a model that relies on each individual measurement!\nWe can add the wind_m_s:location interaction, since it appeared that there is a difference between inner and outer London pollution levels, in relation to wind speed\nWe can start the backwards stepwise elimination with the following model:\n\npm2_5 ~ avg_temp + rain_mm + wind_m_s * location\n\nRPython\n\n\n\n# define the model\nlm_pm2_5 &lt;- lm(pm2_5 ~ avg_temp + rain_mm + wind_m_s * location,\n               data = pm2_5)\n\n# perform BSE\nstep(lm_pm2_5)\n\nStart:  AIC=41.08\npm2_5 ~ avg_temp + rain_mm + wind_m_s * location\n\n                    Df Sum of Sq    RSS     AIC\n- rain_mm            1     0.351 760.01  39.412\n- avg_temp           1     1.804 761.47  40.806\n&lt;none&gt;                           759.66  41.075\n- wind_m_s:location  1   134.820 894.48 158.336\n\nStep:  AIC=39.41\npm2_5 ~ avg_temp + wind_m_s + location + wind_m_s:location\n\n                    Df Sum of Sq    RSS     AIC\n- avg_temp           1     1.758 761.77  39.098\n&lt;none&gt;                           760.01  39.412\n- wind_m_s:location  1   134.530 894.54 156.385\n\nStep:  AIC=39.1\npm2_5 ~ wind_m_s + location + wind_m_s:location\n\n                    Df Sum of Sq    RSS     AIC\n&lt;none&gt;                           761.77  39.098\n- wind_m_s:location  1    136.95 898.72 157.788\n\n\n\nCall:\nlm(formula = pm2_5 ~ wind_m_s + location + wind_m_s:location, \n    data = pm2_5)\n\nCoefficients:\n           (Intercept)                wind_m_s           locationouter  \n               18.2422                 -0.2851                 -2.0597  \nwind_m_s:locationouter  \n               -0.4318  \n\n\n\n\nWe first define the full model, again stringing together a few operations to be more concise.\n\n# define the model\nmodel_full = smf.ols(formula= \"pm2_5 ~ avg_temp + rain_mm + wind_m_s * C(location)\", data = pm2_5_py)\n\n# get the AIC of the model\nmodel_full.fit().aic\n\n2112.725435984824\n\n\nCan we drop the interaction term or any of the remaining main effects?\n\n# define the model\nmodel_1 = smf.ols(\n    formula= \"pm2_5 ~ avg_temp + rain_mm + wind_m_s + C(location)\",\n    data = pm2_5_py)\n    \nmodel_2 = smf.ols(\n    formula= \"pm2_5 ~ avg_temp + wind_m_s * C(location)\",\n    data = pm2_5_py)\n    \nmodel_3 = smf.ols(\n    formula= \"pm2_5 ~ rain_mm + wind_m_s * C(location)\",\n    data = pm2_5_py)\n\n# get the AIC of the models\nmodel_1.fit().aic\n\n2229.9865962235162\n\nmodel_2.fit().aic\n\n2111.06230638372\n\nmodel_3.fit().aic\n\n2112.456639492829\n\n# compare to the full model\nmodel_full.fit().aic\n\n2112.725435984824\n\n\nThe AIC goes up quite a bit if we drop the interaction term. This means that we cannot drop the interaction term, nor any of the main effects that are included in the interaction. These are wind_m_s and location.\nThe model with the lowest AIC is the one without the rain_mm term, so our working model is:\n\nworking_model = smf.ols(\n    formula= \"pm2_5 ~ avg_temp + wind_m_s * C(location)\",\n    data = pm2_5_py)\n\nNow we can again check if dropping the avg_temp term or the wind_m_s:location interaction has any effect on the model performance:\n\nmodel_1 = smf.ols(\n    formula= \"pm2_5 ~ wind_m_s * C(location)\",\n    data = pm2_5_py)\n    \nmodel_2 = smf.ols(\n    formula= \"pm2_5 ~ avg_temp + wind_m_s + C(location)\",\n    data = pm2_5_py)\n\n# get the AIC of the models\nmodel_1.fit().aic\n\n2110.748543452394\n\nmodel_2.fit().aic\n\n2228.035595215867\n\nworking_model.fit().aic\n\n2111.06230638372\n\n\nThis shows that dropping the avg_temp term lowers the AIC, whereas dropping the interaction term makes the model markedly worse.\nSo, our new working model is:\n\nworking_model = smf.ols(\n    formula= \"pm2_5 ~ wind_m_s * C(location)\",\n    data = pm2_5_py)\n\nLastly, now we’ve dropped the avg_temp term we can do one final check on the interaction term:\n\nmodel_1 = smf.ols(\n    formula= \"pm2_5 ~ wind_m_s + C(location) + wind_m_s + C(location)\",\n    data = pm2_5_py)\n\nmodel_1.fit().aic\n\n2229.438631394105\n\nworking_model.fit().aic\n\n2110.748543452394\n\n\nDropping the interaction still makes the model worse.\n\n\n\nOur minimal model is thus:\npm2_5 ~ wind_m_s + location + wind_m_s:location",
    "crumbs": [
      "Multiple predictors",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Model comparisons</span>"
    ]
  },
  {
    "objectID": "materials/cs5_practical_model-comparisons.html#answer",
    "href": "materials/cs5_practical_model-comparisons.html#answer",
    "title": "17  Model comparisons",
    "section": "17.7 Answer",
    "text": "17.7 Answer\nLet’s start by reading in the data and checking which variables are in the data set.\n\nRPython\n\n\n\ntrees &lt;- read_csv(\"data/CS5-trees.csv\")\n\nhead(trees)\n\n# A tibble: 6 × 3\n  girth height volume\n  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1   8.3     70   10.3\n2   8.6     65   10.3\n3   8.8     63   10.2\n4  10.5     72   16.4\n5  10.7     81   18.8\n6  10.8     83   19.7\n\n\n\n\nFirst, we read in the data (if needed) and have a look at the variables:\n\ntrees_py = pd.read_csv(\"data/CS5-trees.csv\")\n\ntrees_py.head()\n\n   girth  height  volume\n0    8.3      70    10.3\n1    8.6      65    10.3\n2    8.8      63    10.2\n3   10.5      72    16.4\n4   10.7      81    18.8\n\n\n\n\n\n\nThe response variable is volume\nThe predictor variables are girth and height\nThe only possible interaction term is girth:height\nWe perform a BSE on the model using the step() function\n\nThe full model is volume ~ girth * height.\nWe perform the BSE as follows:\n\nRPython\n\n\nDefine the model and use the step() function:\n\n# define the full model\nlm_trees &lt;- lm(volume ~ girth * height,\n               data = trees)\n\n# perform BSE\nstep(lm_trees)\n\nStart:  AIC=65.49\nvolume ~ girth * height\n\n               Df Sum of Sq    RSS    AIC\n&lt;none&gt;                      198.08 65.495\n- girth:height  1    223.84 421.92 86.936\n\n\n\nCall:\nlm(formula = volume ~ girth * height, data = trees)\n\nCoefficients:\n (Intercept)         girth        height  girth:height  \n     69.3963       -5.8558       -1.2971        0.1347  \n\n\n\n\nWe first define the full model, then the model without the interaction (model_1).\nWe extract the AICs for both models. Because we do not have an automated way of performing the BSE, we’re stringing together a few operations to make the code a bit more concise (getting the .fit() of the model and immediately extracting the aic value):\n\n# define the models\nmodel_full = smf.ols(formula= \"volume ~ girth * height\",\n                     data = trees_py)\n\nmodel_1 = smf.ols(formula= \"volume ~ girth + height\",\n                  data = trees_py)\n\n# get the AIC of the model\nmodel_full.fit().aic\n\n153.46916240903528\n\nmodel_1.fit().aic\n\n174.9099729872703\n\n\n\n\n\nThis BSE approach only gets as far as the first step (trying to drop the interaction term). We see immediately that dropping the interaction term makes the model worse. This means that the best model is still the full model.",
    "crumbs": [
      "Multiple predictors",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Model comparisons</span>"
    ]
  },
  {
    "objectID": "materials/cs5_practical_model-comparisons.html#answer-1",
    "href": "materials/cs5_practical_model-comparisons.html#answer-1",
    "title": "17  Model comparisons",
    "section": "17.8 Answer",
    "text": "17.8 Answer\n\nRPython\n\n\n\npm2_5 &lt;- read_csv(\"data/CS5-pm2_5.csv\")\n\nhead(pm2_5)\n\n# A tibble: 6 × 6\n  avg_temp date       location pm2_5 rain_mm wind_m_s\n     &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1      4.5 01/01/2019 inner     17.1     2.3     3.87\n2      4.9 01/01/2019 outer     10.8     2.3     5.84\n3      4.3 02/01/2019 inner     14.9     2.3     3.76\n4      4.8 02/01/2019 outer     11.4     2.3     6   \n5      4   03/01/2019 inner     18.5     1.4     2.13\n6      4.5 03/01/2019 outer     15.0     1.4     2.57\n\n\n\n\n\npm2_5_py = pd.read_csv(\"data/CS5-pm2_5.csv\")\n\npm2_5_py.head()\n\n   avg_temp        date location   pm2_5  rain_mm  wind_m_s\n0       4.5  01/01/2019    inner  17.126      2.3      3.87\n1       4.9  01/01/2019    outer  10.821      2.3      5.84\n2       4.3  02/01/2019    inner  14.884      2.3      3.76\n3       4.8  02/01/2019    outer  11.416      2.3      6.00\n4       4.0  03/01/2019    inner  18.471      1.4      2.13\n\n\n\n\n\n\nThe response variable is pm2_5\nThe predictor variables are all the variables, apart from date and pm2_5. It would be strange to try and create a model that relies on each individual measurement!\nWe can add the wind_m_s:location interaction, since it appeared that there is a difference between inner and outer London pollution levels, in relation to wind speed\nWe can start the backwards stepwise elimination with the following model:\n\npm2_5 ~ avg_temp + rain_mm + wind_m_s * location\n\nRPython\n\n\n\n# define the model\nlm_pm2_5 &lt;- lm(pm2_5 ~ avg_temp + rain_mm + wind_m_s * location,\n               data = pm2_5)\n\n# perform BSE\nstep(lm_pm2_5)\n\nStart:  AIC=41.08\npm2_5 ~ avg_temp + rain_mm + wind_m_s * location\n\n                    Df Sum of Sq    RSS     AIC\n- rain_mm            1     0.351 760.01  39.412\n- avg_temp           1     1.804 761.47  40.806\n&lt;none&gt;                           759.66  41.075\n- wind_m_s:location  1   134.820 894.48 158.336\n\nStep:  AIC=39.41\npm2_5 ~ avg_temp + wind_m_s + location + wind_m_s:location\n\n                    Df Sum of Sq    RSS     AIC\n- avg_temp           1     1.758 761.77  39.098\n&lt;none&gt;                           760.01  39.412\n- wind_m_s:location  1   134.530 894.54 156.385\n\nStep:  AIC=39.1\npm2_5 ~ wind_m_s + location + wind_m_s:location\n\n                    Df Sum of Sq    RSS     AIC\n&lt;none&gt;                           761.77  39.098\n- wind_m_s:location  1    136.95 898.72 157.788\n\n\n\nCall:\nlm(formula = pm2_5 ~ wind_m_s + location + wind_m_s:location, \n    data = pm2_5)\n\nCoefficients:\n           (Intercept)                wind_m_s           locationouter  \n               18.2422                 -0.2851                 -2.0597  \nwind_m_s:locationouter  \n               -0.4318  \n\n\n\n\nWe first define the full model, again stringing together a few operations to be more concise.\n\n# define the model\nmodel_full = smf.ols(formula= \"pm2_5 ~ avg_temp + rain_mm + wind_m_s * C(location)\", data = pm2_5_py)\n\n# get the AIC of the model\nmodel_full.fit().aic\n\n2112.725435984824\n\n\nCan we drop the interaction term or any of the remaining main effects?\n\n# define the model\nmodel_1 = smf.ols(\n    formula= \"pm2_5 ~ avg_temp + rain_mm + wind_m_s + C(location)\",\n    data = pm2_5_py)\n    \nmodel_2 = smf.ols(\n    formula= \"pm2_5 ~ avg_temp + wind_m_s * C(location)\",\n    data = pm2_5_py)\n    \nmodel_3 = smf.ols(\n    formula= \"pm2_5 ~ rain_mm + wind_m_s * C(location)\",\n    data = pm2_5_py)\n\n# get the AIC of the models\nmodel_1.fit().aic\n\n2229.9865962235162\n\nmodel_2.fit().aic\n\n2111.06230638372\n\nmodel_3.fit().aic\n\n2112.456639492829\n\n# compare to the full model\nmodel_full.fit().aic\n\n2112.725435984824\n\n\nThe AIC goes up quite a bit if we drop the interaction term. This means that we cannot drop the interaction term, nor any of the main effects that are included in the interaction. These are wind_m_s and location.\nThe model with the lowest AIC is the one without the rain_mm term, so our working model is:\n\nworking_model = smf.ols(\n    formula= \"pm2_5 ~ avg_temp + wind_m_s * C(location)\",\n    data = pm2_5_py)\n\nNow we can again check if dropping the avg_temp term or the wind_m_s:location interaction has any effect on the model performance:\n\nmodel_1 = smf.ols(\n    formula= \"pm2_5 ~ wind_m_s * C(location)\",\n    data = pm2_5_py)\n    \nmodel_2 = smf.ols(\n    formula= \"pm2_5 ~ avg_temp + wind_m_s + C(location)\",\n    data = pm2_5_py)\n\n# get the AIC of the models\nmodel_1.fit().aic\n\n2110.748543452394\n\nmodel_2.fit().aic\n\n2228.035595215867\n\nworking_model.fit().aic\n\n2111.06230638372\n\n\nThis shows that dropping the avg_temp term lowers the AIC, whereas dropping the interaction term makes the model markedly worse.\nSo, our new working model is:\n\nworking_model = smf.ols(\n    formula= \"pm2_5 ~ wind_m_s * C(location)\",\n    data = pm2_5_py)\n\nLastly, now we’ve dropped the avg_temp term we can do one final check on the interaction term:\n\nmodel_1 = smf.ols(\n    formula= \"pm2_5 ~ wind_m_s + C(location) + wind_m_s + C(location)\",\n    data = pm2_5_py)\n\nmodel_1.fit().aic\n\n2229.438631394105\n\nworking_model.fit().aic\n\n2110.748543452394\n\n\nDropping the interaction still makes the model worse.\n\n\n\nOur minimal model is thus:\npm2_5 ~ wind_m_s + location + wind_m_s:location",
    "crumbs": [
      "Multiple predictors",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Model comparisons</span>"
    ]
  },
  {
    "objectID": "materials/cs5_practical_model-comparisons.html#summary",
    "href": "materials/cs5_practical_model-comparisons.html#summary",
    "title": "17  Model comparisons",
    "section": "17.9 Summary",
    "text": "17.9 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nWe can use Backwards Stepwise Elimination (BSE) on a full model to see if certain terms add to the predictive power of the model or not\nThe AIC allows us to compare different models - if there is a difference in AIC of more than 2 between two models, then the smallest AIC score is more supported",
    "crumbs": [
      "Multiple predictors",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Model comparisons</span>"
    ]
  },
  {
    "objectID": "materials/operationalisation.html",
    "href": "materials/operationalisation.html",
    "title": "18  Operationalising variables",
    "section": "",
    "text": "18.1 Libraries and functions",
    "crumbs": [
      "Reconsidering variables",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Operationalising variables</span>"
    ]
  },
  {
    "objectID": "materials/operationalisation.html#libraries-and-functions",
    "href": "materials/operationalisation.html#libraries-and-functions",
    "title": "18  Operationalising variables",
    "section": "",
    "text": "Click to expand\n\n\n\n\n\n\nRPython\n\n\n\n18.1.1 Libraries\n\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\n\n\n\n\n18.1.2 Libraries\n\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Simple yet exhaustive stats functions.\nimport pingouin as pg\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# Statistical models, conducting tests and statistical data exploration\nimport statsmodels.api as sm\n\n# Convenience interface for specifying models using formula strings and DataFrames\nimport statsmodels.formula.api as smf",
    "crumbs": [
      "Reconsidering variables",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Operationalising variables</span>"
    ]
  },
  {
    "objectID": "materials/operationalisation.html#exercise-1---cycling-to-work",
    "href": "materials/operationalisation.html#exercise-1---cycling-to-work",
    "title": "18  Operationalising variables",
    "section": "18.2 Exercise 1 - Cycling to work",
    "text": "18.2 Exercise 1 - Cycling to work\nFor this example, we’re interested in finding out whether cycling to work increases staff members’ productivity.\nDownload the productivity.csv file.\nThis file contains a fictional dataset that explores the relationship between cycling to work and productivity at work. Each row corresponds to a different staff member at a small Cambridge-based company. There are four variables: cycle is a categorical variable denoting whether the individual cycles to work; distance is the distance in kilometres between the individual’s house and the office; projects is the number of projects successfully completed by the individual within the last 6 months; and mean_hrs is the average number of hours worked per week in the last 6 months.\nAs you may have noticed, we have two variables here that could serve as measures of productivity, and two ways of looking at cycling - whether someone cycles, versus how far they cycle.\nFirst, let’s start by reading in the data, and visualising it.\n\nRPython\n\n\n\n# load the data\nproductivity &lt;- read_csv(\"data/productivity.csv\")\n\n# and have a look\nhead(productivity)\n\n\n\n\n# load the data\nproductivity_py = pd.read_csv(\"data/productivity.csv\")\n\n# and have a look\nproductivity_py.head()\n\n  cycle  distance  projects  mean_hrs\n0    no      6.68         2      55.0\n1    no      4.32         3      48.0\n2    no      5.81         2      42.0\n3    no      8.49         2      37.5\n4    no      6.47         4      32.0\n\n\n\n\n\nNow it’s time to explore this data in a bit more detail. We can gain some insight by examining our two measures of “cycling” (our yes/no categorical variable, and the distance between home and office) and our two measures of “productivity” (mean hours worked per week, and projects completed in the last 6 months).\n\nRPython\n\n\n\n# visualise using a boxplot\n\nproductivity %&gt;%\n  ggplot(aes(x = cycle, y = distance)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nNow, we’ll use a t-test to compare distance between those who cycle, and those who don’t.\n\nt.test(distance ~ cycle, data = productivity)\n\n\n    Welch Two Sample t-test\n\ndata:  distance by cycle\nt = 3.4417, df = 10.517, p-value = 0.005871\nalternative hypothesis: true difference in means between group no and group yes is not equal to 0\n95 percent confidence interval:\n 1.801439 8.293561\nsample estimates:\n mean in group no mean in group yes \n           7.3700            2.3225 \n\n\n\n\n\n# visualise using a boxplot\n(ggplot(productivity_py,\n        aes(x = \"cycle\",\n            y = \"distance\")) +\n     geom_boxplot())\n\n\n\n\n\n\n\n\nNext, we compare the distance between those who cycle and those who do not. We use a t-test, since there are only two groups.\nHere we use the ttest() function from the pingouin library. This needs two vectors as input, so we split the data as follows and then run the test:\n\ndist_no_cycle = productivity_py.query('cycle == \"no\"')[\"distance\"]\ndist_yes_cycle = productivity_py.query('cycle == \"yes\"')[\"distance\"]\n\npg.ttest(dist_no_cycle, dist_yes_cycle).transpose()\n\n                  T-test\nT               3.441734\ndof            10.516733\nalternative    two-sided\np-val           0.005871\nCI95%        [1.8, 8.29]\ncohen-d         1.683946\nBF10              15.278\npower           0.960302\n\n\n\n\n\nLet’s look at the second set of variables: the mean hours of worked per week and the number of projects completed in the past 6 months. When visualising this, we need to consider the projects as a categorical variable.\n\nRPython\n\n\n\n# visualise the data\nproductivity %&gt;%\n  ggplot(aes(x = as.factor(projects), y = mean_hrs)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n# construct a one-way ANOVA, treating projects as a categorical variable\n\nlm_prod &lt;- lm(mean_hrs ~ as.factor(projects), data = productivity)\nanova(lm_prod)\n\nAnalysis of Variance Table\n\nResponse: mean_hrs\n                    Df  Sum Sq Mean Sq F value Pr(&gt;F)\nas.factor(projects)  7  936.02  133.72   1.598 0.2066\nResiduals           16 1338.88   83.68               \n\n\n\n\n\n# visualise using a boxplot\n(ggplot(productivity_py,\n        aes(x = productivity_py['projects'].astype('category'),\n            y = \"mean_hrs\")) +\n     geom_boxplot())\n\n\n\n\n\n\n\n\n\n# construct a one-way ANOVA, treating projects as a categorical variable\npg.anova(dv = \"mean_hrs\",\n         between = \"projects\",\n         data = productivity_py,\n         detailed = True).round(3)\n\n     Source        SS  DF       MS      F  p-unc    np2\n0  projects   936.023   7  133.718  1.598  0.207  0.411\n1    Within  1338.883  16   83.680    NaN    NaN    NaN\n\n\n\n\n\nWhat does this tell you about how these two sets of variables, which (in theory at least!) tap into the same underlying construct, relate to one another? Can you spot any problems, or have you got any concerns at this stage?\nIf so, hold that thought.\n\nAssessing the effect of cycling on productivity\nThe next step is to run some exploratory analyses. Since we’re not going to reporting these data in any kind of paper or article, and the whole point is to look at different versions of the same analysis with different variables, we won’t worry about multiple comparison correction here.\nWhen treating mean_hrs as our response variable, we can use standard linear models approach, since this variable is continuous.\n\nRPython\n\n\n\n# visualise using ggplot\n\nproductivity %&gt;%\n  ggplot(aes(x = cycle, y = mean_hrs)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n# run a t-test to compare mean_hrs for those who cycle vs those who don't\n\nt.test(mean_hrs ~ cycle, data = productivity)\n\n\n    Welch Two Sample t-test\n\ndata:  mean_hrs by cycle\nt = -0.51454, df = 11.553, p-value = 0.6166\nalternative hypothesis: true difference in means between group no and group yes is not equal to 0\n95 percent confidence interval:\n -12.803463   7.928463\nsample estimates:\n mean in group no mean in group yes \n          36.6875           39.1250 \n\n\n\n\n\n# visualise using a boxplot\n(ggplot(productivity_py,\n        aes(x = \"cycle\",\n            y = \"mean_hrs\")) +\n     geom_boxplot())\n\n\n\n\n\n\n\n\n\n# run a t-test to compare mean_hrs for those who cycle vs those who don't\nhrs_no_cycle = productivity_py.query('cycle == \"no\"')[\"mean_hrs\"]\nhrs_yes_cycle = productivity_py.query('cycle == \"yes\"')[\"mean_hrs\"]\n\npg.ttest(hrs_no_cycle, hrs_yes_cycle).transpose()\n\n                    T-test\nT                -0.514542\ndof              11.553188\nalternative      two-sided\np-val             0.616573\nCI95%        [-12.8, 7.93]\ncohen-d            0.24139\nBF10                 0.427\npower             0.083194\n\n\n\n\n\nLet’s also look at mean_hrs vs distance:\n\nRPython\n\n\n\nproductivity %&gt;%\n  ggplot(aes(x = distance, y = mean_hrs)) +\n  geom_point()\n\n\n\n\n\n\n\n# run a simple linear regression analysis\n\nlm_hrs1 &lt;- lm(mean_hrs ~ distance, data = productivity)\nanova(lm_hrs1)\n\nAnalysis of Variance Table\n\nResponse: mean_hrs\n          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \ndistance   1  473.28  473.28  5.7793 0.02508 *\nResiduals 22 1801.63   81.89                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n# visualise using a scatterplot\n(ggplot(productivity_py,\n        aes(x = \"distance\",\n            y = \"mean_hrs\")) +\n     geom_point())\n\n\n\n\n\n\n\n\nWe can perform a linear regression on these data:\n\n# create a linear model\nmodel = smf.ols(formula = \"mean_hrs ~ distance\",\n                data = productivity_py)\n# and get the fitted parameters of the model\nlm_hrs1_py = model.fit()\n\n# look at the model output\nprint(lm_hrs1_py.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               mean_hrs   R-squared:                       0.208\nModel:                            OLS   Adj. R-squared:                  0.172\nMethod:                 Least Squares   F-statistic:                     5.779\nDate:                Tue, 16 Apr 2024   Prob (F-statistic):             0.0251\nTime:                        07:45:01   Log-Likelihood:                -85.875\nNo. Observations:                  24   AIC:                             175.8\nDf Residuals:                      22   BIC:                             178.1\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     43.0833      2.711     15.891      0.000      37.461      48.706\ndistance      -1.1912      0.496     -2.404      0.025      -2.219      -0.164\n==============================================================================\nOmnibus:                        1.267   Durbin-Watson:                   1.984\nProb(Omnibus):                  0.531   Jarque-Bera (JB):                0.300\nSkew:                          -0.163   Prob(JB):                        0.861\nKurtosis:                       3.441   Cond. No.                         8.18\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nThis shows us that while cycle does not significantly predict mean_hrs, distance does. (If you had some concerns about the distance variable earlier, continue to hold that thought.)\nSo, that’s the picture for mean_hrs, the first of our two possible outcome variables. What about the predictive relationship(s) of cycling on our other candidate outcome variable, projects?\nLet’s try fitting some models with projects as the outcome. We’ll continue to use linear models for this for now, although technically, as projects is what we would refer to as a count variable, we should technically be fitting a different type of model called a generalised linear model. This topic will come up later in the week, and for this dataset the two types of model lead to similar outcomes, so we won’t worry about the distinction for now.\nFirst, we look at distance vs projects.\n\nRPython\n\n\n\nproductivity %&gt;%\n  ggplot(aes(x = distance, y = projects)) +\n  geom_point()\n\n\n\n\n\n\n\nlm_proj1 &lt;- lm(projects ~ distance, data = productivity)\nanova(lm_proj1)\n\nAnalysis of Variance Table\n\nResponse: projects\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)  \ndistance   1 12.644  12.644   3.667 0.06859 .\nResiduals 22 75.856   3.448                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n# visualise using a scatterplot\n(ggplot(productivity_py,\n        aes(x = \"distance\",\n            y = \"projects\")) +\n     geom_point())\n\n\n\n\n\n\n\n\n\n# create a linear model\nmodel = smf.ols(formula = \"projects ~ distance\",\n                    data = productivity_py)\n# and get the fitted parameters of the model\nlm_proj1_py = model.fit()\n\n# look at the model output\nprint(lm_proj1_py.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               projects   R-squared:                       0.143\nModel:                            OLS   Adj. R-squared:                  0.104\nMethod:                 Least Squares   F-statistic:                     3.667\nDate:                Tue, 16 Apr 2024   Prob (F-statistic):             0.0686\nTime:                        07:45:01   Log-Likelihood:                -47.864\nNo. Observations:                  24   AIC:                             99.73\nDf Residuals:                      22   BIC:                             102.1\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      4.5298      0.556      8.143      0.000       3.376       5.683\ndistance      -0.1947      0.102     -1.915      0.069      -0.406       0.016\n==============================================================================\nOmnibus:                        1.216   Durbin-Watson:                   1.491\nProb(Omnibus):                  0.544   Jarque-Bera (JB):                1.048\nSkew:                           0.317   Prob(JB):                        0.592\nKurtosis:                       2.196   Cond. No.                         8.18\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nNext, we look at cycle vs projects.\n\nRPython\n\n\n\nproductivity %&gt;%\n  ggplot(aes(x = cycle, y = projects)) +\n  geom_boxplot()\n\n\n\n\n\n\n\nlm_proj2 &lt;- lm(projects ~ cycle, data = productivity)\nanova(lm_proj2)\n\nAnalysis of Variance Table\n\nResponse: projects\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)  \ncycle      1  18.75 18.7500   5.914 0.02362 *\nResiduals 22  69.75  3.1705                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n# visualise using a scatterplot\n(ggplot(productivity_py,\n        aes(x = \"cycle\",\n            y = \"projects\")) +\n     geom_boxplot())\n\n\n\n\n\n\n\n\n\n# create a generalised linear model\nmodel = smf.ols(formula = \"projects ~ cycle\",\n                    data = productivity_py)\n# and get the fitted parameters of the model\nlm_proj2_py = model.fit()\n\n# look at the model output\nprint(lm_proj2_py.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               projects   R-squared:                       0.212\nModel:                            OLS   Adj. R-squared:                  0.176\nMethod:                 Least Squares   F-statistic:                     5.914\nDate:                Tue, 16 Apr 2024   Prob (F-statistic):             0.0236\nTime:                        07:45:02   Log-Likelihood:                -46.857\nNo. Observations:                  24   AIC:                             97.71\nDf Residuals:                      22   BIC:                             100.1\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n================================================================================\n                   coef    std err          t      P&gt;|t|      [0.025      0.975]\n--------------------------------------------------------------------------------\nIntercept        2.5000      0.630      3.971      0.001       1.194       3.806\ncycle[T.yes]     1.8750      0.771      2.432      0.024       0.276       3.474\n==============================================================================\nOmnibus:                        0.057   Durbin-Watson:                   1.674\nProb(Omnibus):                  0.972   Jarque-Bera (JB):                0.221\nSkew:                           0.096   Prob(JB):                        0.895\nKurtosis:                       2.571   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nThis shows us that cycle significantly predicts projects, meaning the number of projects that get completed is not completely random, but some of the variance in that can be explained by whether a person cycles to work, or not. In contrast, distance does not appear to be a significant predictor of projects (although it’s only marginally non-significant). This is the opposite pattern, more or less, to the one we had for mean_hrs.\n\n\nThat thought you were holding…\nThose of you who are discerning may have noticed that the distance variable is problematic as a measure of “cycling to work” in this particular dataset - this is because the dataset includes all the distances to work for the staff members who don’t cycle, as well as those who do.\nWhat happens if we remove those values, and look at the relationship between distance and our response variables again?\n\nRPython\n\n\n\n# use the filter function to retain only the rows where the staff member cycles\n\nproductivity_cycle &lt;- productivity %&gt;%\n  filter(cycle == \"yes\")\n\n\n\n\nproductivity_cycle_py = productivity_py[productivity_py[\"cycle\"] == \"yes\"]\n\n\n\n\nWe’ll repeat earlier visualisations and analyses, this time with the colour aesthetic helping us to visualise how the cycle variable affects the relationships between distance, mean_hrs and projects.\n\nRPython\n\n\n\nproductivity %&gt;%\n  ggplot(aes(x = distance, y = mean_hrs, colour = cycle)) +\n  geom_point()\n\n\n\n\n\n\n\nlm_hrs2 &lt;- lm(mean_hrs ~ distance, data = productivity_cycle)\nanova(lm_hrs2)\n\nAnalysis of Variance Table\n\nResponse: mean_hrs\n          Df  Sum Sq Mean Sq F value Pr(&gt;F)\ndistance   1  202.77 202.766  2.6188 0.1279\nResiduals 14 1083.98  77.427               \n\nproductivity %&gt;%\n  ggplot(aes(x = distance, y = projects, colour = cycle)) +\n  geom_point()\n\n\n\n\n\n\n\nlm_proj3 &lt;- lm(projects ~ distance, data = productivity_cycle)\nsummary(lm_proj3)\n\n\nCall:\nlm(formula = projects ~ distance, data = productivity_cycle)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.2086 -1.0679 -0.1724  1.6874  3.4674 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   4.6899     0.7163   6.547  1.3e-05 ***\ndistance     -0.1356     0.2095  -0.647    0.528    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.103 on 14 degrees of freedom\nMultiple R-squared:  0.02904,   Adjusted R-squared:  -0.04031 \nF-statistic: 0.4187 on 1 and 14 DF,  p-value: 0.528\n\n\n\n\n\n# visualise using a scatterplot\n(ggplot(productivity_py,\n        aes(x = \"distance\",\n            y = \"mean_hrs\",\n            colour = \"cycle\")) +\n     geom_point())\n\n\n\n\n\n\n\n\n\n# create a linear model\nmodel = smf.ols(formula = \"mean_hrs ~ distance\",\n                data = productivity_cycle_py)\n# and get the fitted parameters of the model\nlm_hrs2_py = model.fit()\n\n# look at the model output\nprint(lm_hrs2_py.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               mean_hrs   R-squared:                       0.158\nModel:                            OLS   Adj. R-squared:                  0.097\nMethod:                 Least Squares   F-statistic:                     2.619\nDate:                Tue, 16 Apr 2024   Prob (F-statistic):              0.128\nTime:                        07:45:04   Log-Likelihood:                -56.429\nNo. Observations:                  16   AIC:                             116.9\nDf Residuals:                      14   BIC:                             118.4\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     42.4204      2.998     14.151      0.000      35.991      48.850\ndistance      -1.4189      0.877     -1.618      0.128      -3.299       0.462\n==============================================================================\nOmnibus:                        4.148   Durbin-Watson:                   2.906\nProb(Omnibus):                  0.126   Jarque-Bera (JB):                2.008\nSkew:                          -0.820   Prob(JB):                        0.366\nKurtosis:                       3.565   Cond. No.                         4.85\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n# visualise using a scatterplot\n(ggplot(productivity_py,\n        aes(x = \"distance\",\n            y = \"projects\",\n            colour = \"cycle\")) +\n     geom_point())\n\n\n\n\n\n\n\n\n\n# create a linear model\nmodel = smf.ols(formula = \"projects ~ distance\",\n                    data = productivity_cycle_py)\n# and get the fitted parameters of the model\nlm_proj3_py = model.fit()\n\n# look at the model output\nprint(lm_proj3_py.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               projects   R-squared:                       0.029\nModel:                            OLS   Adj. R-squared:                 -0.040\nMethod:                 Least Squares   F-statistic:                    0.4187\nDate:                Tue, 16 Apr 2024   Prob (F-statistic):              0.528\nTime:                        07:45:05   Log-Likelihood:                -33.526\nNo. Observations:                  16   AIC:                             71.05\nDf Residuals:                      14   BIC:                             72.60\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      4.6899      0.716      6.547      0.000       3.154       6.226\ndistance      -0.1356      0.210     -0.647      0.528      -0.585       0.314\n==============================================================================\nOmnibus:                        0.919   Durbin-Watson:                   1.657\nProb(Omnibus):                  0.632   Jarque-Bera (JB):                0.694\nSkew:                           0.016   Prob(JB):                        0.707\nKurtosis:                       1.980   Cond. No.                         4.85\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nAh. Turns out we were right to be concerned; when staff members who don’t cycle are removed from the dataset, the significant relationship that we saw earlier between distance and mean_hrs disappears. And the marginally non-significant relationship we observed between distance and projects becomes much less significant.\nThis leaves us with just one significant result: projects ~ cycle. But if we really were trying to report on these data, in a paper or report of some kind, we’d need to think very carefully about how much we trust this result, or whether perhaps we’ve stumbled on a false positive by virtue of running so many tests. We may also want to think carefully about whether or not we’re happy with these definitions of the variables; for instance, is the number of projects completed really the best metric for productivity at work?",
    "crumbs": [
      "Reconsidering variables",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Operationalising variables</span>"
    ]
  },
  {
    "objectID": "materials/operationalisation.html#summary",
    "href": "materials/operationalisation.html#summary",
    "title": "18  Operationalising variables",
    "section": "18.3 Summary",
    "text": "18.3 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nThere are multiple ways to operationalise a variable, which may affect whether the variable is categorical or continuous\nThe nature of the response variable will alter what type of model can be fitted to the dataset\nSome operationalisations may better capture your variable of interest than others\nIf you do not effectively operationalise your variable in advance, you may find yourself “cherry-picking” your dataset",
    "crumbs": [
      "Reconsidering variables",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Operationalising variables</span>"
    ]
  },
  {
    "objectID": "materials/confounds.html",
    "href": "materials/confounds.html",
    "title": "19  Confounds & bias",
    "section": "",
    "text": "19.1 What is a confounding variable?\nA confound is a variable that covaries with both your predictor(s) and outcome variables, such that the relationship between the predictor(s) and outcomes is affected. Confounding variables (sometimes also referred to as extraneous variables) are often controlled for in research, wherever it’s possible to do so (in which case, you may also see them called control variables).\nLet’s have a look at this visually, with an example, because that’s easier to wrap your head around!\nA confound can obscure an effect of interest, as in the example below. When looking at the effect of soil type alone, it would appear that the difference between the two group means is very small, compared to the spread within the groups (remember that a t-test, along with other linear models, is more or less a signal-to-noise ratio!). However, we can see from the graph on the right that a great deal of this variation within the two groups is due to the effect of sunlight. In this sample, sunlight is a confounding variable that has contributed additional variance to our outcome response. When we take sunlight into account and eliminate some of this additional variance, we now see a consistent pattern - at all levels of sunlight, there is a higher rate of growth for soil A than soil B (split above and below the line of best fit).\nA confound may also make it appear than the variable of interest has an effect, when in reality it does not. Consider an alternative version of the same example, below. Here, when we look at soil type alone, we see what seems to be quite a large effect, with quite a big difference between the group means relative to the spread within each group. However, when we also consider the effect of sunlight, it becomes clear that the effect of soil is artificial. Sunlight has varied systematically with soil type in this sample, such that plants in soil A have received notably more sunlight than soil B. The effect of soil type here is minimal, if it exists at all.\nFrom both of these examples, we can see that the reason we need to worry about confounds is that they can cause us to misattribute variance in our analysis - making us over- or under-estimate the variance explained by one or more of our predictor(s). Modelling data is all about trying to correctly interpret the noise and variation that we see in our datasets, to try to find meaningful relationships between variables that hold true not only in our sample, but in the underlying population we’ve drawn from. So, there’s no wonder we worry about confounds!",
    "crumbs": [
      "Reconsidering variables",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Confounds & bias</span>"
    ]
  },
  {
    "objectID": "materials/confounds.html#what-is-a-confounding-variable",
    "href": "materials/confounds.html#what-is-a-confounding-variable",
    "title": "19  Confounds & bias",
    "section": "",
    "text": "The effect of sunlight has obscured the effect of soil type\n\n\n\n\n\n\nThe effect of sunlight has created an “artificial” effect of soil type",
    "crumbs": [
      "Reconsidering variables",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Confounds & bias</span>"
    ]
  },
  {
    "objectID": "materials/confounds.html#sources-of-confounds",
    "href": "materials/confounds.html#sources-of-confounds",
    "title": "19  Confounds & bias",
    "section": "19.2 Sources of confounds",
    "text": "19.2 Sources of confounds\nConfounds and variation can come from a variety of sources, including:\n\nBatch effects\nOrder effects\nSystematic measurement or technical error\nDemand characteristics & placebo effect\nBiased sampling & assignment to conditions\nExperimenter confounds\nEnvironmental confounds\n\nEach of these is discussed in more detail below.\n\nBatch effects\nBatch effects refers to the variation that can occur between repeats or observations, usually introduced by equipment or technical factors. The most common example of batch effects in classical lab work are plate effects. Samples are grouped together in well plates (most commonly a 96-well plate), and there is often a great deal of variation between plates due to differences in the way they are handled and processed. (Within plates, there are also established row, column and edge effects, which can complicate matters even further!) Sometimes, these differences are larger than the differences observed between experimental conditions.\nFor those who find bench work less relatable: another example of batch effects is growing plants across multiple greenhouses for the same study. There may be systematic differences between greenhouses - perhaps some are south-facing, and others are east-facing; perhaps they are looked after by different research assistants, who aren’t consistent in the amount of water they give the plants. And within the greenhouses, plants near the edges will receive more light and have more space to branch out than those near the centre.\n\n\nOrder effects\nOrder effects may occur in repeated measures designs - i.e., when observations in your sample undergo more than one of the experimental conditions - and the order of conditions or treatments alters the effect those conditions have on the outcome variable.\nSometimes, this is because the conditions have unexpected interactions with one another; for instance, if you are testing the efficacy of two different drugs, but drug A blocks the effects of drug B, leading you to believe that drug B doesn’t work when in fact it could. Other times, it may simply be because participants become fatigued, and their performance declines over time (a particular problem if you’re asking people or animals to perform cognitive tasks like memory or puzzle solving), or the opposite might happen - participants might get better with practice.\n\n\nSystematic measurement or technical error\nThis type of error may be generated by the researcher(s), for instance, if a batch of media for cell culture has been prepared incorrectly, or the wrong dosage of a drug has been given. Alternatively, equipment itself can be systematically incorrect or inconsistent, in a way that may covary with variables of interest. In functional MRI studies, for instance, researchers must contend with a phenomenon called “scanner drift”: a gradual decrease in signal intensity over the course of the scan.\nThis type of confound can be a particular problem in multi-site studies, e.g., clinical studies taking place across multiple hospitals, where the model and calibration of equipment is likely to differ.\n\n\nDemand characteristics\nThis is a term borrowed from psychology, in which this type of confound is particularly prevalent. Demand characteristics refers to the unintended extraneous and environmental factors that might give human participants clues about what the experiment is about, and subconsciously change their behaviour in response.\nSometimes, this means that participants will actively try to behave in a way that confirms the experimenter’s hypothesis; other, more contrary folk like to try to “ruin” or “disprove” the hypothesis by behaving in the opposite way. Sometimes, people can get so wrapped up in the anxiety of being evaluated that they stop behaving normally, viewing the experimenter as an authority figure and trying to follow instructions to the letter. (If you’re curious how far this can go, there are several social psychology studies that investigated this phenomenon, back in those crazy days from before ethics committees existed; look up the Milgram experiments.)\nIn fact, you’re probably familiar with the placebo effect, which is a well-known example of how taking part in a study can alter participants’ responses. Simply believing that you’re being given a substance or treatment of some kind can lead to real, psychosomatic responses to something that doesn’t even exist.\n\n\nBiased sampling & assignment\nA key assumption of null hypothesis significance testing is that our sample is representative of the underlying population we’re trying to learn about. But there are lots of things that might make this untrue. It’s a well-known problem in psychology studies, for instance, that human participants who sign up for studies are often quite different from the general population - they’re usually university students with an interest in psychology. In clinical research, too, you may find that only a certain subset of patients are willing or able to take part (e.g., those who are able to travel, or those who are seeking experimental treatments). This can make it hard to generalise to the population of interest.\nEven if your sample is truly randomly selected, it’s also possible to introduce confounds when assigning to different experimental conditions.\n\n\nExperimenter effects\nBeing there to observe the science sometimes means that we accidentally (or even deliberately, but that’s a whole separate discussion) affect the science.\nThis can happen in a bunch of ways. One is the observer-expectancy effect, in which researchers subconsciously influence the experiment due to their own hypotheses or biases, and is especially prevalent in studies with human participants. This might involve asking leading questions or subconsciously give hints as to how to behave, as happened with the case of the famous “counting” horse, Clever Hans (I’ve put some info about him in the drop-down box below, mostly because it’s a cool story).\nThere is also the phenomenon of confirmation bias. Confirmation bias refers to our tendency to seek out information that confirms what we’re already thinking, and disregard or minimise evidence that goes against it. You can imagine how this might be problematic in research - especially if researchers think that they’re immune to it, which sadly, we are not.\nAnd, of course, we have to consider the basic physical effects of being present as a researcher. People and animals will behave differently when there’s someone with a white lab coat in the room with them. Further, individual characteristics of the researcher may affect responses - for instance, it’s a known phenomenon that lab rats will behave differently or be stressed to different degrees when handled by male versus female researchers.\n\n\n\n\n\n\nThe Clever Hans effect\n\n\n\n\n\nHans was a horse who became famous around the turn of the 19th century in Germany for supposedly being able to count, among other skills (including working with fractions, telling time, reading calendars and even reading and understanding German). When asked a question, he would paw the ground with a hoof the appropriate number of times to give his response, and he was amazingly accurate.\nSadly, of course, Hans was not actually a genius horse performing all these tasks - he was getting his cues from his trainer and owner, Wilhelm von Osten. Hans was picking up on subtle changes in body language that let him know when to stop “counting”; this meant that he couldn’t answer questions that his owner didn’t know the answer to. Crucially, van Osten had no idea he was providing these cues at all, and truly believed his pony was just super smart.\nThe term “Clever Hans effect” has been coined in honour of the study on Hans performed by psychologist Oskar Pfungst, who uncovered what was really going on. The Clever Hans effect is an example of the observer-expectancy effect that most often crops up in studies of animal cognition.\n\n\n\n\n\nEnvironmental confounds\nThis is something of a catch-all term, and you’ll notice that lots of the types of confounds described above could also be described as “environmental” confounds. Here, though, I’m talking specifically about random and unpredictable environmental effects that can’t really be controlled for.\nWe conduct experiments in some kind of physical space, and those spaces are rarely entirely under our control. Sometimes, stuff happens! Perhaps the fridge you’re using to store some of your samples happened to malfunction and got a bit too warm overnight without you realising. Perhaps some of the soil you’re using is too high in nitrogen. Perhaps the testing room was too hot, and the participants who came in for testing on those days were grumpy.",
    "crumbs": [
      "Reconsidering variables",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Confounds & bias</span>"
    ]
  },
  {
    "objectID": "materials/confounds.html#dealing-with-confounds",
    "href": "materials/confounds.html#dealing-with-confounds",
    "title": "19  Confounds & bias",
    "section": "19.3 Dealing with confounds",
    "text": "19.3 Dealing with confounds\n\nMatched pairs designs and blocked randomisation to conditions\nThe aim of both of these methods is to assign participants or observations to experimental groups, while keeping those groups as similar to one another as possible. Often, this is to make sure that demographic variables like gender or age don’t differ systematically between groups.\nA matched pairs design would be used in situations where you as the researcher don’t have any control over how the groups are split. For instance, if you’re comparing patients to healthy controls, you don’t get to assign people to groups - the groups already exist in the environment. What is typically done in these situations is to start with the experimental or patient group, and find a control that matches each of them (for factors like age, gender, education level, and any other possible confounds). This can be quite time consuming, and often requires having a pool of controls to choose from, but it does ensure that there aren’t any systematic differences between the groups that might obscure or exaggerate a real effect of the predictor variable(s).\n\n\n\nMatched pairs design\n\n\nBlocked randomisation can be used in situations where you as the researcher do control which observations or participants are assigned to which group. The first step is to construct blocks within your sample, each of which is broadly representative of the sample as a whole (with respect to the variables that you want to control for). These blocks are then assigned randomly to conditions, which ensures that the sample is stratified.\n\n\n\nBlocked randomisation to conditions\n\n\n\n\nRandomisation\nRandomisation is a relatively broad term in experimental design - it can refer to random selection of the sample, or random allocation to conditions, which has been discussed above.\nRandomisation can be particularly useful in avoiding batch effects, and/or predictable, systematic environmental effects. It essentially involves “mixing” replicates of the different conditions across batches (whether these batches constitute well-plates, greenhouses, clinical sites and so on), rather than having only one condition per batch. This means that any environmental or experimenter variables that affect the batch, will affect replicates of all the conditions similarly. If it’s done properly, randomisation allows us to estimate the variation that’s due to batch differences, and leave behind the variation that’s due to our predictor variable(s) of interest.\nAn example of how randomisation can help to eliminate unwanted batch effects can be found in this blog post discussing the GenADA study on Alzheimer’s by GSK.\n The figure above shows a simplified version of how we might use randomisation. On the far left, we have a batch in which all replicates are from a single condition - this is something we absolutely want to avoid. Our two examples on the right are randomised, where conditions are mixed up between rows and columns; on the far right, we have a special example of such randomisation that we call a Latin square, in which each condition is replicated only once on each row and in each column (I think of it like a sudoku puzzle).\nOf course, it’s not always possible to completely intermingle your replicates like this. Sometimes, attempting to perfectly randomise every single replicate can lead to errors - lab work is a good example of this, since pipetting randomly into a well-plate can really increase the chance of experimenter error, which may be more damaging than the batch effects you were trying to avoid. As a compromise, partially randomised designs may work instead, as shown in the middle left. Although we still have potential row effects here, we have at least avoided the issue of only including one condition per batch!\n\n\nCounterbalancing\nThe primary use of counterbalancing is to prevent order effects. In a repeated measures design, the sample can be “counterbalanced” such that the conditions are presented in a different order to different subsets of that sample.\nWhen counterbalancing, your sample should be equally divided across the counterbalanced conditions/groups; further, within each experimental group, that subset of the sample should also be equally divided across the counterbalanced groups to avoid any systematic bias. This means that the assignment to counterbalanced groups ends up being pseudo-randomised.\nHere’s a visualisation to help explain what that might look like. The sample has a total n = 8, and is split into two experimental groups (let’s pretend we’re interested in the effect of pink versus green). We want each person to perform two different tasks, but we want to make sure that if they’re getting tired after the first one, or if the first task somehow biases their answers to the second, that we’re not disproportionately seeing the impact on Task B. So, we have two different possible task orders, creating two counterbalancing groups/conditions. We assign an equal number to each of those conditions, and further, we have an equal stratification of pink versus green within each of those conditions:\n\n\n\nCounterbalancing a sample across two tasks\n\n\n\n\nBlinding\nIn a single blind protocol, participants are aware which condition they have been assigned to. Of course, the majority of experiments in the life sciences are single blinded, since things like cell cultures, plants, receptors, enzymes etc. aren’t anywhere close to sentient enough to figure things out about the experiment they’re in. This technique is mainly used for studies with human participants, where the control group(s) are given a placebo or sham treatment, such that the overall experience is as close as possible to that of the experimental group.\nIn a double blind protocol, often considered to be the best choice for studies such as clinical trials, both the participant and the experimenter are blind to which condition the participant has been assigned to. In addition to eliminating placebo effects, this also means that the researcher cannot subconsciously influence the results (e.g., by treating groups differently).\nData analysis can also be blinded - i.e., the data is collected and analysed by different researchers. This helps to remove influences of confirmation bias.\n\n\nCovariates of no interest\nSometimes, there is no good way of mitigating a confound in your experimental design. All is not necessarily lost, however, because in some of those cases, you can include a confound as a covariate of no interest in your model.\nThere are two things that need to be true to be able to include a confound as a covariate of no interest:\nIt needs to be continuous.\nExactly as it says on the tin - only continuous variables can be included as covariates of no interest in a model.\nThe confound cannot interact with any of your predictors of interest.\nIf there’s a significant interaction between the covariate and any of your predictors, then it can no longer be considered “of no interest”. Let’s revisit the soil type and sunlight example from the top of this page to explain why. In both of the scenarios I showed earlier, there is no interaction between soil type and sunlight - the effects are additive. This means it’s nice and simple to chuck sunlight into the model and let R/Python figure out how much variance to subtract, and what’s left over is for the predictor(s) you really care about.\nHowever, in scenarios such as the one below, this wouldn’t work:\n\n\n\nSunlight cannot be treated as a covariate of no interest\n\n\nThe effect of soil type on growth isn’t now simply being obscured by sunlight - it depends on it. We now need to talk about sunlight in order to be able to describe the effect of soil type. We can no longer discard sunlight as a covariate of no interest.\nThis isn’t necessarily a bad thing, though. This has told us something interesting about plant growth, and indicates that we really should have been thinking about sunlight from the beginning.\nSo, how do you actually include a covariate of no interest in a model?\nGood news! It’s incredibly easy. You’ve probably even done it before without realising.\nIt’s as simple as including the covariate of no interest in the model as if it were any other predictor (though, depending on the exact function you’re using, you may need to ensure that you include it as the first term in the model). And, don’t forget to check for an interaction with other variables by visualising your dataset first.",
    "crumbs": [
      "Reconsidering variables",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Confounds & bias</span>"
    ]
  },
  {
    "objectID": "materials/confounds.html#summary",
    "href": "materials/confounds.html#summary",
    "title": "19  Confounds & bias",
    "section": "19.4 Summary",
    "text": "19.4 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nA confounding variable covaries with the predictor(s) and outcome variables\nThere are multiple sources of confounds, including the environment, researcher or technical error, and features of the sample\nMany confounds can be controlled for using techniques such as randomisation, blinding, or matched samples\nSome confounds can be accounted for during analysis, by including them as covariates of no interest in the model",
    "crumbs": [
      "Reconsidering variables",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Confounds & bias</span>"
    ]
  },
  {
    "objectID": "materials/replication.html",
    "href": "materials/replication.html",
    "title": "20  Independence & replication",
    "section": "",
    "text": "20.1 Libraries and functions",
    "crumbs": [
      "Reconsidering variables",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Independence & replication</span>"
    ]
  },
  {
    "objectID": "materials/replication.html#libraries-and-functions",
    "href": "materials/replication.html#libraries-and-functions",
    "title": "20  Independence & replication",
    "section": "",
    "text": "Click to expand\n\n\n\n\n\n\nRPython\n\n\n\n20.1.1 Libraries\n\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\n\n\n\n\n20.1.2 Libraries\n\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Simple yet exhaustive stats functions.\nimport pingouin as pg\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# Statistical models, conducting tests and statistical data exploration\nimport statsmodels.api as sm\n\n# Convenience interface for specifying models using formula strings and DataFrames\nimport statsmodels.formula.api as smf",
    "crumbs": [
      "Reconsidering variables",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Independence & replication</span>"
    ]
  },
  {
    "objectID": "materials/replication.html#exercise-1---flower-petals",
    "href": "materials/replication.html#exercise-1---flower-petals",
    "title": "20  Independence & replication",
    "section": "20.2 Exercise 1 - Flower petals",
    "text": "20.2 Exercise 1 - Flower petals\nThis dataset contains three variables: shade, which refers to the degree of shading that the plant received while growing; petals, the number of petals recorded on an individual flower of that plant; and plant, the numerical ID assigned to the plant.\n\nRPython\n\n\n\nflowers &lt;- read_csv(\"data/flowers.csv\")\n\n\n\n\n# load the data\nflowers_py = pd.read_csv(\"data/flowers.csv\")\n\n# and have a look\nflowers_py.head()\n\n  shade  petals  plant\n0  none       3      1\n1  none       3      1\n2  none       6      1\n3  none       6      1\n4  none       4      1\n\n\n\n\n\nHaving read in the dataset, we can start by doing some visualisation and analysis. Let’s have a look at how the petal number differs across the shade conditions, and then run a one-way ANOVA to compare the groups statistically.\n\nRPython\n\n\n\n# construct a boxplot, grouped by shade\n\nflowers %&gt;%\n  ggplot(aes(x = shade, y = petals)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nNext, we run a one-way ANOVA:\n\n# create a linear model and run an ANOVA\nlm_flowers &lt;- lm(petals ~ shade, data = flowers)\nanova(lm_flowers)\n\nAnalysis of Variance Table\n\nResponse: petals\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nshade      2  50.70 25.3500   13.51 1.575e-05 ***\nResiduals 57 106.95  1.8763                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n# visualise using a boxplot\n(ggplot(flowers_py,\n        aes(x = \"shade\",\n            y = \"petals\")) +\n     geom_boxplot())\n\n\n\n\n\n\n\n\nNext, we run a one-way ANOVA:\n\n# construct a one-way ANOVA\npg.anova(dv = \"petals\",\n         between = \"shade\",\n         data = flowers_py,\n         detailed = True).round(5)\n\n   Source      SS  DF        MS         F    p-unc     np2\n0   shade   50.70   2  25.35000  13.51052  0.00002  0.3216\n1  Within  106.95  57   1.87632       NaN      NaN     NaN\n\n\n\n\n\nThe plot and one-way ANOVA are both pretty convincing. It looks as if there are most petals on flowers in full sun, and the least petals on flowers in full shade, with partial shade somewhere in the middle.\nHowever, you may have noticed something about this dataset - namely, that multiple measurements of the petals variable have been made per plant. Or, to put it another way, though we have biological replicates by having measured from 12 different plants, our petals measurements appear to be technical replicates.\nThis dataset is a prime example of pseudoreplication.\nLet’s adapt this dataset, by finding the mean petal count per plant.\n\nRPython\n\n\n\nmean_flowers &lt;- flowers %&gt;%\n  group_by(plant, shade) %&gt;%\n  summarise(petals = mean(petals))\n\nmean_flowers\n\n# A tibble: 12 × 3\n# Groups:   plant [12]\n   plant shade   petals\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n 1     1 none       4.4\n 2     2 none       7.2\n 3     3 none       6.6\n 4     4 none       5.8\n 5     5 partial    5.8\n 6     6 partial    5.2\n 7     7 partial    3.4\n 8     8 partial    4.8\n 9     9 full       5  \n10    10 full       4.2\n11    11 full       3.2\n12    12 full       2.6\n\n\n\n\n\nmean_flowers_py = flowers_py.groupby(['plant', 'shade']).mean().reset_index()\n\nmean_flowers_py\n\n    plant    shade  petals\n0       1     none     4.4\n1       2     none     7.2\n2       3     none     6.6\n3       4     none     5.8\n4       5  partial     5.8\n5       6  partial     5.2\n6       7  partial     3.4\n7       8  partial     4.8\n8       9     full     5.0\n9      10     full     4.2\n10     11     full     3.2\n11     12     full     2.6\n\n\n\n\n\nNow, we have a much clearer n = 12. What happens if we re-run our analyses, with these mean values?\n\nRPython\n\n\n\n# construct a new boxplot\nmean_flowers %&gt;%\n  ggplot(aes(x = shade, y = petals)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nRun a new ANOVA:\n\n# ANOVA on the mean petal counts per plant\nlm_mean &lt;- lm(petals ~ shade, data = mean_flowers)\nanova(lm_mean)\n\nAnalysis of Variance Table\n\nResponse: petals\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)  \nshade      2  10.14  5.0700  4.1824 0.05195 .\nResiduals  9  10.91  1.2122                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n# visualise using a boxplot\n(ggplot(mean_flowers_py,\n        aes(x = \"shade\",\n            y = \"petals\")) +\n     geom_boxplot())\n\n\n\n\n\n\n\n\nNext, we run a one-way ANOVA:\n\n# construct a one-way ANOVA\npg.anova(dv = \"petals\",\n         between = \"shade\",\n         data = mean_flowers_py,\n         detailed = True).round(3)\n\n   Source     SS  DF     MS      F  p-unc    np2\n0   shade  10.14   2  5.070  4.182  0.052  0.482\n1  Within  10.91   9  1.212    NaN    NaN    NaN\n\n\n\n\n\nIf anything, the resulting boxplot looks more convincing than it did before. However, we don’t get the same picture with the ANOVA. The p-value is far larger than before, to the point where this analysis is no longer significant. The reason for this is simple - previously, we ran an analysis with a false n = 60, which gave enough power to detect an effect. However, using the true n = 12, we discover that all that statistical power was an illusion or artefact, and with just 12 plants, we can see only the beginning of a trend.",
    "crumbs": [
      "Reconsidering variables",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Independence & replication</span>"
    ]
  },
  {
    "objectID": "materials/replication.html#exercise-2---cabbages",
    "href": "materials/replication.html#exercise-2---cabbages",
    "title": "20  Independence & replication",
    "section": "20.3 Exercise 2 - Cabbages",
    "text": "20.3 Exercise 2 - Cabbages\nEach row in the cabbages dataset refers to an individual cabbage, harvested by a farmer who has been trying to find the optimum levels of fertiliser in his six fields. There are four variables: response variable weight, the weight of individual cabbages; N_rate, the rate of nitrogen fertiliser applied to the field in kilograms per metre; fertiliser, a categorical variable describing whether the fertiliser was liquid or granular; and field, the ID of the field that the cabbage was harvested from.\nStart by reading in the dataset. It’s also important that we tell R to treat the N_rate variable as an ordinal variable, or factor, rather than as a continuous numerical variable.\n\nRPython\n\n\n\ncabbages &lt;- read_csv(\"data/cabbages.csv\")\n\n# convert the N_rate column to factor\ncabbages &lt;- cabbages %&gt;%\n  mutate(N_rate = as.factor(N_rate))\n\n\n\n\n# load the data\ncabbages_py = pd.read_csv(\"data/cabbages.csv\")\n\n# convert the N_rate column to factor\ncabbages_py['N_rate'] = cabbages_py['N_rate'].astype('category')\n\n\n\n\nThe farmer is interested in knowing whether nitrogen rate and fertiliser type affects the weight of harvested cabbages in his fields.\nOn the face of it, you may therefore start by fitting a linear model with these two variables as predictors (since they’re both categorical, that’s a two-way ANOVA):\n\nRPython\n\n\n\nlm_cabbage &lt;- lm(weight ~ N_rate * fertiliser, data = cabbages)\nanova(lm_cabbage)\n\nAnalysis of Variance Table\n\nResponse: weight\n                  Df  Sum Sq  Mean Sq F value  Pr(&gt;F)  \nN_rate             2 0.23306 0.116531  4.6861 0.01176 *\nfertiliser         1 0.02891 0.028910  1.1626 0.28402  \nN_rate:fertiliser  2 0.23343 0.116715  4.6935 0.01169 *\nResiduals         84 2.08885 0.024867                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n# create a linear model\nmodel = smf.ols(formula = \"weight ~ N_rate * fertiliser\",\n                data = cabbages_py)\n# and get the fitted parameters of the model\nlm_cabbages_py = model.fit()\n\n# look at the model output\nsm.stats.anova_lm(lm_cabbages_py)\n\n                     df    sum_sq   mean_sq         F    PR(&gt;F)\nN_rate              2.0  0.233062  0.116531  4.686123  0.011765\nfertiliser          1.0  0.028910  0.028910  1.162576  0.284018\nN_rate:fertiliser   2.0  0.233429  0.116715  4.693507  0.011687\nResidual           84.0  2.088850  0.024867       NaN       NaN\n\n\n\n\n\nThis indicates that there is a significant interaction between N_rate and fertiliser. To help us visualise the direction of that effect, we can plot the data as follows:\n\nRPython\n\n\n\ncabbages %&gt;%\n  ggplot(aes(x = N_rate, y = weight, fill = fertiliser)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\n# visualise using a boxplot\n(ggplot(cabbages_py,\n        aes(x = \"N_rate\",\n            y = \"weight\",\n            fill = \"fertiliser\")) +\n     geom_boxplot())\n\n\n\n\n\n\n\n\n\n\n\nTogether with the ANOVA table, you might be able to make some recommendations to the farmer about the optimum fertiliser programme for his cabbages.\nBut - is this a sensible approach? Do we trust the conclusions?\nTo help you answer that question, let’s visualise the effect of the field variable, and its relationship to other variables, with a plot:\n\nRPython\n\n\n\ncabbages %&gt;%\n  ggplot(aes(x = field, y = weight,\n             colour = fertiliser, size = N_rate)) +\n  geom_point()\n\nWarning: Using size for a discrete variable is not advised.\n\n\n\n\n\n\n\n\n\n\n\n\n# visualise using a boxplot\n(ggplot(cabbages_py,\n        aes(x = \"field\", y = \"weight\",\n            colour = \"fertiliser\",\n            size = \"N_rate\")) +\n     geom_point())\n\n\n\n\n\n\n\n\n\n\n\nThis is rudimentary, but it hopefully helps to illustrate one of two problems with the approach taken here: our different treatments/conditions in the fertiliser and N_rate variables have been applied, wholesale, to entire fields. Which makes sense, practically speaking - it’s hard to see how you would do any differently - but it does mean that there are issues with treating individual cabbages as independent observations, rather than technical replicates.\nHave a think about how you could actually investigate this question, using the dataset presented here. What is our actual value of n? (Or put another way: which are our biological replicates?) What kind of model might you fit instead of the linear model fitted above?",
    "crumbs": [
      "Reconsidering variables",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Independence & replication</span>"
    ]
  },
  {
    "objectID": "materials/replication.html#criteria-for-true-independent-replication",
    "href": "materials/replication.html#criteria-for-true-independent-replication",
    "title": "20  Independence & replication",
    "section": "20.4 Criteria for true independent replication",
    "text": "20.4 Criteria for true independent replication\nConfusing our biological and technical replicates leads to pseudoreplication, as discussed above. So, how do we make sure that we truly do have biological replicates?\nFor a replicate to qualify as a biological, rather than technical replicate, it needs to meet three criteria for independence. These are:\n1) Independent randomisation to different treatment conditions\nThere should be no systematic bias in how biological replicates are allocated to conditions. This means that allocations can’t be made on the basis of sample characteristics. In the first example above, the flowers weren’t randomly assigned to different shade conditions - they were assigned on the basis of which plant they were growing on, meaning that they weren’t independent of one another.\n2) The experimental intervention must be applied independently\nThis is to ensure that any technical error is random. In the second example above, if the farmer incorrectly measures the nitrogren he’s adding to one of his fields, this will affect more than just a single cabbage - it will likely affect a whole group of them, if not the entire field.\n3) Data points/biological replicates must not influence each other\nWhether they are from the same or different conditions, biological replicates shouldn’t have an affect on one another (at least not before you’ve collected the data you need!). This may involve human participants conferring about the study, or in an experiment that involves cell culture, may involve organisms competing with one another for resources and affecting the rate of growth.",
    "crumbs": [
      "Reconsidering variables",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Independence & replication</span>"
    ]
  },
  {
    "objectID": "materials/replication.html#summary",
    "href": "materials/replication.html#summary",
    "title": "20  Independence & replication",
    "section": "20.5 Summary",
    "text": "20.5 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nBiological replicates increase n, while technical replicates do not\nThe value of n can have a meaningful impact on the results of significance tests\nPseudoreplication in a sample can lead to a researcher drawing the wrong conclusions",
    "crumbs": [
      "Reconsidering variables",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Independence & replication</span>"
    ]
  },
  {
    "objectID": "materials/cs6_practical_power-analysis.html",
    "href": "materials/cs6_practical_power-analysis.html",
    "title": "21  Power analysis",
    "section": "",
    "text": "21.1 Libraries and functions",
    "crumbs": [
      "Statistical power",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Power analysis</span>"
    ]
  },
  {
    "objectID": "materials/cs6_practical_power-analysis.html#libraries-and-functions",
    "href": "materials/cs6_practical_power-analysis.html#libraries-and-functions",
    "title": "21  Power analysis",
    "section": "",
    "text": "Click to expand\n\n\n\n\n\n\nRPython\n\n\n\n21.1.1 Libraries\n\n# A library for power analysis\nlibrary(pwr)\n\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\n# Converts stats functions to a tidyverse-friendly format\nlibrary(rstatix)\n\n\n\n21.1.2 Functions\n\n\n\n\n21.1.3 Libraries\n\n\n21.1.4 Functions",
    "crumbs": [
      "Statistical power",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Power analysis</span>"
    ]
  },
  {
    "objectID": "materials/cs6_practical_power-analysis.html#background",
    "href": "materials/cs6_practical_power-analysis.html#background",
    "title": "21  Power analysis",
    "section": "21.2 Background",
    "text": "21.2 Background\nAll hypothesis tests can be wrong in two ways:\n\nwe can appear to have found a significant result when there really isn’t anything there: a false positive (or Type I error), or\nwe can fail to spot a significant result when there really is something interesting going on: a false negative (or Type II error).\n\nThe probability of getting a false positive in our analysis is precisely the significance level we use in our analysis. So, in order to reduce the likelihood of getting a false positive we simply reduce the significance level of our test (from 0.05 down to 0.01 say). Easy as that.\nUnfortunately, this has unintended consequences (doesn’t everything?). It turns out that reducing the significance level means that we increase the chance of getting false negatives. This should make sense; if we’re increasing the barrier to entry in terms of acceptance then we’ll also accidentally miss out on some of the good stuff.\nPower is the capacity of a test to detect significant different results. It is affected by three things:\n\nthe effect size: i.e. how big of a difference do you want to be able to detect, or alternatively what do you consider a meaningful effect/difference to be?\nsample size\nthe significance level\n\nIn an ideal world we would want to be carrying out highly powerful tests using low significance levels, to both reduce our chance of getting a false positive and maximise our chances of finding a true effect.\nPower analysis allows us to design experiments to do just that. Given:\n\na desired power (0.8 or 80% is considered pretty good)\na significance level (0.05 or 5% is our trusty yet arbitrary steed once again)\nan effect size that we would like to detect\n\nWe can calculate the amount of data that we need to collect in our experiments. (Woohoo! it looks as if statistics will actually give us an answer at last rather than these perpetual shades-of-grey “maybes”).\nThe reality is that most of the easily usable power analysis functions all operate under the assumption that the data that you will collect will meet all of the assumptions of your chosen statistical test perfectly. So, for example, if you want to design an experiment investigating the effectiveness of a single drug compared to a placebo (so a simple t-test) and you want to know how many patients to have in each group in order for the test to work, then the standard power analysis techniques will still assume that all of the data that you end up collecting will meet the assumptions of the t-test that you have to carry out (sorry to have raised your hopes ever so slightly 😉).\n\n21.2.1 Effect size\nAs we shall see the commands for carrying out power analyses are very simple to implement apart from the concept of effect size. This is a tricky issue for most people to get to grips with for two reasons:\n\nEffect size is related to biological significance rather than statistical significance\nThe way in which we specify effect sizes\n\n\n\n\n\n\n\nNote\n\n\n\nWith respect to the first point a common conversation goes a bit like this:\nme: “So you’ve been told to carry out a power analysis, eh? Lucky you. What sort of effect size are you looking for?”\nyou: “I have no idea what you’re talking about. I want to know if my drug is any better than a placebo. How many patients do I need?”\nme: “It depends on how big a difference you think your drug will have compared to the placebo.”\nyou: “I haven’t carried out my experiment yet, so I have absolutely no idea how big the effect will be!”\nme: \n(To be honest this would be a relatively well-informed conversation: this is much closer to how things actually go)\n\n\nThe key point about effect sizes and power analyses is that you need to specify an effect size that you would be interested in observing, or one that would be biologically relevant to see. There may well actually be a 0.1% difference in effectiveness of your drug over a placebo but designing an experiment to detect that would require markedly more individuals than an experiment that was trying to detect a 50% difference in effectiveness. In reality there are three places we can get a sense of effect sizes from:\n\nA pilot study\nPrevious literature or theory\nJacob Cohen\n\nJacob Cohen was an American statistician who developed a large set of measures for effect sizes (which we will use today). He came up with a rough set of numerical measures for “small”, “medium” and “large” effect sizes that are still in use today. These do come with some caveats though; Jacob was a psychologist and so his assessment of what was a large effect may be somewhat different from yours. They do form a useful starting point however.\nThere a lot of different ways of specifying effects sizes, but we can split them up into three distinct families of estimates:\n\nCorrelation estimates: these use \\(R^2\\) as a measure of variance explained by a model (for linear models, anova etc. A large \\(R^2\\) value would indicate that a lot of variance has been explained by our model and we would expect to see a lot of difference between groups, or a tight cluster of points around a line of best fit. The argument goes that we would need fewer data points to observe such a relationship with confidence. Trying to find a relationship with a low \\(R^2\\) value would be trickier and would therefore require more data points for an equivalent power.\nDifference between means: these look at how far apart the means of two groups are, measured in units of standard deviations (for t-tests). An effect size of 2 in this case would be interpreted as the two groups having means that were two standard deviations away from each other (quite a big difference), whereas an effect size of 0.2 would be harder to detect and would require more data to pick it up.\nDifference between count data: these I freely admit I have no idea how to intuitively explain them (shock, horror). Mathematically they are based on the chi-squared statistic but that’s as good as I can tell you I’m afraid. They are, however, pretty easy to calculate.\n\nFor reference here are some of Cohen’s suggested values for effect sizes for different tests. You’ll probably be surprised by how small some of these are.\n\n\n\nTest\nSmall\nMedium\nLarge\n\n\n\n\nt-tests\n0.2\n0.5\n0.8\n\n\nanova\n0.1\n0.25\n0.4\n\n\nlinear models\n0.02\n0.15\n0.35\n\n\nchi-squared\n0.1\n0.3\n0.5\n\n\n\nWe will look at how to carry out power analyses and estimate effect sizes in this section.",
    "crumbs": [
      "Statistical power",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Power analysis</span>"
    ]
  },
  {
    "objectID": "materials/cs6_practical_power-analysis.html#power-analysis-t-test",
    "href": "materials/cs6_practical_power-analysis.html#power-analysis-t-test",
    "title": "21  Power analysis",
    "section": "21.3 Power analysis t-test",
    "text": "21.3 Power analysis t-test\nThe first example we’ll look at is how to perform a power analysis on two groups of data.\nLet’s assume that we want to design an experiment to determine whether there is a difference in the mean price of what male and female students pay at a cafe. How many male and female students would we need to observe in order to detect a “medium” effect size with 80% power and a significance level of 0.05?\nWe first need to think about which test we would use to analyse the data. Here we would have two groups of continuous response. Clearly a t-test.\n\n21.3.1 Determine effect size\nThe first thing we need to do is figure out what a “medium” effect size is. In absence of any further information we refer back to Cohen’s effect sizes.\n\nRPython\n\n\nWe’re using the pwr library, so make sure that you have installed and loaded it with the following commands:\n\n# install pwr package if needed\ninstall.packages(\"pwr\")\n\n# load the pwr package\nlibrary(pwr)\n\nWe can get Cohen’s effect size using the cohen.ES() function (ES stands for Effect Size):\n\ncohen.ES(test = \"t\", size = \"medium\")\n\n\n     Conventional effect size from Cohen (1982) \n\n           test = t\n           size = medium\n    effect.size = 0.5\n\n\nThis function just returns the default conventional values for effect sizes as determined by Jacob Cohen back in the day. It just saves us scrolling back up the page to look at the table I provided. It only takes two arguments:\n\ntest which is one of\n\n“t”, for t-tests,\n“anova” for anova,\n“f2” for linear models\n“chisq” for chi-squared test\n\nsize, which is just one of “small”, “medium” or “large”.\n\nThe bit we want is on the bottom line; we apparently want an effect size of 0.5.\n\n\nUnlike in R, Cohen’s effect sizes are not available through a package (that I am aware of). So in this case we’re referring back to the effect size table we saw earlier and define “medium” as 0.5.\n\n\n\nFor this sort of study effect size is measured in terms of Cohen’s d statistic. This is simply a measure of how different the means of the two groups are expressed in terms of the number of standard deviations they are apart from each other. So, in this case we’re looking to detect two means that are 0.5 standard deviations away from each other. In a minute we’ll look at what this means for real data.\n\n\n21.3.2 Calculating sample sizes\n\nRPython\n\n\nWe do this as follows:\n\npwr.t.test(d = 0.5, sig.level = 0.05, power = 0.8,\n           type = \"two.sample\", alternative = \"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 63.76561\n              d = 0.5\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nThe first line is what we’re looking for n = 63.76 tells that we need 64 (rounding up) students in each group (so 128 in total) in order to carry out this study with sufficient power. The other lines should be self-explanatory (well they should be by this stage; if you need me to tell you that the function is just returning the values that you’ve just typed in then you have bigger problems to worry about).\nThe pwr.t.test() function has six arguments. Two of them specify what sort of t-test you’ll be carrying out * type; which describes the type of t-test you will eventually be carrying out (one of two.sample, one.sample or paired), and * alternative; which describes the type of alternative hypothesis you want to test (one of two.sided, less or greater)\nThe other four arguments are what is used in the power analysis:\n\nd; this is the effect size, a single number calculated using Cohen’s d statistic.\nsig.level; this is the significance level\npower; is the power\nn; this is the number of observations per sample.\n\n\n\nWe do this with the power_ttest() function from pingouin:\n\npg.power_ttest(d = 0.5,\n               alpha = 0.05,\n               power = 0.80)\n\n63.76561137745676\n\n\nThe output n = 63.76 tells that we need 64 (rounding up) students in each group (so 128 in total) in order to carry out this study with sufficient power.\nThe power_ttest() function has six arguments. Two of them specify what sort of t-test you’ll be carrying out * contrast; which describes the type of t-test you will eventually be carrying out (one of one-sample, two-samples or paired), and * alternative; which describes the type of alternative hypothesis you want to test. These can be \"two-sided\" (default), \"greater\" or \"less\"\nThe other four arguments are what is used in the power analysis:\n\nd; this is the effect size, a single number calculated using Cohen’s d statistic.\nalpha; this is the significance level (default is 0.05)\npower; is the power\nn; this is the number of observations per sample.\n\n\n\n\nThe function works by allowing you to specify any three of these four arguments and the function works out the fourth. In the example above we have used the test in the standard fashion by specifying power, significance and desired effect size and getting the function to tell us the necessary sample size.\n\n\n21.3.3 Calculating effect sizes\nWe can use the function to answer a different question:\n\nIf I know in advance that I can only observe 30 students per group, what is the effect size that I should be able to observe with 80% power at a 5% significance level?\n\nLet’s see how we do this:\n\nRPython\n\n\n\npwr.t.test(n = 30, sig.level = 0.05, power = 0.8,\n           type = \"two.sample\", alternative = \"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 30\n              d = 0.7356292\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n\n\npg.power_ttest(n = 30,\n               alpha = 0.05,\n               power = 0.80,\n               contrast = \"two-samples\",\n               alternative = \"two-sided\")\n\n0.73562107868047\n\n\n\n\n\nThis time we want to see what the effect size is so we look at the second line and we can see that an experiment with this many people would only be expected to detect a difference in means of d = 0.74 standard deviations. Is this good or bad? Well, it depends on the natural variation of your data; if your data is really noisy then it will have a large variation and a large standard deviation which will mean that 0.74 standard deviations might actually be quite a big difference between your groups. If on the other hand your data doesn’t vary very much, then 0.74 standard deviations might actually be a really small number and this test could pick up even quite small differences in mean.",
    "crumbs": [
      "Statistical power",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Power analysis</span>"
    ]
  },
  {
    "objectID": "materials/cs6_practical_power-analysis.html#power-analysis-on-data",
    "href": "materials/cs6_practical_power-analysis.html#power-analysis-on-data",
    "title": "21  Power analysis",
    "section": "21.4 Power analysis on data",
    "text": "21.4 Power analysis on data\nIn both of the previous two examples we were a little bit context-free in terms of effect size. Let’s look at how we can use a pilot study with real data to calculate effect sizes and perform a power analysis to inform a future study.\nLet’s look again at the fishlength data we saw in the first practical relating to the lengths of fish from two separate rivers. This is saved as data/CS1-twosample.csv.\n\nRPython\n\n\n\n# read in the data\nfishlength &lt;- read_csv(\"data/CS1-twosample.csv\")\n\n# visualise the data\nfishlength %&gt;% \n  ggplot(aes(x = river, y = length)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nFrom the plot we can see that the groups appear to have different means. This difference is significant, as per a two-sample t-test:\n\n# perform t-test\nt.test(length ~ river,\n       data = fishlength,\n       var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  length by river\nt = 3.8433, df = 66, p-value = 0.0002754\nalternative hypothesis: true difference in means between group Aripo and group Guanapo is not equal to 0\n95 percent confidence interval:\n 0.9774482 3.0909868\nsample estimates:\n  mean in group Aripo mean in group Guanapo \n             20.33077              18.29655 \n\n\n\n\n\n# read in the data\nfishlength_py = pd.read_csv(\"data/CS1-twosample.csv\")\n\n# visualise the data\n(ggplot(fishlength_py, aes(x = \"river\", y = \"length\")) +\n     geom_boxplot())\n\n\n\n\n\n\n\n\nFrom the plot we can see that the groups appear to have different means. This difference is significant, as per a two-sample t-test.\nThe ttest() function in pingouin needs two vectors as input, so we split the data as follows:\n\naripo = fishlength_py.query('river == \"Aripo\"')[\"length\"]\nguanapo = fishlength_py.query('river == \"Guanapo\"')[\"length\"]\n\nNext, we perform the t-test. We specify that the variance are equal by setting correction = False. We also transpose() the data, so we can actually see the entire output.\n\npg.ttest(aripo, guanapo,\n         correction = False).transpose()\n\n                   T-test\nT                3.843267\ndof                    66\nalternative     two-sided\np-val            0.000275\nCI95%        [0.98, 3.09]\ncohen-d          0.942375\nBF10               92.191\npower            0.966135\n\n\n\n\n\nCan we use this information to design a more efficient experiment? One that we would be confident was powerful enough to pick up a difference in means as big as was observed in this study but with fewer observations?\nLet’s first work out exactly what the effect size of this previous study really was by estimating Cohen’s d using this data.\n\nRPython\n\n\nTo this, we use the cohens_d function from the rstatix package:\n\ncohens_d(length ~ river,\n         var.equal = TRUE,\n         data = fishlength)\n\n# A tibble: 1 × 7\n  .y.    group1 group2  effsize    n1    n2 magnitude\n* &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;ord&gt;    \n1 length Aripo  Guanapo   0.942    39    29 large    \n\n\nThe cohens_d() function calculates the effect size using the formula of the test. The effsize column contains the information that we want, in this case 0.94 .\nWe can now actually answer our question and see how many fish we really need to catch in the future:\n\npwr.t.test(d = 0.94, power = 0.8, sig.level = 0.05,\n           type = \"two.sample\", alternative = \"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 18.77618\n              d = 0.94\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n\nTo do this, we use the compute_effsize() function from pingouin. This takes two vectors as input, so we use the aripo and guanapo objects we created earlier:\n\npg.compute_effsize(aripo, guanapo,\n                   paired = False,\n                   eftype = \"cohen\")\n\n0.9423748389254938\n\n\nNote: the compute_effsize() function is able to compute various effect sizes, but we’re specifying Cohen’s d here.\nSo, the Cohen’s d value for these data are d = 0.94 .\nWe can now actually answer our question and see how many fish we really need to catch in the future:\n\npg.power_ttest(d = 0.94,\n               alpha = 0.05,\n               power = 0.80,\n               contrast = \"two-samples\",\n               alternative = \"two-sided\")\n\n18.776177961938313\n\n\n\n\n\nFrom this we can see that any future experiments would really only need to use 19 fish for each group (we always round this number up, so no fish will be harmed during the experiment…) if we wanted to be confident of detecting the difference we observed in the previous study.\nThis approach can also be used when the pilot study showed a smaller effect size that wasn’t observed to be significant (indeed arguably, a pilot study shouldn’t really concern itself with significance but should only really be used as a way of assessing potential effect sizes which can then be used in a follow-up study).",
    "crumbs": [
      "Statistical power",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Power analysis</span>"
    ]
  },
  {
    "objectID": "materials/cs6_practical_power-analysis.html#linear-model-power-calculations",
    "href": "materials/cs6_practical_power-analysis.html#linear-model-power-calculations",
    "title": "21  Power analysis",
    "section": "21.5 Linear model power calculations",
    "text": "21.5 Linear model power calculations\nThankfully the ideas we’ve covered in the t-test section should see us in good stead going forward and I can stop writing everything out in such excruciating detail (I do have other things to do you know…).\nLet’s read in data/CS2-lobsters.csv. This data set was used in an earlier practical and describes the effect of three different food sources on lobster weight .\nAs a quick reminder we’ll also plot the data and perform an ANOVA:\n\nRPython\n\n\n\n# read in the data\nlobsters &lt;- read_csv(\"data/CS2-lobsters.csv\")\n\n\n# visualise the data\nggplot(lobsters,\n       aes(x = diet, y = weight)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n# define the linear model\nlm_lobster &lt;- lm(weight ~ diet,\n                 data = lobsters)\n\n# perform ANOVA on model\nanova(lm_lobster)\n\nAnalysis of Variance Table\n\nResponse: weight\n          Df Sum Sq Mean Sq F value Pr(&gt;F)\ndiet       2 1567.2  783.61  1.6432 0.2263\nResiduals 15 7153.1  476.87               \n\n\n\n\n\n# read in the data\nlobsters_py = pd.read_csv(\"data/CS2-lobsters.csv\")\n\n\n# visualise the data\n(ggplot(lobsters_py, aes(x = \"diet\",\n                         y = \"weight\")) +\n     geom_boxplot())\n\n\n\n\n\n\n\n\n\n# create a linear model\nmodel = smf.ols(formula = \"weight ~ C(diet)\", data = lobsters_py)\n# and get the fitted parameters of the model\nlm_lobsters_py = model.fit()\n\n\n# perform the anova on the fitted model\nsm.stats.anova_lm(lm_lobsters_py)\n\n            df       sum_sq     mean_sq        F    PR(&gt;F)\nC(diet)    2.0  1567.229381  783.614690  1.64324  0.226313\nResidual  15.0  7153.075619  476.871708      NaN       NaN\n\n\n\n\n\n\nthe box plot shows us that there might well be some differences between groups\nthe ANOVA analysis though shows that there isn’t sufficient evidence to support that claim given the insignificant p-value we observe.\n\nSo the question we can ask is:\n\nIf there really is a difference between the different food sources as big as appears here, how big a sample would we need in order to be able to detect it statistically?\n\nFirst let’s calculate the observed effect size from this study.\n\nRPython\n\n\nFor linear models the effect size is called Cohen’s \\(f^2\\). We can calculate it easily by using the \\(R^2\\) value from the model fit and shoving it in the following formula:\n\\[\\begin{equation}\nf^2 = \\frac{R^2}{1-R^2}\n\\end{equation}\\]\nWe find \\(R^2\\) from the lm_lobster summary. We can either just look at the results (spoiler alert, the \\(R^2\\) is 0.1797) and add it manually or extract the value with the broom::glance() function.\nEither way, we can calculate Cohen’s \\(f^2\\):\n\n# get the effect size for ANOVA\nR2 &lt;- summary(lm_lobster) %&gt;%\n    glance() %&gt;% \n    pull(r.squared)\n\n# calculate Cohen's f2\nR2 / (1 - R2)\n\n[1] 0.2190987\n\n\nSo now we’ve got Cohen’s \\(f^2\\).\n\n\nFor linear models the effect size metric we use is called \\(\\eta^2\\), or eta-squared.\nThe eta-squared value measures the contribution of the individual model terms. This is closely linked to the \\(R^2\\) value, which measures the total amount of variation that is explained by the entire model.\nSince we only have one model term here (diet), the \\(R^2\\) and \\(\\eta^2\\) values are the same.\nWe can get the \\(R^2\\) (0.1797) value from the model as follows:\n\n# get the R2 value\nR2 = lm_lobsters_py.rsquared\n\n\n\n\n\nRPython\n\n\nThere’s one more thing that we need for the power calculation for a linear model; the degrees of freedom.\nWe have two different degrees of freedom: the numerator degrees of freedom and the denominator degrees of freedom. Here the numerator degrees of freedom is 2. This is the number that we want. It is simply the number of parameters in the model minus 1. In this model there are three parameters for the three groups, so 3 - 1 = 2 (see the math isn’t too bad). The other number is called the denominator degrees of freedom, which in this case is 15. This is actually the number we want the power analysis to calculate as it’s a proxy for the number of observations used in the model, and we’ll see how in a minute.\nThe degrees of freedom are mentioned at the bottom of the model summary:\n\nsummary(lm_lobster)\n\n\nCall:\nlm(formula = weight ~ diet, data = lobsters)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.129 -16.155  -4.279  15.195  46.720 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  114.433      8.915  12.836 1.71e-09 ***\ndietMussels   21.895     12.149   1.802   0.0916 .  \ndietPellets   14.047     13.223   1.062   0.3049    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.84 on 15 degrees of freedom\nMultiple R-squared:  0.1797,    Adjusted R-squared:  0.07035 \nF-statistic: 1.643 on 2 and 15 DF,  p-value: 0.2263\n\n\nSo, we now want to run a power analysis for this linear model, using the following information:\n\npower = 0.8\nsignificance = 0.05\neffect size = 0.219\nnumerator DF = 2\n\nWe can feed this into the pwr.f2.test() function, where we use\n\nu to represent the numerator DF value\nf2 to represent Cohen’s \\(f^2\\) effect size value\n\n\npwr.f2.test(u = 2, f2 = 0.219,\n            sig.level = 0.05, power = 0.8)\n\n\n     Multiple regression power calculation \n\n              u = 2\n              v = 44.12292\n             f2 = 0.219\n      sig.level = 0.05\n          power = 0.8\n\n\nAs before most of these numbers are just what you’ve put into the function yourself. The new number is v. This is the denominator degrees of freedom required for the analysis to have sufficient power. Thankfully this number is related to the number of observations that we should use in a straightforward manner:\n\\(number\\:of\\:observations = u + v + 1\\)\nSo in our case we would ideally have 48 observations (45 + 2 + 1, remembering to round up) in our experiment.\nThe most challenging part for using power analyses for linear models is working out what the numerator degrees of freedom should be. The easiest way of thinking about it is to say that it’s the number of parameters in your model, excluding the intercept. If you look back at how we wrote out the linear model equations, then you should be able to see how many non-zero parameters would be expected. For some of the simple cases the table below will help you, but for complex linear models you will need to write out the linear model equation and count parameters (sorry!).\n\n\n\n\n\n\n\nTest\nu\n\n\n\n\none-way ANOVA\nno. of groups - 1\n\n\nsimple linear regression\n1\n\n\ntwo-way ANOVA with interaction\nno. of groups (v1) x no. of groups (v2) - 1\n\n\ntwo-way ANOVA without interaction\nno. of groups (v1) + no. of groups (v2) - 2\n\n\nANCOVA with interaction\n2 x no. of groups – 1\n\n\nANCOVA without interaction\nno. of groups\n\n\n\n\n\nWe can use the eta-squared value in the power_anova() function from pingouin.\nIf we’re trying to figure out the sample size, we need to give it the following information:\n\neta_squared, the effect size we’re after (we saved this as R2)\nk, the number of groups (three, in our case)\npower, the statistical power we’re after, in this case 80%\nalpha, the significance threshold at which we want to detect it\n\n\npg.power_anova(eta_squared = R2, k = 3, power = 0.80, alpha = 0.05)\n\n15.701046156535305\n\n\nWhen we fill in all that information, then we find that we need 15.701 samples per group - rounding up this gives 16. This means that we need a total of \\(16 \\times 3 = 48\\) samples altogether.\n\n\n\nThere are two questions you might now ask (if you’re still following all of this that is – you’re quite possibly definitely in need of a coffee by now):\n\nhow many observations should go into each group?\n\nideally they should be equally distributed (so in this case 16 per group).\n\nwhy is this so complicated, why isn’t there just a single function that just does this, and just tells me how many observations I need?\n\nVery good question – I have no answer to that sorry – sometimes life is just hard.",
    "crumbs": [
      "Statistical power",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Power analysis</span>"
    ]
  },
  {
    "objectID": "materials/cs6_practical_power-analysis.html#r-squared-and-eta-squared",
    "href": "materials/cs6_practical_power-analysis.html#r-squared-and-eta-squared",
    "title": "21  Power analysis",
    "section": "21.6 R-squared and eta-squared",
    "text": "21.6 R-squared and eta-squared\nLike we’ve seen before in the previous sessions, the \\(R^2\\) value can give us an indication of how much of the variance is explained by our model.\nSometimes you also come across \\(\\eta^2\\). What that does is that it partitions \\(R^2\\) across the predictors. This means that \\(\\eta^2\\) represents how much variance is explained by each of the predictors. If you have multiple predictors, then you would get multiple values.\nIn the case where there is one predictor, \\(R^2 = \\eta^2\\).",
    "crumbs": [
      "Statistical power",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Power analysis</span>"
    ]
  },
  {
    "objectID": "materials/cs6_practical_power-analysis.html#exercises",
    "href": "materials/cs6_practical_power-analysis.html#exercises",
    "title": "21  Power analysis",
    "section": "21.7 Exercises",
    "text": "21.7 Exercises\n\n21.7.1 Power: one-sample\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLevel: \nPerforming a power analysis on a one-sample data set\nLoad in data/CS1-onesample.csv (this is the same data we looked at in the earlier practical containing information on fish lengths from the Guanapo river).\n\nAssume this was a pilot study and analyse the data using a one-sample t-test to see if there is any evidence that the mean length of fish differs from 20 mm.\nUse the results of this analysis to estimate the effect size.\nWork out how big a sample size would be required to detect an effect this big with power 0.8 and significance 0.05.\nHow would the sample size change if we wanted 0.9 power and significance 0.01?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n21.8 Answer\n\nRPython\n\n\nFirst, read in the data:\n\nfish_data &lt;- read_csv(\"data/CS1-onesample.csv\")\n\nLet’s run the one-sample t-test as we did before:\n\nt.test(length ~ 1,\n       mu = 20,\n       alternative = \"two.sided\",\n       data = fish_data)\n\n\n    One Sample t-test\n\ndata:  length\nt = -3.5492, df = 28, p-value = 0.001387\nalternative hypothesis: true mean is not equal to 20\n95 percent confidence interval:\n 17.31341 19.27969\nsample estimates:\nmean of x \n 18.29655 \n\n\n\n\nFirst, read in the data:\n\nfish_data_py = pd.read_csv(\"data/CS1-onesample.csv\")\n\nLet’s run the one-sample t-test as we did before:\n\npg.ttest(x = fish_data_py.length,\n         y = 20,\n         alternative = \"two-sided\").transpose()\n\n                     T-test\nT                 -3.549184\ndof                      28\nalternative       two-sided\np-val              0.001387\nCI95%        [17.31, 19.28]\ncohen-d            0.659067\nBF10                 25.071\npower               0.92855\n\n\n\n\n\nThere does appear to be a statistically significant result here; the mean length of the fish appears to be different from 20 mm.\nLet’s calculate the effect size using these data. This gives us the following output for the effect size in terms of the Cohen’s d metric.\n\nRPython\n\n\n\ncohens_d(length ~ 1,\n         mu = 20,\n         data = fish_data)\n\n# A tibble: 1 × 6\n  .y.    group1 group2     effsize     n magnitude\n* &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt; &lt;ord&gt;    \n1 length 1      null model  -0.659    29 moderate \n\n\n\n\n\npg.compute_effsize(x = fish_data_py.length,\n                   y = 20,\n                   paired = False,\n                   eftype = \"cohen\")\n\n-0.6590669150482831\n\n\n\n\n\nOur effect size is -0.66 which is a moderate effect size. This is pretty good and it means we might have been able to detect this effect with fewer samples.\n\n\n\n\n\n\nImportant\n\n\n\nAlthough the effect size here is negative, it does not matter in terms of the power calculations whether it’s negative or positive.\n\n\nSo, let’s do the power analysis to actually calculate the minimum sample size required:\n\nRPython\n\n\n\npwr.t.test(d = -0.6590669, sig.level = 0.05, power = 0.8,\n           type = \"one.sample\")\n\n\n     One-sample t test power calculation \n\n              n = 20.07483\n              d = 0.6590669\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\n\n\n\npg.power_ttest(d = -0.6590669,\n               alpha = 0.05,\n               power = 0.80,\n               contrast = \"one-sample\",\n               alternative = \"two-sided\")\n\n20.074833996884752\n\n\n\n\n\nWe would need 21 (you round up the n value) observations in our experimental protocol in order to be able to detect an effect size this big (small?) at a 5% significance level and 80% power. Let’s see what would happen if we wanted to be even more stringent and calculate this at a significance level of 1%:\n\nRPython\n\n\n\npwr.t.test(d = -0.6590669, sig.level = 0.01, power = 0.9,\n           type = \"one.sample\")\n\n\n     One-sample t test power calculation \n\n              n = 37.62974\n              d = 0.6590669\n      sig.level = 0.01\n          power = 0.9\n    alternative = two.sided\n\n\n\n\n\npg.power_ttest(d = -0.6590669,\n               alpha = 0.01,\n               power = 0.80,\n               contrast = \"one-sample\",\n               alternative = \"two-sided\")\n\n30.25402339849718\n\n\n\n\n\nThen we’d need quite a few more observations! We would need to do a bit more work if we wanted to work to this level of significance and power. Are such small differences in fish length biologically meaningful?\n\n\n\n\n\n\n\n\n\n\n\n\n21.8.1 Power: two-sample paired\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLevel: \nPower analysis on a paired two-sample data set\nLoad in data/CS1-twopaired.csv (again this is the same data that we used in an earlier practical and relates to cortisol levels measured on 20 participants in the morning and evening).\n\nfirst carry out a power analysis to work out how big of an effect size this experiment should be able to detect at a power of 0.8 and significance level of 0.05. Don’t look at the data just yet!\nNow calculate the actual observed effect size from the study.\nIf you were to repeat the study in the future, how many observations would be necessary to detect the observed effect with 80% power and significance level 0.01?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n21.9 Answer\n\nRPython\n\n\nFirst, read in the data:\n\ncortisol &lt;- read_csv(\"data/CS1-twopaired.csv\")\n\n\n\n\ncortisol_py = pd.read_csv(\"data/CS1-twopaired.csv\")\n\n\n\n\nWe have a paired data set with 20 pairs of observations, what sort of effect size could we detect at a significance level of 0.05 and power of 0.8?\n\nRPython\n\n\n\npwr.t.test(n = 20, sig.level = 0.05, power = 0.8,\n           type = \"paired\")\n\n\n     Paired t test power calculation \n\n              n = 20\n              d = 0.6604413\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n\n\n\n\n\npg.power_ttest(n = 20,\n               alpha = 0.05,\n               power = 0.80,\n               contrast = \"paired\",\n               alternative = \"two-sided\")\n\n0.660441660152974\n\n\n\n\n\nRemember that we get effect size measured in Cohen’s d metric. So here this experimental design would be able to detect a d value of 0.66, which is a medium to large effect size.\nNow let’s look at the actual data and work out what the effect size actually is.\n\nRPython\n\n\n\ncohens_d(cortisol ~ time,\n         paired = TRUE,\n         data = cortisol)\n\n# A tibble: 1 × 7\n  .y.      group1  group2  effsize    n1    n2 magnitude\n* &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;ord&gt;    \n1 cortisol evening morning   -1.16    20    20 large    \n\n\n\n\nTo do this, we need reformat our data a bit:\n\ncortisol_wide_py = pd.pivot(cortisol_py, index = \"patient_id\", columns = \"time\", values = \"cortisol\")\n\ncortisol_wide_py.head()\n\ntime        evening  morning\npatient_id                  \n1             273.2    310.6\n2              65.7    146.1\n3             256.6    297.0\n4             321.0    270.9\n5              80.3    267.5\n\n\n\npg.compute_effsize(x = cortisol_wide_py.evening,\n                   y = cortisol_wide_py.morning,\n                   paired = False,\n                   eftype = \"cohen\")\n\n-1.434358623934538\n\n\n\n\n\nThis value is a massive effect size. It’s quite likely that we actually have more participants in this study than we actually need given such a large effect. Let calculate how many individuals we would actually need:\n\nRPython\n\n\n\npwr.t.test(d = -1.159019, sig.level = 0.01, power = 0.8,\n           type = \"paired\")\n\n\n     Paired t test power calculation \n\n              n = 12.10628\n              d = 1.159019\n      sig.level = 0.01\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n\n\nSo we would have only needed 13 pairs of participants in this study given the size of effect we were trying to detect.\n\n\n\npg.power_ttest(d = -1.434359,\n               alpha = 0.01,\n               power = 0.80,\n               contrast = \"paired\",\n               alternative = \"two-sided\")\n\n9.094695334046879\n\n\nSo we would have only needed 10 pairs of participants in this study given the size of effect we were trying to detect.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n21.9.1 Mussel muscles\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLevel: \nIn this exercise we’re going to determine a required sample size, dependent on a calculated effect size. The file data/CS6-shelllength.csv contains information from a pilot study looking at whether the standardised length of the anterior adductor muscle scar in the mussel Mytilus trossulus differs across five locations around the world (well it might be of interest to someone…).\nFind the effect size from this study and perform a power calculation (at 0.8 and 0.05 significance level) to determine how many mussel muscles need to be recorded in order to be confident that an effect really exists.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n21.10 Answer\nLet’s first load in the data and have a look at them.\n\nRPython\n\n\n\nmussels &lt;- read_csv(\"data/CS6-shelllength.csv\")\n\n\nggplot(mussels,\n       aes(x = location,\n           y = length)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\nmussels_py = pd.read_csv(\"data/CS6-shelllength.csv\")\n\n\n(ggplot(mussels_py,\n        aes(x = \"location\",\n            y = \"length\")) +\n    geom_boxplot())\n\n\n\n\n\n\n\n\n\n\n\nSo we are effectively looking at a one-way ANOVA with five groups. This will be useful to know later.\nNow we fit a linear model, and perform our calculations:\n\nRPython\n\n\n\n# define the model\nlm_mussels &lt;- lm(length ~ location,\n                data = mussels)\n\n# summarise the model\nsummary(lm_mussels)\n\n\nCall:\nlm(formula = length ~ location, data = mussels)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.025400 -0.007956  0.000100  0.007000  0.031757 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         0.078012   0.004454  17.517  &lt; 2e-16 ***\nlocationNewport    -0.003213   0.006298  -0.510  0.61331    \nlocationPetersburg  0.025430   0.006519   3.901  0.00043 ***\nlocationTillamook   0.002187   0.005975   0.366  0.71656    \nlocationTvarminne   0.017687   0.006803   2.600  0.01370 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0126 on 34 degrees of freedom\nMultiple R-squared:  0.4559,    Adjusted R-squared:  0.3918 \nF-statistic: 7.121 on 4 and 34 DF,  p-value: 0.0002812\n\n\nFrom this we can see that the R-squared value is 0.4559. We can extract that as follows and then use it to calculate Cohen’s \\(f^2\\):\n\n# get the effect size for ANOVA\nR2 &lt;- summary(lm_mussels) %&gt;%\n    glance() %&gt;% \n    pull(r.squared)\n\n# calculate Cohen's f2\nf2 &lt;- R2 / (1 - R2)\n\n\nf2\n\n[1] 0.837767\n\n\nNow, our model has 5 parameters (because we have 5 groups) and so the numerator degrees of freedom will be 4 \\((5 - 1 = 4)\\). This means that we can now carry our our power analysis:\n\npwr.f2.test(u = 4, f2 = 0.837767,\n            sig.level = 0.05 , power = 0.8)\n\n\n     Multiple regression power calculation \n\n              u = 4\n              v = 14.62396\n             f2 = 0.837767\n      sig.level = 0.05\n          power = 0.8\n\n\nThis tells us that the denominator degrees of freedom should be 15 (14.62 rounded up), and this means that we would only need 20 observations in total across all five groups to detect this effect size (Remember: number of observations = numerator d.f. + denominator d.f. + 1)\n\n\n\n# create a linear model\nmodel = smf.ols(formula = \"length ~ C(location)\", data = mussels_py)\n# and get the fitted parameters of the model\nlm_mussels_py = model.fit()\n\n\n# perform the anova on the fitted model\nsm.stats.anova_lm(lm_mussels_py)\n\n               df    sum_sq   mean_sq         F    PR(&gt;F)\nC(location)   4.0  0.004520  0.001130  7.121019  0.000281\nResidual     34.0  0.005395  0.000159       NaN       NaN\n\n\nSince we only have one model term here (location), the \\(R^2\\) and \\(\\eta^2\\) values are the same.\nWe can get the \\(R^2\\) (0.4559) value from the model as follows:\n\n# get the R2 value\nR2 = lm_mussels_py.rsquared\n\nWe can now calculate the number of required samples.\nWe use the following values:\n\neta_squared = R2\nk = 5 (we have five groups)\npower = 0.80\nalpha = 0.05 (our significance threshold)\n\n\npg.power_anova(eta_squared = R2,\n               k = 5, power = 0.80, alpha = 0.05)\n\n3.9247946339568163\n\n\nThis means we need 4 samples per group, so 20 in total (\\(4 \\times 5\\)).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n21.10.1 Power and effect\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLevel: \nThe file /data/CS6-epilepsy1.csv contains information on the ages and rates of seizures of 236 patients undertaking a clinical trial.\n\nAnalyse the data using a linear model and calculate the effect size.\nIf there would be a relationship that large between age and seizure rate how big a study would be needed to observe the effect with a 90% power?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n21.11 Answer\nLet’s first load in the data and have a look at them.\n\nRPython\n\n\nLet’s load in the data:\n\nepilepsy &lt;- read_csv(\"data/CS6-epilepsy1.csv\")\n\nRows: 236 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): age, seizure\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLet’s just have a quick look at the data to see what we’re dealing with:\n\nggplot(data = epilepsy,\n       mapping = aes(x = age,\n                     y = seizure)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\nLet’s load in the data:\n\nepilepsy_py = pd.read_csv(\"data/CS6-epilepsy1.csv\")\n\nLet’s just have a quick look at the data to see what we’re dealing with:\n\n(ggplot(epilepsy_py, aes(x = \"age\",\n                         y = \"seizure\")) +\n  geom_point())\n\n\n\n\n\n\n\n\n\n\n\nSo we are effectively looking at a simple linear regression here.\nNow we fit a linear model and determine the number of samples for the observed effect size at 90% power:\n\nRPython\n\n\n\n# define the model\nlm_epilepsy &lt;- lm(seizure ~ age,\n                  data = epilepsy)\n\n# summarise the model\nsummary(lm_epilepsy)\n\n\nCall:\nlm(formula = seizure ~ age, data = epilepsy)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.77513 -0.19585 -0.04333  0.22288  1.24168 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.814935   0.124857   6.527 4.12e-10 ***\nage         -0.001990   0.004303  -0.463    0.644    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.413 on 234 degrees of freedom\nMultiple R-squared:  0.0009134, Adjusted R-squared:  -0.003356 \nF-statistic: 0.2139 on 1 and 234 DF,  p-value: 0.6441\n\n\nFrom this we get that the \\(R^2\\) value is 9.134^{-4} (which is tiny!) and we can use this to calculate Cohen’s \\(f^2\\) value using the formula in the notes:\n\nR2 &lt;- lm_epilepsy %&gt;% \n    glance() %&gt;% \n    pull(r.squared)\n\nf2 &lt;- R2 / (1 - R2)\nf2\n\n[1] 0.0009142378\n\n\nThis effect size is absolutely tiny. If we really wanted to design an experiment to pick up an effect size this small then we would expect that we’ll need 1000s of participants.\nNow, our model has 2 parameters (an intercept and a slope) and so the numerator degrees of freedom (u) will be 1 (2 - 1 = 1!). This means that we can now carry our our power analysis:\n\npwr.f2.test(u = 1, f2 = f2,\n            sig.level = 0.05 , power = 0.9)\n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 11493.01\n             f2 = 0.0009142378\n      sig.level = 0.05\n          power = 0.9\n\n\nThis tells us that the denominator degrees of freedom (v) should be 1.1494^{4} (rounded up to the nearest number), and this means that we would need 11496 participants to detect this effect size (Remember: number of observations = numerator d.f. (u) + denominator d.f. (v) + 1).\n\n\n\n# create a linear model\nmodel = smf.ols(formula = \"seizure ~ age\", data = epilepsy_py)\n# and get the fitted parameters of the model\nlm_epilepsy_py = model.fit()\n\nSince we only have one model term here (age), the \\(R^2\\) and \\(\\eta^2\\) values are the same.\nWe can get the \\(R^2\\) value from the model as follows:\n\n# get the R2 value\nR2 = lm_epilepsy_py.rsquared\n\nR2\n\n0.0009134027561267244\n\n\nThis is a tiny effect size! In order to detect this, we would need a hell of a lot of samples.\nThere is no straightforward way (that I currently know of) to calculate sample sizes in Python for a linear model with a continuous predictor. We can use an external script that is based on the pwr.f2.test() function in R - this uses a couple of different metrics to calculate the sample size (or effect size, power or significance threshold - depending on which values are given).\nThis takes the following arguments:\n\nf2, Cohen’s \\(f^2\\) which is calculated as \\(f^2 = \\frac{R^2}{1-R^2}\\)\nu, numerator degrees of freedom. This is the number of parameters in the model minus 1\nv, denominator degrees of freedom. This is the number of observations - number of parameters\nsig_level, significance level\npower, desired power of the test\n\nIt has the following dependencies, so you’ll need to load these:\n\nfrom numpy import sqrt, ceil, abs\nfrom scipy.optimize import brenth\nfrom scipy.stats import f\nfrom scipy.special import ncfdtr\n\nNext, we load the pwr_f2_test() function (here I’ve saved it in scripts/pwr_f2_test.py):\n\nexec(open('scripts/pwr_f2_test.py').read())\n\nNext, we calculated f2:\n\nf2 = R2 / (1 - R2)\n\n\npwr_f2_test(u = 1, f2 = f2, sig_level = 0.05 , power = 0.9)\n\nPower analysis results: \n u is: 1,\n v is: 11492.99883145477,\n f2 is: 0.0009142378234744412,\n sig_level is: 0.05,\n power is: 0.9,\n num_obs is: 11495\n\n\nSo we see that the number of observations we need is 11495!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n21.11.1 Study size with multiple regression\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLevel: \nWe wish to test the effectiveness of a new drug against a placebo. It is thought that the sex and age of the patients may have an effect on their response.\n\nWrite down a linear model equation that might describe the relationship between these variables including all possible two-way interactions.\nHow big a study would we need to detect a medium effect size (according to Cohen, this is 0.15) at a power of 90%, with significance level 0.05?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n21.12 Answer\nHere we have a system with a single response variable, and three predictor variables. One of them is gender, a categorical predictor with two possible levels (M, F). One of them is treatment, again a categorical predictor with two possible levels (Drug, placebo) and one of them is continuous (age). The last one could even be viewed as a categorical predictor, where each year is a category. If we would like to model age as such, then we’d have to define it as a factor. We’re not doing this here.\nA linear model with all possible two-way interactions would look something like this:\nresponse ~ treatment + gender + age + treatment:gender + treatment:age + age:gender\nIn order to do a power calculation for this set up, we’ll need four things:\n\nthe effect size. Here we’re told it’s a medium effect size according to Cohen so we can use his default values. Here the value is 0.15, see table further above. Alternatively, we could have looked this up online (which may give us different values, or values that are relevant to a specific discipline).\nThe desired power. Here we’re told it’s 90%\nThe significance level to work to. Again we’re told this is going to be 0.05.\nThe numerator degrees of freedom. This is the tricky bit. We can do this by adding up the degrees of freedom for each term separately.\n\nThe numerator degrees of freedom is best calculated by working out the degrees of freedom of each of the six terms separately and then adding these up.\nThere are three simple ideas here that you need:\n\nThe degrees of freedom for a categorical variable is just the number of groups - 1\nThe degrees of freedom for a continuous variable is always 1\nthe degrees of freedom for any interaction is simple the product of the degrees of the main effects involved in the interaction.\n\nSo this means:\n\nThe df for gender is 1 (2 groups - 1)\nThe df for treatment is 1 (2 groups -1)\nThe df for age is 1 (continuous predictor)\nThe df for gender:treatment is 1 (1 x 1)\nThe df for gender:age is 1 (1 x 1)\nThe df for age:treatment is 1 (1 x 1)\n\nRather boring that all of them were 1 to be honest. Anyway, given that the denominator degrees of freedom is just the sum of all of these, we can see that \\(u = 6\\).\nWe now have all of the information to carry out the power analysis.\n\nRPython\n\n\n\npwr.f2.test(u = 6, f2 = 0.15,\n            sig.level = 0.05, power = 0.9)\n\n\n     Multiple regression power calculation \n\n              u = 6\n              v = 115.5826\n             f2 = 0.15\n      sig.level = 0.05\n          power = 0.9\n\n\n\n\n\npwr_f2_test(u = 6, f2 = 0.15,\n            sig_level = 0.05, power = 0.9)\n\nPower analysis results: \n u is: 6,\n v is: 115.58168763404052,\n f2 is: 0.15,\n sig_level is: 0.05,\n power is: 0.9,\n num_obs is: 123\n\n\n\n\n\nWe get a denominator df of 116, which means that we would need at least 123 participants in our study (Remember: number of observations = numerator d.f. (u) + denominator d.f. (v) + 1). Given that we have four unique combinations of gender and treatment, it would be practically sensible to round this up to 124 participants so that we could have an equal number (31) in each combination of sex and treatment. It would also be sensible to aim for a similar distribution of age ranges in each group as well.",
    "crumbs": [
      "Statistical power",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Power analysis</span>"
    ]
  },
  {
    "objectID": "materials/cs6_practical_power-analysis.html#answer",
    "href": "materials/cs6_practical_power-analysis.html#answer",
    "title": "21  Power analysis",
    "section": "21.8 Answer",
    "text": "21.8 Answer\n\nRPython\n\n\nFirst, read in the data:\n\nfish_data &lt;- read_csv(\"data/CS1-onesample.csv\")\n\nLet’s run the one-sample t-test as we did before:\n\nt.test(length ~ 1,\n       mu = 20,\n       alternative = \"two.sided\",\n       data = fish_data)\n\n\n    One Sample t-test\n\ndata:  length\nt = -3.5492, df = 28, p-value = 0.001387\nalternative hypothesis: true mean is not equal to 20\n95 percent confidence interval:\n 17.31341 19.27969\nsample estimates:\nmean of x \n 18.29655 \n\n\n\n\nFirst, read in the data:\n\nfish_data_py = pd.read_csv(\"data/CS1-onesample.csv\")\n\nLet’s run the one-sample t-test as we did before:\n\npg.ttest(x = fish_data_py.length,\n         y = 20,\n         alternative = \"two-sided\").transpose()\n\n                     T-test\nT                 -3.549184\ndof                      28\nalternative       two-sided\np-val              0.001387\nCI95%        [17.31, 19.28]\ncohen-d            0.659067\nBF10                 25.071\npower               0.92855\n\n\n\n\n\nThere does appear to be a statistically significant result here; the mean length of the fish appears to be different from 20 mm.\nLet’s calculate the effect size using these data. This gives us the following output for the effect size in terms of the Cohen’s d metric.\n\nRPython\n\n\n\ncohens_d(length ~ 1,\n         mu = 20,\n         data = fish_data)\n\n# A tibble: 1 × 6\n  .y.    group1 group2     effsize     n magnitude\n* &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt; &lt;ord&gt;    \n1 length 1      null model  -0.659    29 moderate \n\n\n\n\n\npg.compute_effsize(x = fish_data_py.length,\n                   y = 20,\n                   paired = False,\n                   eftype = \"cohen\")\n\n-0.6590669150482831\n\n\n\n\n\nOur effect size is -0.66 which is a moderate effect size. This is pretty good and it means we might have been able to detect this effect with fewer samples.\n\n\n\n\n\n\nImportant\n\n\n\nAlthough the effect size here is negative, it does not matter in terms of the power calculations whether it’s negative or positive.\n\n\nSo, let’s do the power analysis to actually calculate the minimum sample size required:\n\nRPython\n\n\n\npwr.t.test(d = -0.6590669, sig.level = 0.05, power = 0.8,\n           type = \"one.sample\")\n\n\n     One-sample t test power calculation \n\n              n = 20.07483\n              d = 0.6590669\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\n\n\n\npg.power_ttest(d = -0.6590669,\n               alpha = 0.05,\n               power = 0.80,\n               contrast = \"one-sample\",\n               alternative = \"two-sided\")\n\n20.074833996884752\n\n\n\n\n\nWe would need 21 (you round up the n value) observations in our experimental protocol in order to be able to detect an effect size this big (small?) at a 5% significance level and 80% power. Let’s see what would happen if we wanted to be even more stringent and calculate this at a significance level of 1%:\n\nRPython\n\n\n\npwr.t.test(d = -0.6590669, sig.level = 0.01, power = 0.9,\n           type = \"one.sample\")\n\n\n     One-sample t test power calculation \n\n              n = 37.62974\n              d = 0.6590669\n      sig.level = 0.01\n          power = 0.9\n    alternative = two.sided\n\n\n\n\n\npg.power_ttest(d = -0.6590669,\n               alpha = 0.01,\n               power = 0.80,\n               contrast = \"one-sample\",\n               alternative = \"two-sided\")\n\n30.25402339849718\n\n\n\n\n\nThen we’d need quite a few more observations! We would need to do a bit more work if we wanted to work to this level of significance and power. Are such small differences in fish length biologically meaningful?",
    "crumbs": [
      "Statistical power",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Power analysis</span>"
    ]
  },
  {
    "objectID": "materials/cs6_practical_power-analysis.html#answer-1",
    "href": "materials/cs6_practical_power-analysis.html#answer-1",
    "title": "21  Power analysis",
    "section": "21.9 Answer",
    "text": "21.9 Answer\n\nRPython\n\n\nFirst, read in the data:\n\ncortisol &lt;- read_csv(\"data/CS1-twopaired.csv\")\n\n\n\n\ncortisol_py = pd.read_csv(\"data/CS1-twopaired.csv\")\n\n\n\n\nWe have a paired data set with 20 pairs of observations, what sort of effect size could we detect at a significance level of 0.05 and power of 0.8?\n\nRPython\n\n\n\npwr.t.test(n = 20, sig.level = 0.05, power = 0.8,\n           type = \"paired\")\n\n\n     Paired t test power calculation \n\n              n = 20\n              d = 0.6604413\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n\n\n\n\n\npg.power_ttest(n = 20,\n               alpha = 0.05,\n               power = 0.80,\n               contrast = \"paired\",\n               alternative = \"two-sided\")\n\n0.660441660152974\n\n\n\n\n\nRemember that we get effect size measured in Cohen’s d metric. So here this experimental design would be able to detect a d value of 0.66, which is a medium to large effect size.\nNow let’s look at the actual data and work out what the effect size actually is.\n\nRPython\n\n\n\ncohens_d(cortisol ~ time,\n         paired = TRUE,\n         data = cortisol)\n\n# A tibble: 1 × 7\n  .y.      group1  group2  effsize    n1    n2 magnitude\n* &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;ord&gt;    \n1 cortisol evening morning   -1.16    20    20 large    \n\n\n\n\nTo do this, we need reformat our data a bit:\n\ncortisol_wide_py = pd.pivot(cortisol_py, index = \"patient_id\", columns = \"time\", values = \"cortisol\")\n\ncortisol_wide_py.head()\n\ntime        evening  morning\npatient_id                  \n1             273.2    310.6\n2              65.7    146.1\n3             256.6    297.0\n4             321.0    270.9\n5              80.3    267.5\n\n\n\npg.compute_effsize(x = cortisol_wide_py.evening,\n                   y = cortisol_wide_py.morning,\n                   paired = False,\n                   eftype = \"cohen\")\n\n-1.434358623934538\n\n\n\n\n\nThis value is a massive effect size. It’s quite likely that we actually have more participants in this study than we actually need given such a large effect. Let calculate how many individuals we would actually need:\n\nRPython\n\n\n\npwr.t.test(d = -1.159019, sig.level = 0.01, power = 0.8,\n           type = \"paired\")\n\n\n     Paired t test power calculation \n\n              n = 12.10628\n              d = 1.159019\n      sig.level = 0.01\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n\n\nSo we would have only needed 13 pairs of participants in this study given the size of effect we were trying to detect.\n\n\n\npg.power_ttest(d = -1.434359,\n               alpha = 0.01,\n               power = 0.80,\n               contrast = \"paired\",\n               alternative = \"two-sided\")\n\n9.094695334046879\n\n\nSo we would have only needed 10 pairs of participants in this study given the size of effect we were trying to detect.",
    "crumbs": [
      "Statistical power",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Power analysis</span>"
    ]
  },
  {
    "objectID": "materials/cs6_practical_power-analysis.html#answer-2",
    "href": "materials/cs6_practical_power-analysis.html#answer-2",
    "title": "21  Power analysis",
    "section": "21.10 Answer",
    "text": "21.10 Answer\nLet’s first load in the data and have a look at them.\n\nRPython\n\n\n\nmussels &lt;- read_csv(\"data/CS6-shelllength.csv\")\n\n\nggplot(mussels,\n       aes(x = location,\n           y = length)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\nmussels_py = pd.read_csv(\"data/CS6-shelllength.csv\")\n\n\n(ggplot(mussels_py,\n        aes(x = \"location\",\n            y = \"length\")) +\n    geom_boxplot())\n\n\n\n\n\n\n\n\n\n\n\nSo we are effectively looking at a one-way ANOVA with five groups. This will be useful to know later.\nNow we fit a linear model, and perform our calculations:\n\nRPython\n\n\n\n# define the model\nlm_mussels &lt;- lm(length ~ location,\n                data = mussels)\n\n# summarise the model\nsummary(lm_mussels)\n\n\nCall:\nlm(formula = length ~ location, data = mussels)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.025400 -0.007956  0.000100  0.007000  0.031757 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         0.078012   0.004454  17.517  &lt; 2e-16 ***\nlocationNewport    -0.003213   0.006298  -0.510  0.61331    \nlocationPetersburg  0.025430   0.006519   3.901  0.00043 ***\nlocationTillamook   0.002187   0.005975   0.366  0.71656    \nlocationTvarminne   0.017687   0.006803   2.600  0.01370 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0126 on 34 degrees of freedom\nMultiple R-squared:  0.4559,    Adjusted R-squared:  0.3918 \nF-statistic: 7.121 on 4 and 34 DF,  p-value: 0.0002812\n\n\nFrom this we can see that the R-squared value is 0.4559. We can extract that as follows and then use it to calculate Cohen’s \\(f^2\\):\n\n# get the effect size for ANOVA\nR2 &lt;- summary(lm_mussels) %&gt;%\n    glance() %&gt;% \n    pull(r.squared)\n\n# calculate Cohen's f2\nf2 &lt;- R2 / (1 - R2)\n\n\nf2\n\n[1] 0.837767\n\n\nNow, our model has 5 parameters (because we have 5 groups) and so the numerator degrees of freedom will be 4 \\((5 - 1 = 4)\\). This means that we can now carry our our power analysis:\n\npwr.f2.test(u = 4, f2 = 0.837767,\n            sig.level = 0.05 , power = 0.8)\n\n\n     Multiple regression power calculation \n\n              u = 4\n              v = 14.62396\n             f2 = 0.837767\n      sig.level = 0.05\n          power = 0.8\n\n\nThis tells us that the denominator degrees of freedom should be 15 (14.62 rounded up), and this means that we would only need 20 observations in total across all five groups to detect this effect size (Remember: number of observations = numerator d.f. + denominator d.f. + 1)\n\n\n\n# create a linear model\nmodel = smf.ols(formula = \"length ~ C(location)\", data = mussels_py)\n# and get the fitted parameters of the model\nlm_mussels_py = model.fit()\n\n\n# perform the anova on the fitted model\nsm.stats.anova_lm(lm_mussels_py)\n\n               df    sum_sq   mean_sq         F    PR(&gt;F)\nC(location)   4.0  0.004520  0.001130  7.121019  0.000281\nResidual     34.0  0.005395  0.000159       NaN       NaN\n\n\nSince we only have one model term here (location), the \\(R^2\\) and \\(\\eta^2\\) values are the same.\nWe can get the \\(R^2\\) (0.4559) value from the model as follows:\n\n# get the R2 value\nR2 = lm_mussels_py.rsquared\n\nWe can now calculate the number of required samples.\nWe use the following values:\n\neta_squared = R2\nk = 5 (we have five groups)\npower = 0.80\nalpha = 0.05 (our significance threshold)\n\n\npg.power_anova(eta_squared = R2,\n               k = 5, power = 0.80, alpha = 0.05)\n\n3.9247946339568163\n\n\nThis means we need 4 samples per group, so 20 in total (\\(4 \\times 5\\)).",
    "crumbs": [
      "Statistical power",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Power analysis</span>"
    ]
  },
  {
    "objectID": "materials/cs6_practical_power-analysis.html#answer-3",
    "href": "materials/cs6_practical_power-analysis.html#answer-3",
    "title": "21  Power analysis",
    "section": "21.11 Answer",
    "text": "21.11 Answer\nLet’s first load in the data and have a look at them.\n\nRPython\n\n\nLet’s load in the data:\n\nepilepsy &lt;- read_csv(\"data/CS6-epilepsy1.csv\")\n\nRows: 236 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): age, seizure\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLet’s just have a quick look at the data to see what we’re dealing with:\n\nggplot(data = epilepsy,\n       mapping = aes(x = age,\n                     y = seizure)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\nLet’s load in the data:\n\nepilepsy_py = pd.read_csv(\"data/CS6-epilepsy1.csv\")\n\nLet’s just have a quick look at the data to see what we’re dealing with:\n\n(ggplot(epilepsy_py, aes(x = \"age\",\n                         y = \"seizure\")) +\n  geom_point())\n\n\n\n\n\n\n\n\n\n\n\nSo we are effectively looking at a simple linear regression here.\nNow we fit a linear model and determine the number of samples for the observed effect size at 90% power:\n\nRPython\n\n\n\n# define the model\nlm_epilepsy &lt;- lm(seizure ~ age,\n                  data = epilepsy)\n\n# summarise the model\nsummary(lm_epilepsy)\n\n\nCall:\nlm(formula = seizure ~ age, data = epilepsy)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.77513 -0.19585 -0.04333  0.22288  1.24168 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.814935   0.124857   6.527 4.12e-10 ***\nage         -0.001990   0.004303  -0.463    0.644    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.413 on 234 degrees of freedom\nMultiple R-squared:  0.0009134, Adjusted R-squared:  -0.003356 \nF-statistic: 0.2139 on 1 and 234 DF,  p-value: 0.6441\n\n\nFrom this we get that the \\(R^2\\) value is 9.134^{-4} (which is tiny!) and we can use this to calculate Cohen’s \\(f^2\\) value using the formula in the notes:\n\nR2 &lt;- lm_epilepsy %&gt;% \n    glance() %&gt;% \n    pull(r.squared)\n\nf2 &lt;- R2 / (1 - R2)\nf2\n\n[1] 0.0009142378\n\n\nThis effect size is absolutely tiny. If we really wanted to design an experiment to pick up an effect size this small then we would expect that we’ll need 1000s of participants.\nNow, our model has 2 parameters (an intercept and a slope) and so the numerator degrees of freedom (u) will be 1 (2 - 1 = 1!). This means that we can now carry our our power analysis:\n\npwr.f2.test(u = 1, f2 = f2,\n            sig.level = 0.05 , power = 0.9)\n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 11493.01\n             f2 = 0.0009142378\n      sig.level = 0.05\n          power = 0.9\n\n\nThis tells us that the denominator degrees of freedom (v) should be 1.1494^{4} (rounded up to the nearest number), and this means that we would need 11496 participants to detect this effect size (Remember: number of observations = numerator d.f. (u) + denominator d.f. (v) + 1).\n\n\n\n# create a linear model\nmodel = smf.ols(formula = \"seizure ~ age\", data = epilepsy_py)\n# and get the fitted parameters of the model\nlm_epilepsy_py = model.fit()\n\nSince we only have one model term here (age), the \\(R^2\\) and \\(\\eta^2\\) values are the same.\nWe can get the \\(R^2\\) value from the model as follows:\n\n# get the R2 value\nR2 = lm_epilepsy_py.rsquared\n\nR2\n\n0.0009134027561267244\n\n\nThis is a tiny effect size! In order to detect this, we would need a hell of a lot of samples.\nThere is no straightforward way (that I currently know of) to calculate sample sizes in Python for a linear model with a continuous predictor. We can use an external script that is based on the pwr.f2.test() function in R - this uses a couple of different metrics to calculate the sample size (or effect size, power or significance threshold - depending on which values are given).\nThis takes the following arguments:\n\nf2, Cohen’s \\(f^2\\) which is calculated as \\(f^2 = \\frac{R^2}{1-R^2}\\)\nu, numerator degrees of freedom. This is the number of parameters in the model minus 1\nv, denominator degrees of freedom. This is the number of observations - number of parameters\nsig_level, significance level\npower, desired power of the test\n\nIt has the following dependencies, so you’ll need to load these:\n\nfrom numpy import sqrt, ceil, abs\nfrom scipy.optimize import brenth\nfrom scipy.stats import f\nfrom scipy.special import ncfdtr\n\nNext, we load the pwr_f2_test() function (here I’ve saved it in scripts/pwr_f2_test.py):\n\nexec(open('scripts/pwr_f2_test.py').read())\n\nNext, we calculated f2:\n\nf2 = R2 / (1 - R2)\n\n\npwr_f2_test(u = 1, f2 = f2, sig_level = 0.05 , power = 0.9)\n\nPower analysis results: \n u is: 1,\n v is: 11492.99883145477,\n f2 is: 0.0009142378234744412,\n sig_level is: 0.05,\n power is: 0.9,\n num_obs is: 11495\n\n\nSo we see that the number of observations we need is 11495!",
    "crumbs": [
      "Statistical power",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Power analysis</span>"
    ]
  },
  {
    "objectID": "materials/cs6_practical_power-analysis.html#answer-4",
    "href": "materials/cs6_practical_power-analysis.html#answer-4",
    "title": "21  Power analysis",
    "section": "21.12 Answer",
    "text": "21.12 Answer\nHere we have a system with a single response variable, and three predictor variables. One of them is gender, a categorical predictor with two possible levels (M, F). One of them is treatment, again a categorical predictor with two possible levels (Drug, placebo) and one of them is continuous (age). The last one could even be viewed as a categorical predictor, where each year is a category. If we would like to model age as such, then we’d have to define it as a factor. We’re not doing this here.\nA linear model with all possible two-way interactions would look something like this:\nresponse ~ treatment + gender + age + treatment:gender + treatment:age + age:gender\nIn order to do a power calculation for this set up, we’ll need four things:\n\nthe effect size. Here we’re told it’s a medium effect size according to Cohen so we can use his default values. Here the value is 0.15, see table further above. Alternatively, we could have looked this up online (which may give us different values, or values that are relevant to a specific discipline).\nThe desired power. Here we’re told it’s 90%\nThe significance level to work to. Again we’re told this is going to be 0.05.\nThe numerator degrees of freedom. This is the tricky bit. We can do this by adding up the degrees of freedom for each term separately.\n\nThe numerator degrees of freedom is best calculated by working out the degrees of freedom of each of the six terms separately and then adding these up.\nThere are three simple ideas here that you need:\n\nThe degrees of freedom for a categorical variable is just the number of groups - 1\nThe degrees of freedom for a continuous variable is always 1\nthe degrees of freedom for any interaction is simple the product of the degrees of the main effects involved in the interaction.\n\nSo this means:\n\nThe df for gender is 1 (2 groups - 1)\nThe df for treatment is 1 (2 groups -1)\nThe df for age is 1 (continuous predictor)\nThe df for gender:treatment is 1 (1 x 1)\nThe df for gender:age is 1 (1 x 1)\nThe df for age:treatment is 1 (1 x 1)\n\nRather boring that all of them were 1 to be honest. Anyway, given that the denominator degrees of freedom is just the sum of all of these, we can see that \\(u = 6\\).\nWe now have all of the information to carry out the power analysis.\n\nRPython\n\n\n\npwr.f2.test(u = 6, f2 = 0.15,\n            sig.level = 0.05, power = 0.9)\n\n\n     Multiple regression power calculation \n\n              u = 6\n              v = 115.5826\n             f2 = 0.15\n      sig.level = 0.05\n          power = 0.9\n\n\n\n\n\npwr_f2_test(u = 6, f2 = 0.15,\n            sig_level = 0.05, power = 0.9)\n\nPower analysis results: \n u is: 6,\n v is: 115.58168763404052,\n f2 is: 0.15,\n sig_level is: 0.05,\n power is: 0.9,\n num_obs is: 123\n\n\n\n\n\nWe get a denominator df of 116, which means that we would need at least 123 participants in our study (Remember: number of observations = numerator d.f. (u) + denominator d.f. (v) + 1). Given that we have four unique combinations of gender and treatment, it would be practically sensible to round this up to 124 participants so that we could have an equal number (31) in each combination of sex and treatment. It would also be sensible to aim for a similar distribution of age ranges in each group as well.",
    "crumbs": [
      "Statistical power",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Power analysis</span>"
    ]
  },
  {
    "objectID": "materials/cs6_practical_power-analysis.html#summary",
    "href": "materials/cs6_practical_power-analysis.html#summary",
    "title": "21  Power analysis",
    "section": "21.13 Summary",
    "text": "21.13 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nPower is the capacity of a test to detect significant results and is affected by\n\nthe effect size\nsample size\nthe significance level\n\nPower analysis optimises the trade-off between power, significance level and the desired effect size that we would like to detect",
    "crumbs": [
      "Statistical power",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Power analysis</span>"
    ]
  },
  {
    "objectID": "materials/power-analysis.html",
    "href": "materials/power-analysis.html",
    "title": "22  Factors affecting power",
    "section": "",
    "text": "22.1 Other factors to consider\nAside from a small effect size, there are other reasons why your sample size may need to be increased, depending on your planned analysis.",
    "crumbs": [
      "Statistical power",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Factors affecting power</span>"
    ]
  },
  {
    "objectID": "materials/power-analysis.html#other-factors-to-consider",
    "href": "materials/power-analysis.html#other-factors-to-consider",
    "title": "22  Factors affecting power",
    "section": "",
    "text": "You’re expecting a high attrition or exclusion rate\nThis is likely to be outside of your control as a researcher, but is important to consider and account for when planning experiments. Attrition and exclusion are both ways in which your sample can be decreased, once you’ve started collecting data.\nExclusion refers to the deliberate choice of an experimenter to remove observations from the sample, perhaps because the participant fails to meet certain criteria, or the quality of the data is not high enough. For instance, in neuroimaging studies, participants’ data may be removed from the study if they moved too much during the functional scan and caused motion artefacts. (Crucially, these exclusions shouldn’t be related to the outcome variable - i.e., there should be no systematic bias in which observations are excluded from the sample, otherwise your dataset is no longer representative, and then we have an entirely new problem besides reduced power!)\nThe term attrition is used to refer to all other reductions in sample size that happen throughout a study, which generally aren’t due to the experimenter. This can include scenarios such as human participants choosing to drop out of a study, a number of plants in the greenhouse dying, or cell cultures failing. Attrition is a particular problem in clinical studies as they recruit from small populations of sometimes vulnerable or unwell participants; further, many clinical studies also have matched samples, which means that one participant dropping out means their match in other experimental group(s) must also be excluded, compounding the difficulty. As a general role, an attrition rate of &gt;20% is considered problematic in a clinical setting.\nThe best way to deal with attrition or exclusion in a sample is to prepare for it in advance, by building in a “buffer” to your sample size. It’s likely that you’ll already know in advance that the type of experiment you’re designing is likely to have a high attrition or exclusion rate; hopefully, you will also have some indication of roughly what that rate might be, based on similar studies conducted by you or colleagues. You should increase your desired final sample size by this amount, so that if the sample is reduced during data collection or analysis, there will still be sufficient power.\n\n\nYou’re planning to separately analyse subsets of the dataset, or make multiple comparisons\nIt’s absolutely acceptable to plan to do either of these two things in your analysis. The trick is to plan for them.\nIf you’re intending to analyse a smaller subset of the data, this means that there needs to be sufficient power within that subset in order to detect the effect(s) of interest. It’s not enough for your overall sample size to be sufficient.\nAlso - analysing multiple subsets of the data will almost always constitute making multiple comparisons; it may be one of the most common ways, in fact, that multiple comparisons are introduced to an analysis pipeline. When making multiple comparisons, it’s typically recommended that you should adjust your significance threshold (or your p-values, which is mathematically equivalent) to reduce the chance of making a type I error. Increasing your significance threshold necessarily will reduce power, however, because of MathsTM. So, when making lots of multiple comparisons, you need a larger sample to boost power.\n\n\nYou know (or suspect) that there will be lots of parameters in your model\nThis includes, broadly, two sets of scenarios: 1) you have lots of predictors of interest, and/or 2) you have lots of uncontrolled variables that can affect your outcome unpredictably, that you plan to include as covariates of no interest. The overall impact on power is the same in both cases. Regardless of whether a variable is a predictor of interest, its inclusion in the model will still decrease the degrees of freedom, as more parameters need to be estimated, and increase the model’s complexity.\nWith any luck, you’ll be able to perform model comparison and simplify your model somewhat after building it (as a reminder: we are always looking for the simplest model that does a good job of explaining the variance in the dataset, in the goodness-of-fit vs complexity trade-off). But it’s best to make sure that you account for all possible parameters when performing a priori power analyses.",
    "crumbs": [
      "Statistical power",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Factors affecting power</span>"
    ]
  },
  {
    "objectID": "materials/power-analysis.html#summary",
    "href": "materials/power-analysis.html#summary",
    "title": "22  Factors affecting power",
    "section": "22.2 Summary",
    "text": "22.2 Summary\n\n\n\n\n\n\nKey Points\n\n\n\n\nError, significance, effect size and power are all related to one another, such that you can calculate desired sample size a priori\nMultiple factors can decrease power, including making corrections for multiple comparisons and including a large number of parameters in your model\nIt’s also important to be aware of decreases in sample size from attrition and exclusion, which will reduce power",
    "crumbs": [
      "Statistical power",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Factors affecting power</span>"
    ]
  },
  {
    "objectID": "materials/piloting.html",
    "href": "materials/piloting.html",
    "title": "23  Pilot studies",
    "section": "",
    "text": "23.1 Purpose(s) of a pilot study\nThere are different reasons to run a pilot study, and they might depend on what field of research you’re in. These reasons can include (but are not necessarily limited to!):",
    "crumbs": [
      "Statistical power",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Pilot studies</span>"
    ]
  },
  {
    "objectID": "materials/piloting.html#purposes-of-a-pilot-study",
    "href": "materials/piloting.html#purposes-of-a-pilot-study",
    "title": "23  Pilot studies",
    "section": "",
    "text": "Testing and refining your protocol or methodology\nThis may include things like calibrating equipment or testing out a new behavioural task or set of stimuli. It’s also likely to involve spotting errors or problems that you would otherwise have needed to deal with during the main experiment - perhaps your instructions to participants or fellow researchers are unclear and need refining, or perhaps the lab stocks of the media you need are too low.\nNote that this does not mean that you should go into your pilot study without a protocol or plan. You should have already worked out as many of the details as possible, such as how you plan to operationalise your variables, whether there are any known factors to control, and an idea of the type of analysis you might run. The more detailed your plan is before you pilot, the easier it will be to make refinements!\n\n\nTraining researchers\nThis ties in closely with the point above. The experiment may be the first time that you or other researchers have used certain protocols or equipment before, and research involves lots of skills that take practice - whether that involves interacting with patients, learning how to safely use potentially dangerous equipment, or complicated/precise bench work.\n\n\nDemonstrating feasibility & exploring possible results\nSometimes, a pilot can serve as a low-risk way to check whether something is likely to work at all. For instance: is it possible to get high enough resolution images of this phenomenon? Is the rare patient population willing to take part in this sort of study? Will this plant species grow at all in our greenhouses? Are we likely to have a manageable exclusion or attrition rate?\nIf you’re lucky, your pilot study may also give you some idea as to the likely results that a larger study will give you; more on this in the point below. But it’s worth emphasising at this stage - this does not mean you should use a pilot study to perform hypothesis testing of your overall research question! (For instance, if you get a significant result in your pilot sample, this doesn’t mean that you don’t still need to perform the main experiment.)\n\n\nCalculating an effect size for power analysis\nGiven a pilot dataset, it’s possible to calculate the effect size (using either the difference between group means for a t-test, or from the R2 value for linear models) for use in a power analysis - even if/when the pilot sample doesn’t yield any significant results.\nIt’s worth noting, however, that doing this can be a bit contentious. Some people will tell you that estimating an effect size from a pilot study is a bad idea, because smaller pilot samples are very noisy, and our confidence interval for calculated effect size from small samples is therefore very wide. Such people will tell you that the better approach is to determmine the smallest scientifically meaningful effect size that you’re interested in detecting, and use that for your power analyses instead. This raises the question, of course, of what “scientifically meaningful” means; this is often easier to determine in clinical and drug studies, than it is in more basic scientific research. But you can use standard “small”, “medium” and “large” effect sizes to guide you on this, or alternatively, you can look at existing full-scale studies (e.g., existing publications, unpublished data from your group or collaborators) to give you an idea of what sort of effect sizes might be expected for the type of research you plan to conduct. (These course materials cover effect size in a little more detail, if you’re curious.)\nAs a compromise between these approaches, you might use the estimated effect size from a pilot study to supplement an estimate from a previous full-scale study, to determine whether your planned experiment is likely to yield similar effect sizes to comparable research.",
    "crumbs": [
      "Statistical power",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Pilot studies</span>"
    ]
  },
  {
    "objectID": "materials/piloting.html#pilot-sample",
    "href": "materials/piloting.html#pilot-sample",
    "title": "23  Pilot studies",
    "section": "23.2 Pilot sample",
    "text": "23.2 Pilot sample\nDeciding how large your pilot sample should be is often a difficult exercise - you’re trying to make a difficult trade-off between gathering information, versus not launching into a full experiment.\nIf you plug the question “how big should my pilot sample be?” into Google, you may see a couple of common rules-of-thumb: “10-20% of the final desired sample size”, “12 observations in each group”, and so on. These aren’t bad starting points, but they do assume that you’re doing a certain type of research (often, these are written for clinical researchers). There are situations where the size of your pilot sample might be guided by other factors.\nIf you’re hoping to use your pilot to get the best possible effect size estimate, then you will likely want to push your pilot to be as large as possible, to get the narrowest confidence interval for the effect size that you possibly can.\nIf, conversely, you’re using the pilot simply to test feasibility (e.g., can we image this phenomenon? Will this reaction occur under these conditions?) then you may go for the minimum sample needed to confirm feasibility, and stop once you’ve achieved that.\nIf your intention is to make iterative changes and improvements on your protocol, then you should consider very carefully whether you truly have a single pilot study, or whether it would be better described as a series of pilots. In any case, you should ensure that the final version of the experiment that you pilot, has a sufficient sample size in itself for you to be happy that the changes and revisions you’ve made have had the effect you wanted.\n\n\n\n\n\n\nAn important note about keeping your pilot sample separate\n\n\n\n\n\nIt’s quite common to see researchers including their pilot sample in the main dataset, once they conduct the larger experiment. This is an easy temptation to fall for, but it’s often best to avoid doing so.\nFirstly, your pilot study will almost certainly be imperfect in some way. If you have been refining or updating your protocol or training researchers as part of your pilot, then the data quality may be lower in the pilot dataset. Worse still, differences between the pilot and main protocol might introduce a confounding variable, if those differences have a meaningful impact on your response variable.\nFrom a purely statistical point of view, there is also the possibility of increasing error by folding your pilot sample into your main sample, particularly if you hadn’t planned to do so in advance. Analysing your pilot sample, and then re-analysing these data as part of the experimental sample, also constitutes making multiple comparisons, which can increase your chance of a type I error (false positive).\nOf course, as with a lot of what these course materials talk about, there are scenarios where these concerns might be outweighed. For instance, if you are working with a very rare population, it might not be feasible to discard any of the data you’ve collected; maintaining as large a sample as possible might be your priority. In these cases, you could choose to make adjustments in your analysis (e.g., including covariates) to account for any differences in protocol or equipment.",
    "crumbs": [
      "Statistical power",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Pilot studies</span>"
    ]
  },
  {
    "objectID": "materials/piloting.html#summary",
    "href": "materials/piloting.html#summary",
    "title": "23  Pilot studies",
    "section": "23.3 Summary",
    "text": "23.3 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nA pilot study can be considered a “trial run” for your experiment\nThe pilot can be used for testing/refining your protocol, training researchers, demonstrating feasibility and sometimes for estimating effect sizes\nThe required sample size for a pilot will depend on the purpose and field of research",
    "crumbs": [
      "Statistical power",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Pilot studies</span>"
    ]
  },
  {
    "objectID": "materials/glm-intro-lm.html",
    "href": "materials/glm-intro-lm.html",
    "title": "24  Linear models",
    "section": "",
    "text": "24.1 Data\nFor this example, we’ll be using the several data sets about Darwin’s finches. They are part of a long-term genetic and phenotypic study on the evolution of several species of finches. The exact details are less important for now, but there are data on multiple species where several phenotypic characteristics were measured (see Figure 24.1).",
    "crumbs": [
      "Extending linear models",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Linear models</span>"
    ]
  },
  {
    "objectID": "materials/glm-intro-lm.html#data",
    "href": "materials/glm-intro-lm.html#data",
    "title": "24  Linear models",
    "section": "",
    "text": "Figure 24.1: Finches phenotypes (courtesy of HHMI BioInteractive)",
    "crumbs": [
      "Extending linear models",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Linear models</span>"
    ]
  },
  {
    "objectID": "materials/glm-intro-lm.html#exploring-data",
    "href": "materials/glm-intro-lm.html#exploring-data",
    "title": "24  Linear models",
    "section": "24.2 Exploring data",
    "text": "24.2 Exploring data\nIt’s always a good idea to explore your data visually. Here we are focussing on the (potential) relationship between beak length (blength) and beak depth (bdepth).\nOur data contains measurements from two years (year) and two species (species). If we plot beak depth against beak length, colour our data by species and look across the two time points (1975 and 2012), we get the following graph:\n\n\n\n\n\n\n\n\nFigure 24.2: Beak depth and length for G. fortis and G. scandens\n\n\n\n\n\nIt seems that there is a potential linear relationship between beak depth and beak length. There are some differences between the two species and two time points with, what seems, more spread in the data in 2012. The data for both species also seem to be less separated than in 1975.\nFor the current purpose, we’ll focus on one group of data: those of Geospiza fortis in 1975.",
    "crumbs": [
      "Extending linear models",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Linear models</span>"
    ]
  },
  {
    "objectID": "materials/glm-intro-lm.html#linear-model",
    "href": "materials/glm-intro-lm.html#linear-model",
    "title": "24  Linear models",
    "section": "24.3 Linear model",
    "text": "24.3 Linear model\nLet’s look at the G. fortis data more closely, assuming that the have a linear relationship. We can visualise that as follows:\n\n\n\n\n\n\n\n\nFigure 24.3: Beak depth vs beak length G. fortis (1975)\n\n\n\n\n\nIf you recall from the Core statistics linear regression session, what we’re doing here is assuming that there is a linear relationship between the response variable (in this case bdepth) and predictor variable (here, blength).\nWe can get more information on this linear relationship by defining a linear model, which has the form of:\n\\[\nY = \\beta_0 + \\beta_1X\n\\]\nwhere \\(Y\\) is the response variable (the thing we’re interested in), \\(X\\) the predictor variable and \\(\\beta_0\\) and \\(\\beta_1\\) are model coefficients. More explicitly for our data, we get:\n\\[\nbeak\\ depth = \\beta_0 + \\beta_1 \\times beak\\ length\n\\]\nBut how do we find this model? The computer uses a method called least-squares regression. There are several steps involved in this.\n\n24.3.1 Line of best fit\nThe computer tries to find the line of best fit. This is a linear line that best describes your data. We could draw a linear line through our cloud of data points in many ways, but the least-squares method converges to a single solution, where the sum of squared residual deviations is at its smallest.\nTo understand this a bit better, it’s helpful to realise that each data point consists of a fitted value (the beak depth predicted by the model at a given beak length), combined with the error. The error is the difference between the fitted value and the data point.\nLet’s look at this for one of the observations, for example finch 473:\n\n\n\n\n\n\n\n\nFigure 24.4: Beak depth vs beak length (finch 473, 1975)\n\n\n\n\n\nObtaining the fitted value and error happens for each data point. All these residuals are then squared (to ensure that they are positive), and added together. This is the so-called sum-of-squares.\nYou can imagine that if you draw a line through the data that doesn’t fit the data all that well, the error associated with each data point increases. The sum-of-squares then also increases. Equally, the closer the data are to the line, the smaller the error. This results in a smaller sum-of-squares.\nThe linear line where the sum-of-squares is at its smallest, is called the line of best fit. This line acts as a model for our data.\nFor finch 473 we have the following values:\n\nthe observed beak depth is 9.5 mm\nthe observed beak length is 10.5 mm\nthe fitted value is 9.11 mm\nthe error is 0.39 mm\n\n\n\n24.3.2 Linear regression\nOnce we have the line of best fit, we can perform a linear regression. What we’re doing with the regression, is asking:\n\nIs the line of best fit a better predictor of our data than a horizontal line across the average value?\n\nVisually, that looks like this:\n\n\n\n\n\n\n\n\nFigure 24.5: Regression: is the slope different from zero?\n\n\n\n\n\nWhat we’re actually testing is whether the slope (\\(\\beta_1\\)) of the line of best fit is any different from zero.\nTo find the answer, we perform an ANOVA. This gives us a p-value of 1.68e-78.\nNeedless to say, this p-value is extremely small, and definitely smaller than any common significance threshold, such as \\(p &lt; 0.05\\). This suggests that beak length is a statistically significant predictor of beak depth.\nIn this case the model has an intercept (\\(\\beta_0\\)) of -0.34 and a slope (\\(\\beta_1\\)) of 0.9. We can use this to write a simple linear equation, describing our data. Remember that this takes the form of:\n\\[\nY = \\beta_0 + \\beta_1X\n\\]\nwhich in our case is\n\\[\nbeak\\ depth = \\beta_0 + \\beta_1 \\times beak\\ length\n\\]\nand gives us\n\\[\nbeak\\ depth = -0.34 + 0.90 \\times beak\\ length\n\\]\n\n\n24.3.3 Assumptions\nIn example above we just got on with things once we suspected that there was a linear relationship between beak depth and beak length. However, for the linear regression to be valid, several assumptions need to be met. If any of those assumptions are violated, we can’t trust the results. The following four assumptions need to be met, with a 5th point being a case of good scientific practice:\n\nData should be linear\nResiduals are normally distributed\nEquality of variance\nThe residuals are independent\n(no influential points)\n\nAs we did many times during the Core statistics sessions, we mainly rely on diagnostic plots to check these assumptions. For this particular model they look as follows:\n\n\n\n\n\n\n\n\nFigure 24.6: Diagnostic plots for G. fortis (1975) model\n\n\n\n\n\nThese plots look very good to me. For a recap on how to interpret these plots, see CS2: ANOVA.\nTaken together, we can see the relationship between beak depth and beak length as a linear one, described by a (linear) model that has a predicted value for each data point, and an associated error.",
    "crumbs": [
      "Extending linear models",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Linear models</span>"
    ]
  },
  {
    "objectID": "materials/glm-intro-glm.html",
    "href": "materials/glm-intro-glm.html",
    "title": "25  Generalising your model",
    "section": "",
    "text": "25.1 Putting the “G” into GLM\nIn the previous linear model example all the assumptions were met. But what if we have data where that isn’t the case? For example, what if we have data where we can’t describe the relationship between the predictor and response variables in a linear way?\nOne of the ways we can deal with this is by using a generalised linear model, also abbreviated as GLM. In a way it’s an extension of the linear model we discussed in the previous section. As with the normal linear model, the predictor variables in the model are in a linear combination, such as:\n\\[\n\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_3 + ...\n\\]\nHere, the \\(\\beta_0\\) value is the constant or intercept, whereas each subsequent \\(\\beta_i\\) is a unique regression coefficient for each \\(X_i\\) predictor variable. So far so good.\nHowever, the GLM makes the linear model more flexible in two ways:\nWe’ll introduce each of these elements below, then illustrate how they are used in practice, using different types of data.\nThe link function and different distributions are closely…err, linked. To make sense of what the link function is doing it’s useful to understand the different distributional assumptions. So we’ll start with those.",
    "crumbs": [
      "Extending linear models",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Generalising your model</span>"
    ]
  },
  {
    "objectID": "materials/glm-intro-glm.html#putting-the-g-into-glm",
    "href": "materials/glm-intro-glm.html#putting-the-g-into-glm",
    "title": "25  Generalising your model",
    "section": "",
    "text": "Important\n\n\n\n\nIn a standard linear model the linear combination (e.g. like we see above) becomes the predicted outcome value. With a GLM a transformation is specified, which turns the linear combination into the predicted outcome value. This is called a link function.\nA standard linear model assumes a continuous, normally distributed outcome, whereas with GLM the outcome can be both continuous or integer. Furthermore, the outcome does not have to be normally distributed. Indeed, the outcome can follow a different kind of distribution, such as binomial, Poisson, exponential etc.",
    "crumbs": [
      "Extending linear models",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Generalising your model</span>"
    ]
  },
  {
    "objectID": "materials/glm-intro-glm.html#distributions",
    "href": "materials/glm-intro-glm.html#distributions",
    "title": "25  Generalising your model",
    "section": "25.2 Distributions",
    "text": "25.2 Distributions\nIn the examples of a standard linear model we’ve seen that the residuals needed to be normally distributed. We’ve mainly used the Q-Q plot to assess this assumption of normality.\nBut what does “normal” actually mean? It assumes that the residuals are coming from a normal or Gaussian distribution. This distribution has a symmetrical bell-shape, where the centre is the mean, and half of the data are on either side of this.\nWe can see this in Figure 25.1. The mean of the normal distribution is indicated with the dashed blue line.\n\n\n\n\n\n\n\n\nFigure 25.1: Normal distribution\n\n\n\n\n\nWe can use the linear model we created previously, where we looked at the possible linear relationship between beak depth and beak length. This is based on measurements of G. fortis beaks in 1975.\nThe individual values of the residuals from this linear model are shown in Figure 25.1, panel B (in red), with the corresponding theoretical normal distribution in the background. We can see that the residuals follow this distribution reasonably well, which matches our conclusions from the Q-Q plot (see Figure 24.6).\nAll this means is that assuming that these residuals may come from a normal distribution isn’t such a daft suggestion after all.\nNow look at the example in Figure 25.2. This shows the classification of beak shape for a number of finches. Their beaks are either classed as blunt or pointed. Various (continuous) measurements were taken from each bird, with the beak length shown here.\n\n\n\n\n\n\n\n\nFigure 25.2: Classification in beak shape\n\n\n\n\n\nWe’ll look into this example in more detail later. For now it’s important to note that the response variable (the beak shape classification) is not continuous. Here it is a binary response (blunt or pointed). As a result, the assumptions for a regular linear model go out of the window. If we were foolish enough to fit a linear model to these data (see blue line in A), then the residuals would look rather non-normal (Figure 25.2 B).\nSo what do we do? Well, the normal distribution is not the only one there is. In Figure 25.3 there are a few examples of distributions (including the normal one).\n\n\n\n\n\n\n\n\nFigure 25.3: Different distributions\n\n\n\n\n\nDifferent distributions are useful for different types of data. For example, a logistic distribution is particularly useful in the context of binary or proportional response data. The Poisson distribution is useful when we have count data as a response.\nIn order to understand how this can help us, we need to be aware of two more concepts: linear predictors and link functions.",
    "crumbs": [
      "Extending linear models",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Generalising your model</span>"
    ]
  },
  {
    "objectID": "materials/glm-intro-glm.html#linear-predictors",
    "href": "materials/glm-intro-glm.html#linear-predictors",
    "title": "25  Generalising your model",
    "section": "25.3 Linear predictors",
    "text": "25.3 Linear predictors\nThe nice thing about linear models is that the predictors are, well, linear. Straight lines make for easy interpretation of any potential relationship between predictor and response.\nAs mentioned before, predictors are in the form of a linear combination, where each predictor variable is multiplied by a coefficient and all the terms are added together:\n\\[\n\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_3 + ...\n\\]\nFortunately, this is no different for generalised linear models! We still have a linear combination but, as we’ll see, if the relationship is not linear then we need an additional step before we can model the data in this way.\nAt this point, we have two options at our disposal (well, there are more, but let’s not muddy the waters too much).\n\n\n\n\n\n\nImportant\n\n\n\n\nTransform our data and use a normal linear model on the transformed data\nTransform the linear predictor\n\n\n\nThe first option, to transform our data, seems like a useful option and can work. It keeps things familiar (we’d still use a standard linear model) and so all is well with the world. Up to the point of interpreting the data. If we, for example, log-transform our data, how do we interpret this? After all, the predictions of the linear model are directly related to the outcome or response variable. Transforming the data is usually done so that the residuals of the linear model resemble a more normal distribution. An unwanted side-effect of this is that this also changes the ratio scale properties of the measured variables (Stevens 1946).\nThe second option would be to transform the linear predictor. This enables us to map a non-linear outcome (or response variable) to a linear model. This transformation is done using a link function.",
    "crumbs": [
      "Extending linear models",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Generalising your model</span>"
    ]
  },
  {
    "objectID": "materials/glm-intro-glm.html#link-functions",
    "href": "materials/glm-intro-glm.html#link-functions",
    "title": "25  Generalising your model",
    "section": "25.4 Link functions",
    "text": "25.4 Link functions\nSimply put: link functions connect the predictors in our model to the response variables in a linear way.\nHowever, and similar to the standard linear model, there are two parts to each model:\n\nthe coefficients for each predictor (linking each parameter to a predictor)\nthe error or random component (which specifies a probability distribution)\n\nWhich link function you use depends on your analysis. Some common link functions and corresponding distributions are (adapted from (Glen 2021)):\n\n\n\ndistribution\ndata type\nlink name\n\n\n\n\nbinomial\nbinary / proportion\nlogit\n\n\nnormal\nany real number\nidentity\n\n\npoisson\ncount data\nlog\n\n\n\nLet’s again look at the earlier example of beak shape.\n\n\n\n\n\n\n\n\nFigure 25.4: Beak shape classification\n\n\n\n\n\nWe’ve seen the data in Figure 25.4 A before, where we had information on what beak shape our observed finches had, plotted against their beak length.\nLet’s say we now want to make some predictions about what beak shape we would expect, given a certain beak length. In this scenario we’d need some way of modelling the response variable (beak shape; blunt or pointed) as a function of the predictor variable (beak length).\nThe issue we have is that the response variable is not continuous, but binary! We could fit a standard linear model to these data (blue line in Figure 25.2 A) but this is really bad practice. Why? Well, what such a linear model represents is the probability - or how likely it is - that an observed finch has a pointed beak, given a certain beak length (Figure 25.4 B).\nSimply fitting a linear line through those data suggests that it is possible to have a higher than 1 and lower-than-zero probability that a beak would be pointed! That, of course, makes no sense. So, we can’t describe these data as a linear relationship.\nInstead, we’ll use a logistic model to analyse these data. We’ll cover the practicalities of how to do this in more detail in a later chapter, but for now it’s sufficient to realise that one of the ways we could model these data could look like this:\n\n\n\n\n\n\n\n\nFigure 25.5: Logistic model for beak classification\n\n\n\n\n\nUsing this sigmoidal curve ensures that our predicted probabilities do not exceed the \\([0, 1]\\) range.\nNow, what happened behind the scenes is that the generalised linear model has taken the linear predictor and transformed it using the logit link function. This links the non-linear response variable (beak shape) to a linear model, using beak length as a predictor.\nWe’ll practice how to perform this analysis in the next section.",
    "crumbs": [
      "Extending linear models",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Generalising your model</span>"
    ]
  },
  {
    "objectID": "materials/glm-intro-glm.html#summary",
    "href": "materials/glm-intro-glm.html#summary",
    "title": "25  Generalising your model",
    "section": "25.5 Summary",
    "text": "25.5 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nGLMs allow us to map a non-linear outcome to a linear model\nThe link function determines how this occurs, transforming the linear predictor\n\n\n\n\n\n\n\nGlen, Stephanie. 2021. “Link Function.” Statistics How To: Elementary Statistics for the Rest of Us! https://www.statisticshowto.com/link-function/.\n\n\nStevens, S. S. 1946. “On the Theory of Scales of Measurement.” Science 103 (2684): 677–80. https://doi.org/10.1126/science.103.2684.677.",
    "crumbs": [
      "Extending linear models",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Generalising your model</span>"
    ]
  },
  {
    "objectID": "materials/stats-week_glm-practical-logistic-binary.html",
    "href": "materials/stats-week_glm-practical-logistic-binary.html",
    "title": "26  Binary response",
    "section": "",
    "text": "26.1 Libraries and functions\nThe example in this section uses the following data set:\ndata/finches_early.csv\nThese data come from an analysis of gene flow across two finch species (Lamichhaney et al. 2020). They are slightly adapted here for illustrative purposes.\nThe data focus on two species, Geospiza fortis and G. scandens. The original measurements are split by a uniquely timed event: a particularly strong El Niño event in 1983. This event changed the vegetation and food supply of the finches, allowing F1 hybrids of the two species to survive, whereas before 1983 they could not. The measurements are classed as early (pre-1983) and late (1983 onwards).\nHere we are looking only at the early data. We are specifically focussing on the beak shape classification, which we saw earlier in Figure 25.5.",
    "crumbs": [
      "Extending linear models",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Binary response</span>"
    ]
  },
  {
    "objectID": "materials/stats-week_glm-practical-logistic-binary.html#libraries-and-functions",
    "href": "materials/stats-week_glm-practical-logistic-binary.html#libraries-and-functions",
    "title": "26  Binary response",
    "section": "",
    "text": "Click to expand\n\n\n\n\n\n\nRPython\n\n\n\n26.1.1 Libraries\n\n\n26.1.2 Functions\n\n\n\n\n26.1.3 Libraries\n\n# A maths library\nimport math\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# Statistical models, conducting tests and statistical data exploration\nimport statsmodels.api as sm\n\n# Convenience interface for specifying models using formula strings and DataFrames\nimport statsmodels.formula.api as smf\n\n\n\n26.1.4 Functions",
    "crumbs": [
      "Extending linear models",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Binary response</span>"
    ]
  },
  {
    "objectID": "materials/stats-week_glm-practical-logistic-binary.html#load-and-visualise-the-data",
    "href": "materials/stats-week_glm-practical-logistic-binary.html#load-and-visualise-the-data",
    "title": "26  Binary response",
    "section": "26.2 Load and visualise the data",
    "text": "26.2 Load and visualise the data\nFirst we load the data, then we visualise it.\n\nRPython\n\n\n\nearly_finches &lt;- read_csv(\"data/finches_early.csv\")\n\n\n\n\nearly_finches_py = pd.read_csv(\"data/finches_early.csv\")\n\n\n\n\nLooking at the data, we can see that the pointed_beak column contains zeros and ones. These are actually yes/no classification outcomes and not numeric representations.\nWe’ll have to deal with this soon. For now, we can plot the data:\n\nRPython\n\n\n\nggplot(early_finches,\n       aes(x = factor(pointed_beak),\n          y = blength)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\nWe could just give Python the pointed_beak data directly, but then it would view the values as numeric. Which doesn’t really work, because we have two groups as such: those with a pointed beak (1), and those with a blunt one (0).\nWe can force Python to temporarily covert the data to a factor, by making the pointed_beak column an object type. We can do this directly inside the ggplot() function.\n\n(ggplot(early_finches_py,\n         aes(x = early_finches_py.pointed_beak.astype(object),\n             y = \"blength\")) +\n     geom_boxplot())\n\n\n\n\n\n\n\n\n\n\n\nIt looks as though the finches with blunt beaks generally have shorter beak lengths.\nWe can visualise that differently by plotting all the data points as a classic binary response plot:\n\nRPython\n\n\n\nggplot(early_finches,\n       aes(x = blength, y = pointed_beak)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n(ggplot(early_finches_py,\n         aes(x = \"blength\",\n             y = \"pointed_beak\")) +\n     geom_point())\n\n\n\n\n\n\n\n\n\n\n\nThis presents us with a bit of an issue. We could fit a linear regression model to these data, although we already know that this is a bad idea…\n\nRPython\n\n\n\nggplot(early_finches,\n       aes(x = blength, y = pointed_beak)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n(ggplot(early_finches_py,\n         aes(x = \"blength\",\n             y = \"pointed_beak\")) +\n     geom_point() +\n     geom_smooth(method = \"lm\",\n                 colour = \"blue\",\n                 se = False))\n\n\n\n\n\n\n\n\n\n\n\nOf course this is rubbish - we can’t have a beak classification outside the range of \\([0, 1]\\). It’s either blunt (0) or pointed (1).\nBut for the sake of exploration, let’s look at the assumptions:\n\nRPython\n\n\n\nlm_bks &lt;- lm(pointed_beak ~ blength,\n             data = early_finches)\n\nresid_panel(lm_bks,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\n\n\n\nFirst, we create a linear model:\n\n# create a linear model\nmodel = smf.ols(formula = \"pointed_beak ~ blength\",\n                data = early_finches_py)\n# and get the fitted parameters of the model\nlm_bks_py = model.fit()\n\nNext, we can create the diagnostic plots:\n\ndgplots(lm_bks_py)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThey’re pretty extremely bad.\n\nThe response is not linear (Residual Plot, binary response plot, common sense).\nThe residuals do not appear to be distributed normally (Q-Q Plot)\nThe variance is not homogeneous across the predicted values (Location-Scale Plot)\nBut - there is always a silver lining - we don’t have influential data points.",
    "crumbs": [
      "Extending linear models",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Binary response</span>"
    ]
  },
  {
    "objectID": "materials/stats-week_glm-practical-logistic-binary.html#creating-a-suitable-model",
    "href": "materials/stats-week_glm-practical-logistic-binary.html#creating-a-suitable-model",
    "title": "26  Binary response",
    "section": "26.3 Creating a suitable model",
    "text": "26.3 Creating a suitable model\nSo far we’ve established that using a simple linear model to describe a potential relationship between beak length and the probability of having a pointed beak is not a good idea. So, what can we do?\nOne of the ways we can deal with binary outcome data is by performing a logistic regression. Instead of fitting a straight line to our data, and performing a regression on that, we fit a line that has an S shape. This avoids the model making predictions outside the \\([0, 1]\\) range.\nWe described our standard linear relationship as follows:\n\\(Y = \\beta_0 + \\beta_1X\\)\nWe can now map this to our non-linear relationship via the logistic link function:\n\\(Y = \\frac{\\exp(\\beta_0 + \\beta_1X)}{1 + \\exp(\\beta_0 + \\beta_1X)}\\)\nNote that the \\(\\beta_0 + \\beta_1X\\) part is identical to the formula of a straight line.\nThe rest of the function is what makes the straight line curve into its characteristic S shape.\n\n\n\n\n\n\nEuler’s number (\\(\\exp\\)): would you like to know more?\n\n\n\n\n\nIn mathematics, \\(\\rm e\\) represents a constant of around 2.718. Another notation is \\(\\exp\\), which is often used when notations become a bit cumbersome. Here, I exclusively use the \\(\\exp\\) notation for consistency.\n\n\n\nWe can fit such an S-shaped curve to our early_finches data set, by creating a generalised linear model.\n\nRPython\n\n\nIn R we have a few options to do this, and by far the most familiar function would be glm(). Here we save the model in an object called glm_bks:\n\nglm_bks &lt;- glm(pointed_beak ~ blength,\n               family = binomial,\n               data = early_finches)\n\nThe format of this function is similar to that used by the lm() function for linear models. The important difference is that we must specify the family of error distribution to use. For logistic regression we must set the family to binomial.\nIf you forget to set the family argument, then the glm() function will perform a standard linear model fit, identical to what the lm() function would do.\n\n\n\n# create a linear model\nmodel = smf.glm(formula = \"pointed_beak ~ blength\",\n                family = sm.families.Binomial(),\n                data = early_finches_py)\n# and get the fitted parameters of the model\nglm_bks_py = model.fit()",
    "crumbs": [
      "Extending linear models",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Binary response</span>"
    ]
  },
  {
    "objectID": "materials/stats-week_glm-practical-logistic-binary.html#model-output",
    "href": "materials/stats-week_glm-practical-logistic-binary.html#model-output",
    "title": "26  Binary response",
    "section": "26.4 Model output",
    "text": "26.4 Model output\nThat’s the easy part done! The trickier part is interpreting the output. First of all, we’ll get some summary information.\n\nRPython\n\n\n\nsummary(glm_bks)\n\n\nCall:\nglm(formula = pointed_beak ~ blength, family = binomial, data = early_finches)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  -43.410     15.250  -2.847  0.00442 **\nblength        3.387      1.193   2.839  0.00452 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 84.5476  on 60  degrees of freedom\nResidual deviance:  9.1879  on 59  degrees of freedom\nAIC: 13.188\n\nNumber of Fisher Scoring iterations: 8\n\n\n\n\n\nprint(glm_bks_py.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:           pointed_beak   No. Observations:                   61\nModel:                            GLM   Df Residuals:                       59\nModel Family:                Binomial   Df Model:                            1\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -4.5939\nDate:                Tue, 16 Apr 2024   Deviance:                       9.1879\nTime:                        07:46:21   Pearson chi2:                     15.1\nNo. Iterations:                     8   Pseudo R-squ. (CS):             0.7093\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    -43.4096     15.250     -2.847      0.004     -73.298     -13.521\nblength        3.3866      1.193      2.839      0.005       1.049       5.724\n==============================================================================\n\n\n\n\n\nThere’s a lot to unpack here, but for the purpose of today we are mostly focussing on the coefficients.\n\nRPython\n\n\nThe coefficients can be found in the Coefficients block. The main numbers to extract from the output are the two numbers underneath Estimate.Std:\nCoefficients:\n            Estimate Std.\n(Intercept)  -43.410\nblength        3.387 \n\n\nRight at the bottom is a table showing the model coefficients. The main numbers to extract from the output are the two numbers in the coef column:\n======================\n                 coef\n----------------------\nIntercept    -43.4096\nblength        3.3866\n======================\n\n\n\nThese are the coefficients of the logistic model equation and need to be placed in the correct equation if we want to be able to calculate the probability of having a pointed beak for a given beak length.\nThe \\(p\\) values at the end of each coefficient row merely show whether that particular coefficient is significantly different from zero. This is similar to the \\(p\\) values obtained in the summary output of a linear model. As with continuous predictors in simple models, these \\(p\\) values can be used to decide whether that predictor is important (so in this case beak length appears to be significant). However, these \\(p\\) values aren’t great to work with when we have multiple predictor variables, or when we have categorical predictors with multiple levels (since the output will give us a \\(p\\) value for each level rather than for the predictor as a whole).\nWe can use the coefficients to calculate the probability of having a pointed beak for a given beak length:\n\\[ P(pointed \\ beak) = \\frac{\\exp(-43.41 + 3.39 \\times blength)}{1 + \\exp(-43.41 + 3.39 \\times blength)} \\]\nHaving this formula means that we can calculate the probability of having a pointed beak for any beak length. How do we work this out in practice?\n\nRPython\n\n\nWell, the probability of having a pointed beak if the beak length is large (for example 15 mm) can be calculated as follows:\n\nexp(-43.41 + 3.39 * 15) / (1 + exp(-43.41 + 3.39 * 15))\n\n[1] 0.9994131\n\n\nIf the beak length is small (for example 10 mm), the probability of having a pointed beak is extremely low:\n\nexp(-43.41 + 3.39 * 10) / (1 + exp(-43.41 + 3.39 * 10))\n\n[1] 7.410155e-05\n\n\n\n\nWell, the probability of having a pointed beak if the beak length is large (for example 15 mm) can be calculated as follows:\n\n# import the math library\nimport math\n\n\nmath.exp(-43.41 + 3.39 * 15) / (1 + math.exp(-43.41 + 3.39 * 15))\n\n0.9994130595039192\n\n\nIf the beak length is small (for example 10 mm), the probability of having a pointed beak is extremely low:\n\nmath.exp(-43.41 + 3.39 * 10) / (1 + math.exp(-43.41 + 3.39 * 10))\n\n7.410155028945912e-05\n\n\n\n\n\nWe can calculate the the probabilities for all our observed values and if we do that then we can see that the larger the beak length is, the higher the probability that a beak shape would be pointed. I’m visualising this together with the logistic curve, where the blue points are the calculated probabilities:\n\n\n\n\n\n\nCode available here\n\n\n\n\n\n\nRPython\n\n\n\nglm_bks %&gt;% \n  augment(type.predict = \"response\") %&gt;% \n  ggplot() +\n  geom_point(aes(x = blength, y = pointed_beak)) +\n  geom_line(aes(x = blength, y = .fitted),\n            linetype = \"dashed\",\n            colour = \"blue\") +\n  geom_point(aes(x = blength, y = .fitted),\n             colour = \"blue\", alpha = 0.5) +\n  labs(x = \"beak length (mm)\",\n       y = \"Probability\")\n\n\n\n\n(ggplot(early_finches_py) +\n  geom_point(aes(x = \"blength\", y = \"pointed_beak\")) +\n  geom_line(aes(x = \"blength\", y = glm_bks_py.fittedvalues),\n            linetype = \"dashed\",\n            colour = \"blue\") +\n  geom_point(aes(x = \"blength\", y = glm_bks_py.fittedvalues),\n             colour = \"blue\", alpha = 0.5) +\n  labs(x = \"beak length (mm)\",\n       y = \"Probability\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 26.1: Predicted probabilities for beak classification\n\n\n\n\n\nThe graph shows us that, based on the data that we have and the model we used to make predictions about our response variable, the probability of seeing a pointed beak increases with beak length.\nShort beaks are more closely associated with the bluntly shaped beaks, whereas long beaks are more closely associated with the pointed shape. It’s also clear that there is a range of beak lengths (around 13 mm) where the probability of getting one shape or another is much more even.\nThere is an interesting genetic story behind all this, but that will be explained in more detail in the full-day Generalised linear models course.",
    "crumbs": [
      "Extending linear models",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Binary response</span>"
    ]
  },
  {
    "objectID": "materials/stats-week_glm-practical-logistic-binary.html#exercises",
    "href": "materials/stats-week_glm-practical-logistic-binary.html#exercises",
    "title": "26  Binary response",
    "section": "26.5 Exercises",
    "text": "26.5 Exercises\n\n26.5.1 Diabetes\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLevel: \nFor this exercise we’ll be using the data from data/diabetes.csv.\nThis is a data set comprising 768 observations of three variables (one dependent and two predictor variables). This records the results of a diabetes test result as a binary variable (1 is a positive result, 0 is a negative result), along with the result of a glucose tolerance test and the diastolic blood pressure for each of 768 women. The variables are called test_result, glucose and diastolic.\nWe want to see if the glucose tolerance is a meaningful predictor for predictions on a diabetes test. To investigate this, do the following:\n\nLoad and visualise the data\nCreate a suitable model\nDetermine if there are any statistically significant predictors\nCalculate the probability of a positive diabetes test result for a glucose tolerance test value of glucose = 150\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n26.6 Answer\n\nLoad and visualise the data\nFirst we load the data, then we visualise it.\n\nRPython\n\n\n\ndiabetes &lt;- read_csv(\"data/diabetes.csv\")\n\n\n\n\ndiabetes_py = pd.read_csv(\"data/diabetes.csv\")\n\n\n\n\nLooking at the data, we can see that the test_result column contains zeros and ones. These are yes/no test result outcomes and not actually numeric representations.\nWe’ll have to deal with this soon. For now, we can plot the data, by outcome:\n\nRPython\n\n\n\nggplot(diabetes,\n       aes(x = factor(test_result),\n           y = glucose)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\nWe could just give Python the test_result data directly, but then it would view the values as numeric. Which doesn’t really work, because we have two groups as such: those with a negative diabetes test result, and those with a positive one.\nWe can force Python to temporarily covert the data to a factor, by making the test_result column an object type. We can do this directly inside the ggplot() function.\n\n(ggplot(diabetes_py,\n         aes(x = diabetes_py.test_result.astype(object),\n             y = \"glucose\")) +\n     geom_boxplot())\n\n\n\n\n\n\n\n\n\n\n\nIt looks as though the patients with a positive diabetes test have slightly higher glucose levels than those with a negative diabetes test.\nWe can visualise that differently by plotting all the data points as a classic binary response plot:\n\nRPython\n\n\n\nggplot(diabetes,\n       aes(x = glucose,\n           y = test_result)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n(ggplot(diabetes_py,\n         aes(x = \"glucose\",\n             y = \"test_result\")) +\n     geom_point())\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreate a suitable model\n\nRPython\n\n\nWe’ll use the glm() function to create a generalised linear model. Here we save the model in an object called glm_dia:\n\nglm_dia &lt;- glm(test_result ~ glucose,\n               family = binomial,\n               data = diabetes)\n\nThe format of this function is similar to that used by the lm() function for linear models. The important difference is that we must specify the family of error distribution to use. For logistic regression we must set the family to binomial.\n\n\n\n# create a linear model\nmodel = smf.glm(formula = \"test_result ~ glucose\",\n                family = sm.families.Binomial(),\n                data = diabetes_py)\n# and get the fitted parameters of the model\nglm_dia_py = model.fit()\n\n\n\n\nLet’s look at the model parameters:\n\nRPython\n\n\n\nsummary(glm_dia)\n\n\nCall:\nglm(formula = test_result ~ glucose, family = binomial, data = diabetes)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.611732   0.442289  -12.69   &lt;2e-16 ***\nglucose      0.039510   0.003398   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 936.6  on 727  degrees of freedom\nResidual deviance: 752.2  on 726  degrees of freedom\nAIC: 756.2\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\nprint(glm_dia_py.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:            test_result   No. Observations:                  728\nModel:                            GLM   Df Residuals:                      726\nModel Family:                Binomial   Df Model:                            1\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -376.10\nDate:                Tue, 16 Apr 2024   Deviance:                       752.20\nTime:                        07:46:25   Pearson chi2:                     713.\nNo. Iterations:                     4   Pseudo R-squ. (CS):             0.2238\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -5.6117      0.442    -12.688      0.000      -6.479      -4.745\nglucose        0.0395      0.003     11.628      0.000       0.033       0.046\n==============================================================================\n\n\n\n\n\nWe can see that glucose is a significant predictor for the test_result (the \\(p\\) value is much smaller than 0.05).\nKnowing this, we’re interested in the coefficients. We have an intercept of -5.61 and 0.0395 for glucose. We can use these coefficients to write a formula that describes the potential relationship between the probability of having a positive test result, dependent on the glucose tolerance level value:\n\\[ P(positive \\ test\\ result) = \\frac{\\exp(-5.61 + 0.04 \\times glucose)}{1 + \\exp(-5.61 + 0.04 \\times glucose)} \\]\n\n\nCalculating probabilities\nUsing the formula above, we can now calculate the probability of having a positive test result, for a given glucose value. If we do this for glucose = 150, we get the following:\n\nRPython\n\n\n\nexp(-5.61 + 0.04 * 150) / (1 + exp(-5.61 + 0.04 * 145))\n\n[1] 0.6685441\n\n\n\n\n\nmath.exp(-5.61 + 0.04 * 150) / (1 + math.exp(-5.61 + 0.04 * 145))\n\n0.6685441044999503\n\n\n\n\n\nThis tells us that the probability of having a positive diabetes test result, given a glucose tolerance level of 150 is around 67%.",
    "crumbs": [
      "Extending linear models",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Binary response</span>"
    ]
  },
  {
    "objectID": "materials/stats-week_glm-practical-logistic-binary.html#answer",
    "href": "materials/stats-week_glm-practical-logistic-binary.html#answer",
    "title": "26  Binary response",
    "section": "26.6 Answer",
    "text": "26.6 Answer\n\nLoad and visualise the data\nFirst we load the data, then we visualise it.\n\nRPython\n\n\n\ndiabetes &lt;- read_csv(\"data/diabetes.csv\")\n\n\n\n\ndiabetes_py = pd.read_csv(\"data/diabetes.csv\")\n\n\n\n\nLooking at the data, we can see that the test_result column contains zeros and ones. These are yes/no test result outcomes and not actually numeric representations.\nWe’ll have to deal with this soon. For now, we can plot the data, by outcome:\n\nRPython\n\n\n\nggplot(diabetes,\n       aes(x = factor(test_result),\n           y = glucose)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\nWe could just give Python the test_result data directly, but then it would view the values as numeric. Which doesn’t really work, because we have two groups as such: those with a negative diabetes test result, and those with a positive one.\nWe can force Python to temporarily covert the data to a factor, by making the test_result column an object type. We can do this directly inside the ggplot() function.\n\n(ggplot(diabetes_py,\n         aes(x = diabetes_py.test_result.astype(object),\n             y = \"glucose\")) +\n     geom_boxplot())\n\n\n\n\n\n\n\n\n\n\n\nIt looks as though the patients with a positive diabetes test have slightly higher glucose levels than those with a negative diabetes test.\nWe can visualise that differently by plotting all the data points as a classic binary response plot:\n\nRPython\n\n\n\nggplot(diabetes,\n       aes(x = glucose,\n           y = test_result)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n(ggplot(diabetes_py,\n         aes(x = \"glucose\",\n             y = \"test_result\")) +\n     geom_point())\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreate a suitable model\n\nRPython\n\n\nWe’ll use the glm() function to create a generalised linear model. Here we save the model in an object called glm_dia:\n\nglm_dia &lt;- glm(test_result ~ glucose,\n               family = binomial,\n               data = diabetes)\n\nThe format of this function is similar to that used by the lm() function for linear models. The important difference is that we must specify the family of error distribution to use. For logistic regression we must set the family to binomial.\n\n\n\n# create a linear model\nmodel = smf.glm(formula = \"test_result ~ glucose\",\n                family = sm.families.Binomial(),\n                data = diabetes_py)\n# and get the fitted parameters of the model\nglm_dia_py = model.fit()\n\n\n\n\nLet’s look at the model parameters:\n\nRPython\n\n\n\nsummary(glm_dia)\n\n\nCall:\nglm(formula = test_result ~ glucose, family = binomial, data = diabetes)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.611732   0.442289  -12.69   &lt;2e-16 ***\nglucose      0.039510   0.003398   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 936.6  on 727  degrees of freedom\nResidual deviance: 752.2  on 726  degrees of freedom\nAIC: 756.2\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\nprint(glm_dia_py.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:            test_result   No. Observations:                  728\nModel:                            GLM   Df Residuals:                      726\nModel Family:                Binomial   Df Model:                            1\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -376.10\nDate:                Tue, 16 Apr 2024   Deviance:                       752.20\nTime:                        07:46:25   Pearson chi2:                     713.\nNo. Iterations:                     4   Pseudo R-squ. (CS):             0.2238\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -5.6117      0.442    -12.688      0.000      -6.479      -4.745\nglucose        0.0395      0.003     11.628      0.000       0.033       0.046\n==============================================================================\n\n\n\n\n\nWe can see that glucose is a significant predictor for the test_result (the \\(p\\) value is much smaller than 0.05).\nKnowing this, we’re interested in the coefficients. We have an intercept of -5.61 and 0.0395 for glucose. We can use these coefficients to write a formula that describes the potential relationship between the probability of having a positive test result, dependent on the glucose tolerance level value:\n\\[ P(positive \\ test\\ result) = \\frac{\\exp(-5.61 + 0.04 \\times glucose)}{1 + \\exp(-5.61 + 0.04 \\times glucose)} \\]\n\n\nCalculating probabilities\nUsing the formula above, we can now calculate the probability of having a positive test result, for a given glucose value. If we do this for glucose = 150, we get the following:\n\nRPython\n\n\n\nexp(-5.61 + 0.04 * 150) / (1 + exp(-5.61 + 0.04 * 145))\n\n[1] 0.6685441\n\n\n\n\n\nmath.exp(-5.61 + 0.04 * 150) / (1 + math.exp(-5.61 + 0.04 * 145))\n\n0.6685441044999503\n\n\n\n\n\nThis tells us that the probability of having a positive diabetes test result, given a glucose tolerance level of 150 is around 67%.",
    "crumbs": [
      "Extending linear models",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Binary response</span>"
    ]
  },
  {
    "objectID": "materials/stats-week_glm-practical-logistic-binary.html#summary",
    "href": "materials/stats-week_glm-practical-logistic-binary.html#summary",
    "title": "26  Binary response",
    "section": "26.7 Summary",
    "text": "26.7 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nWe use a logistic regression to model a binary response\nWe can feed new observations into the model and get probabilities for the outcome\n\n\n\n\n\n\n\nLamichhaney, Sangeet, Fan Han, Matthew T. Webster, B. Rosemary Grant, Peter R. Grant, and Leif Andersson. 2020. “Female-Biased Gene Flow Between Two Species of Darwin’s Finches.” Nature Ecology & Evolution 4 (7): 979–86. https://doi.org/10.1038/s41559-020-1183-9.",
    "crumbs": [
      "Extending linear models",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Binary response</span>"
    ]
  },
  {
    "objectID": "materials/mixed-effects-models.html",
    "href": "materials/mixed-effects-models.html",
    "title": "27  Mixed effects models",
    "section": "",
    "text": "27.1 Libraries and functions",
    "crumbs": [
      "Extending linear models",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Mixed effects models</span>"
    ]
  },
  {
    "objectID": "materials/mixed-effects-models.html#libraries-and-functions",
    "href": "materials/mixed-effects-models.html#libraries-and-functions",
    "title": "27  Mixed effects models",
    "section": "",
    "text": "Click to expand\n\n\n\n\n\nWe’ll be using the lme4 package in R, which is by far the most common and best choice of package for this type of model. (It’s an update of the older package nlme, which you might also see people using.) The syntax is nice and simple and extends what we’ve been doing so far with the lm() function in (hopefully!) a very intuitive way. The package also contains functions for fitting non-linear mixed effects and generalised mixed effects models - though we won’t be focusing on those here, it’s nice to know that the package can handle them in case you ever choose to explore them in future!\nFor Python users, the pymer4 package in Python allows you to “borrow” most of the functionality of R’s lme4, though it still has many bugs that make it difficult to run on any system except Linux. There is also some functionality for fitting mixed models using statsmodels in Python. We won’t be using those packages here, but you may wish to explore them if you are a die-hard Python user!\n\nR\n\n\n\n27.1.1 Libraries\n\n# you'll need this for plotting\nlibrary(tidyverse)\n\n# install and load lme4 for fitting mixed effects models\ninstall.packages(\"lme4\")\nlibrary(lme4)",
    "crumbs": [
      "Extending linear models",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Mixed effects models</span>"
    ]
  },
  {
    "objectID": "materials/mixed-effects-models.html#what-is-a-random-effect",
    "href": "materials/mixed-effects-models.html#what-is-a-random-effect",
    "title": "27  Mixed effects models",
    "section": "27.2 What is a random effect?",
    "text": "27.2 What is a random effect?\nThere are a few things that characterise a random effect:\n\nAll random effects are categorical variables or factors\nThey create clusters or groups within your dataset (i.e., non-independence)\nThe levels/groups of that factor have been chosen “at random” from a larger set of possible levels/groups - this is called exchangeability\nUsually, we are not interested in the random effect as a predictor; instead, we are trying to account for it in our analysis\nWe expect to have 5 or more distinct levels/groups to be able to treat a variable as a random effect",
    "crumbs": [
      "Extending linear models",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Mixed effects models</span>"
    ]
  },
  {
    "objectID": "materials/mixed-effects-models.html#the-sleepstudy-data",
    "href": "materials/mixed-effects-models.html#the-sleepstudy-data",
    "title": "27  Mixed effects models",
    "section": "27.3 The sleepstudy data",
    "text": "27.3 The sleepstudy data\nAs a worked example, we’re going to use the internal sleepstudy dataset from the lme4 package in R (this dataset is also provided as a .csv file, if you’d prefer to read it in or are using Python). This is a simple dataset taken from a real study that investigated the effects of sleep deprivation on reaction times in 18 subjects, and has just three variables: Reaction, reaction time in milliseconds; Days, number of days of sleep deprivation; and Subject, subject ID.\n\ndata(\"sleepstudy\")\n\nhead(sleepstudy)\n\n  Reaction Days Subject\n1 249.5600    0     308\n2 258.7047    1     308\n3 250.8006    2     308\n4 321.4398    3     308\n5 356.8519    4     308\n6 414.6901    5     308\n\n\nHave a look at the data more closely. You’ll notice that for each subject, we’ve got 10 measurements, one for each day of sleep deprivation. This repeated measurement means that our data are not independent of one another; for each subject in the study we would expect measurements of reaction times to be more similar to one another than they are to reaction times of another subject.\nLet’s start by doing something that we know is wrong, and ignoring this dependence for now. We’ll begin by visualising the data with a simple scatterplot.\n\nggplot(sleepstudy, aes(x = Days, y = Reaction)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nThis gives the overall impression that we might expect - reaction time does seem to slow as people become more sleep deprived.\nBut, as we’ve already pointed out, ignoring the fact that subjects’ own reaction times will be more similar to themselves than to another subject’s, we should make a point of accounting for this.",
    "crumbs": [
      "Extending linear models",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Mixed effects models</span>"
    ]
  },
  {
    "objectID": "materials/mixed-effects-models.html#adding-a-random-effect",
    "href": "materials/mixed-effects-models.html#adding-a-random-effect",
    "title": "27  Mixed effects models",
    "section": "27.4 Adding a random effect",
    "text": "27.4 Adding a random effect\nIn this dataset, we want to treat Subject as a random effect, which means fitting a mixed effects model. Why Subject? There are two things at play here that make us what to treat this as a random effect:\n\nSubject is a grouping variable within our dataset, and is causing us problems with independence.\nIt’s not these specific 18 subjects that we’re interested in - they instead represent 18 random selections from a broader distribution/population of subjects that we could have tested. We would like to generalise our findings to this broader population.\n\nTo fit the model, we use a different function to what we’ve used so far, but the syntax looks very similar. The difference is the addition of a new term (1|Subject), which represents our random effect.\n\n# construct a linear mixed effects model with Subject\n# as a random effect\nlme_sleep1 &lt;- lmer(Reaction ~ Days + (1|Subject),\n                   data = sleepstudy)\n\n\n# summarise the model\nsummary(lme_sleep1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Reaction ~ Days + (1 | Subject)\n   Data: sleepstudy\n\nREML criterion at convergence: 1786.5\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.2257 -0.5529  0.0109  0.5188  4.2506 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n Subject  (Intercept) 1378.2   37.12   \n Residual              960.5   30.99   \nNumber of obs: 180, groups:  Subject, 18\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 251.4051     9.7467   25.79\nDays         10.4673     0.8042   13.02\n\nCorrelation of Fixed Effects:\n     (Intr)\nDays -0.371\n\n\nOkay. The syntax might have looked similar, but the output does not.\nMixed effects models are much easier to get your head around them if you visualise - so let’s give that a go.\n\n# create a linear model - we'll use this in our graph\nlm_sleep &lt;- lm(Reaction ~ Days,\n               data = sleepstudy)\n\n# set up our basic plot\nggplot(sleepstudy, aes(x = Days, y = Reaction)) +\n  \n  # create separate plots for each subject in the sample\n  # and add the data points\n  facet_wrap(facets = vars(Subject), nrow = 3) +\n  geom_point() +\n  \n  # this adds the line of best fit for the whole sample\n  # (without the random effect), using coefficients\n  # from our simple linear model object\n  geom_line(data = cbind(sleepstudy, pred = predict(lm_sleep)),\n            aes(y = pred)) + \n  \n  # and finally, this will add different lines of best fit\n  # for each subject as calculated in our mixed model object\n  geom_line(data = cbind(sleepstudy, pred = predict(lme_sleep1)),\n            aes(y = pred), colour = \"blue\")\n\n\n\n\n\n\n\n\nEach plot represents a different subject’s data. On each plot, we’ve added the following:\n\nin black we have the same overall line of best fit from our original (incorrect) linear model.\nin blue are the individual lines of best fit for each subject. These lines move up and down the plot relative to the global line of best fit. This reflects the fact that, though all subjects are declining as they become more sleep deprived, some of them started with slower baseline reaction times, with different y-intercepts to match. Subject 310, for instance, seems to have pretty good reflexes relative to everyone else, while subject 337 isn’t quite as quick on the trigger.\n\nThe eagle-eyed among you, though, might have spotted that the gradient of each of these blue lines is still the same as the overall line of best fit. This is because we’ve added a random intercept in our model, but have kept the same slope. This reflects an underlying assumption that the relationship between sleep deprivation and reaction time is the same - i.e. that people get worse at the same rate - even if their starting baselines differ.\nWe might not think that this assumption is a good one, however. And that’s where random slopes come in.",
    "crumbs": [
      "Extending linear models",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Mixed effects models</span>"
    ]
  },
  {
    "objectID": "materials/mixed-effects-models.html#adding-random-slopes-and-random-intercepts",
    "href": "materials/mixed-effects-models.html#adding-random-slopes-and-random-intercepts",
    "title": "27  Mixed effects models",
    "section": "27.5 Adding random slopes and random intercepts",
    "text": "27.5 Adding random slopes and random intercepts\nTo add a random slope as well as a random intercept, we need to alter the syntax slightly for our random effect. Now, instead of (1|Subject), we’ll instead use (1 + Days|Subject). This allows the relationship between Days and Reaction to vary between subjects.\nLet’s fit that new model and summarise it.\n\nlme_sleep2 &lt;- lmer(Reaction ~ Days + (1 + Days|Subject),\n                   data = sleepstudy)\n\nsummary(lme_sleep2)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Reaction ~ Days + (1 + Days | Subject)\n   Data: sleepstudy\n\nREML criterion at convergence: 1743.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.9536 -0.4634  0.0231  0.4634  5.1793 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n Subject  (Intercept) 612.10   24.741       \n          Days         35.07    5.922   0.07\n Residual             654.94   25.592       \nNumber of obs: 180, groups:  Subject, 18\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  251.405      6.825  36.838\nDays          10.467      1.546   6.771\n\nCorrelation of Fixed Effects:\n     (Intr)\nDays -0.138\n\n\nWe can go ahead and add our new lines (in red) to our earlier facet plot:\n\nggplot(sleepstudy, aes(x = Days, y = Reaction)) +\n  facet_wrap(facets = vars(Subject), nrow = 3) +\n  geom_point() +\n  \n  # the global line of best fit\n  geom_line(data = cbind(sleepstudy, pred = predict(lm_sleep)),\n            aes(y = pred)) + \n  \n  # our previous lines of best fit, with random intercepts\n  # but constant slope\n  geom_line(data = cbind(sleepstudy, pred = predict(lme_sleep1)),\n            aes(y = pred), colour = \"blue\") +\n  \n  # our lines of best with random intercepts and random slopes\n  geom_line(data = cbind(sleepstudy, pred = predict(lme_sleep2)),\n            aes(y = pred), colour = \"red\") \n\n\n\n\n\n\n\n\nWhile for some of our subjects, the red, blue and black lines look quite similar, for others they diverge a fair amount. Subjects 309 and 335, for instance, are displaying a remarkably flat trend that suggests they’re not really suffering delays in reaction time from their sleep deprivation very much at all, while subject 308 definitely seems to struggle without their eight hours.\nAs an extra observation, let’s use geom_smooth to add the lines of best fit that we would see if we fitted each subject with their own individual regression:\n\nggplot(sleepstudy, aes(x = Days, y = Reaction)) +\n  facet_wrap(facets = vars(Subject), nrow = 3) +\n  geom_point() +\n  \n  # the global line of best fit\n  geom_line(data = cbind(sleepstudy, pred = predict(lm_sleep)),\n            aes(y = pred)) + \n  \n  # random intercepts only\n  geom_line(data = cbind(sleepstudy, pred = predict(lme_sleep1)),\n            aes(y = pred), colour = \"blue\") +\n  \n  # random intercepts and random slopes\n  geom_line(data = cbind(sleepstudy, pred = predict(lme_sleep2)),\n            aes(y = pred), colour = \"red\") +\n  \n  # individual regression lines for each individual\n  geom_smooth(method = \"lm\", se = FALSE,\n              colour = \"green\", linewidth = 0.5)\n\n\n\n\n\n\n\n\nHere, the black line (which is the same on every plot) represents a global line of best fit - this is what we would see using complete pooling.\nThe blue and red lines represent our mixed effects models - the difference between the two is whether we allowed the slope to vary randomly, as well as the random intercept. In both cases, we are using partial pooling.\nThe green line, meanwhile, represents what happens when we allow no pooling. In other words, we’ve fit individual regressions between Reaction and Days for each subject. Comparing this to the red lines allows us to see the phenomenon of “shrinkage”. The green lines are all slightly closer to the black line than the red line is; in other words, there’s some shrinkage towards the global line. (Subjects 330, 335 and 370 perhaps show this best.) It’s subtle, but it’s a nice demonstration of what happens when we share information between levels of a random effect.",
    "crumbs": [
      "Extending linear models",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Mixed effects models</span>"
    ]
  },
  {
    "objectID": "materials/mixed-effects-models.html#evaluating-models-and-assessing-significance",
    "href": "materials/mixed-effects-models.html#evaluating-models-and-assessing-significance",
    "title": "27  Mixed effects models",
    "section": "27.6 Evaluating models and assessing significance",
    "text": "27.6 Evaluating models and assessing significance\nYou may have noticed the lack of p-values in any of our model outputs, and my avoidance of discussing significance. There’s a very good reason for this: it’s a fair bit more complicated than for standard linear models.\nOne of the main reasons is that when you are using partial pooling, there is no way to precisely figure out how many degrees of freedom you have, like we can do for fixed effects. This matters, because you need to know the degrees of freedom in order to be able to calculate p-values from the test statistics - an F-value alone is meaningless without associated degrees of freedom!\nThis is why the authors of the lme4 package have deliberately excluded p-values from the model summaries, and also why we won’t go any further for this particular course - talking about model comparison and significance testing when we have random effects included is a topic that deserves a course all to itself.\n\n\n\n\n\n\nBut, if you’re really keen to know a bit more about significance…\n\n\n\n\n\nSo, I know I said we weren’t going to go any further on this topic, but for those who are really interested, I figured I might as well include a short summary here of some of the approaches that are taken for significance testing of mixed effects models and their parameters, as a starting point for further reading.\n\nUse approximations for the degrees of freedom, to yield estimated p-values. There is a companion package called lmerTest that allows for p-values to be calculated using two common approximations, the Satterthwaite and Kenward-Roger approximations.\nLikelihood ratio tests. LRTs involve making comparisons between models, to determine whether or not a particular parameter should be included. In other words, if you compare the model with versus without the parameter, you can see how that changes the fit; this is typically used for random effects, but sometimes for fixed effects as well. When using LRTs, however, you sometimes have to refit the model using maximum likelihood estimation instead of restricted maximum likelihood estimation (lme4 uses the latter by default) so it’s not always straightforward.\nMarkov chain Monte Carlo (MCMC) sampling, to determine the probability distribution associated with model parameters without a requirement for degrees of freedom. Unfortunately this technique cannot be used when the model contains random slopes, and therefore is not used very often.\nMake z-to-t approximations. There are Wald t-values reported as standard in the lme4 outputs; by treating them instead as z-values, you can find associated p-values. This approach relies on the fact that the z and t distributions are identical when degrees of freedom are infinite (so if the degrees of freedom are large enough, i.e., you have a lot of data, the approximation is decent).",
    "crumbs": [
      "Extending linear models",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Mixed effects models</span>"
    ]
  },
  {
    "objectID": "materials/mixed-effects-models.html#exercises",
    "href": "materials/mixed-effects-models.html#exercises",
    "title": "27  Mixed effects models",
    "section": "27.7 Exercises",
    "text": "27.7 Exercises\n\n27.7.1 Exercise 1 - Ferns\nLevel: \nA plant scientist is investigating how light intensity affects the growth rate of young fern seedlings.\nHe cultivates 240 seedlings in total in the greenhouse, split across ten trays (24 seedlings in each). Each tray receives one of three different light intensities, which can be varied by changing the settings on purpose-built growlights.\nThe height of each seedling is then measured repeatedly at five different time points (days 1, 3, 5, 7 and 9).\nWhat are our variables? What’s the relationship we’re interested in, and which of the variables (if any) should be treated as random effects?\n\n\n\nPredictor variables\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThere are four things here that vary: tray, light intensity, timepoint and height.\nWe’re interested in the relationship between growth rate and light intensity. This makes our first two predictor variables easier to decide about:\n\n\n\nFixed versus random effects\n\n\nThe variable tray is a random effect here. We are not interested in differences between these 10 particular trays that we’ve grouped our seedlings into, but we do need to recognise the non-independence created by these natural clusters - particularly because we’ve applied the “treatment” (changes in light intensity) to entire trays, rather than to individual seedlings.\nIn contrast, light intensity - though a categorical variable - is a fixed effect. We are specifically interested in comparing across the three light intensity levels, so we don’t want to share information between them; we want fixed estimates of the differences between the group means here.\nPerhaps the trickiest variable to decide about is time. Sometimes, we will want to treat time as a random effect in mixed effects models. And we have enough timepoints to satisfy the requirement for 5+ levels in this dataset.\nBut in this instance, where we are looking at growth rate, we have a good reason to believe that time is an important predictor variable, that may have an interesting interaction with light intensity.\nFurther, our particular levels of time - the specific days that we have measured - are not necessarily exchangeable, nor do we necessarily want to share information between these levels.\nIn this case, then, time would probably be best treated as a fixed rather than random effect.\nHowever, if we were not measuring a response variable that changes over time (like growth), that might change. If, for instance, we were investigating the relationship between light intensity and chlorophyll production in adult plants, then measuring across different time points would be a case of technical replication instead, and time would be best treated as a random effect. The research question is key in making this decision.\n\n\n\n\n\n27.7.2 Exercise 2 - Wolves\nLevel: \nAn ecologist is conducting a study to demonstrate how the presence of wolves in US national parks predicts the likelihood of flooding. For six different national parks across the country that contain rivers, they record the estimated wolf population, and the average number of centimetres by which the major river in the park overflows its banks, for the last 10 years - for a total of 60 observations.\nWhat’s the relationship of interest? Is our total n really 60?\n\n\n\nPredictor variables\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThough we have 60 observations, it would of course be a case of pseudoreplication if we failed to understand the clustering within these data.\nWe have four things that vary: wolf population, flood depth, national park and year.\nWith flood depth as our response variable, we already know how to treat that. And by now, you’ve hopefully got the pattern that our continuous effect of interest wolf population will always have to be a fixed effect.\n\n\n\nFixed versus random effects\n\n\nBut there’s also year and national park to contend with, and here, we likely want to treat both as random effects.\nWe have measured across several national parks, and over a 10 year period, in order to give us a large enough dataset for sufficient statistical power - these are technical replicates. But from a theoretical standpoint, the exact years and the exact parks that we’ve measured from, probably aren’t that relevant. It’s fine if we share information across these different levels.\nOf course, you might know more about ecology than me, and have a good reason to believe that the exact years do matter - that perhaps something fundamental in the relationship between flood depth ~ wolf population really does vary with year in a meaningful way. But given that our research question does not focus on change over time, both year and national park would be best treated as random effects given the information we currently have.\n\n\n\n\n\n27.7.3 Exercise 3 - Primary schools\nLevel: \nAn education researcher is interested in the impact of socio-economic status (SES) and gender on students’ primary school achievement.\nFor twelve schools across the UK, she records the following variables for each child in their final year of school (age 11):\n\nStandardised academic test scores\nWhether the child is male or female\nStandardised SES score, based on parental income, occupation and education\n\nThe response variable in this example is the standardised academic test scores, and we have two predictors: gender and SES score. Note that we have also tested these variables across three schools.\n\n\n\nPredictor variables\n\n\nWhich of these predictors should be treated as fixed versus random effects? Are there any other “hidden” grouping variables that we should consider, based on the description of the experiment?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe care about the effects of gender and SES score. We might also be interested in testing for the interaction between them, like so: academic test scores ~ SES + gender + SES:gender.\nThis helps us to determine straight away that both gender and SES score are fixed effects - we’re interested in them directly. Supporting this is the fact that we have restricted gender here to a categorical variable with only two levels, while SES score is continuous - neither of these could be treated as random effects.\nHowever, school should be treated as a random effect. We collected data from 12 different schools, but we are not particularly interested in differences between these specific schools. In fact we’d prefer to generalise our results to students across all UK primary schools, and so it makes sense to share information across the levels. But we can’t neglect school as a variable in this case, as it does create natural clusters in our dataset.\n\n\n\nFixed versus random effects\n\n\nWe also have two possible “hidden” random effects in this dataset, however.\nThe first is classroom. If the final year students are grouped into more than one class within each school, then they have been further “clustered”. Students from the same class share a teacher, and thus will be more similar to one another than to students in another class, even within the same school.\nThe classroom variable would in fact be “nested” inside the school variable - more on nested variables in later sections of this course.\nOur other possible hidden variable is family. If siblings have been included in the study, they will share an identical SES score, because this has been derived from the parent(s) rather than the students themselves. Siblings are, in this context, technical replicates! One way to deal with this is to simply remove siblings from the study; or, if there are enough sibling pairs to warrant it, we could also treat family as a random effect.",
    "crumbs": [
      "Extending linear models",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Mixed effects models</span>"
    ]
  },
  {
    "objectID": "materials/mixed-effects-models.html#summary-and-additional-resources",
    "href": "materials/mixed-effects-models.html#summary-and-additional-resources",
    "title": "27  Mixed effects models",
    "section": "27.8 Summary and additional resources",
    "text": "27.8 Summary and additional resources\nFor our purposes this week, the main takeaway from this section is to be aware of what a random effect is, and how you might identify when a mixed effects model would be appropriate for your data.\nThese sorts of data are very common in the biological sciences, though, so there might be several of you who are thinking about trying to use a mixed effects model already. I really recommend this blog post for further reading - it’s one of my favourites for introducing mixed effects models. In addition to talking about random slopes and intercepts, it also touches on how to cope with nested and crossed factors.\nHere is some more discussion that might be useful to you if you’re still wrapping your head around the fixed vs random distinction.\nA final note on some terminology: when searching for information on mixed effects models, you may also wish to use the term “multilevel” model, or even “hierarchical” model. The name “mixed model” refers specifically to the fact that there are both fixed and random effects within the same model; but you’ll also see the same sort of model referred to as “multilevel/hierarchical”, which references the fact that there are grouping variables that give the dataset a structure with different levels in it.\n\n\n\n\n\n\nKey points\n\n\n\n\nWe would fit a random effect if we have a categorical variable, with 5+ levels, that represent non-independent “clusters” or “groups” within the data\nRandom effects are estimated by sharing information across levels/groups, which are typically chosen “at random” from a larger set of exchangeable levels\n\nA model with both fixed and random effects is referred to as a mixed effects model\nThese models can be fitted in the lme4 package in R, using specialised syntax for random effects\nFor random intercepts, we use (1|B), while random intercepts with random slopes can be fitted using (1 + A|B)\nRandom effects are fitted using partial pooling, which results in the phenomenon of “shrinkage”",
    "crumbs": [
      "Extending linear models",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Mixed effects models</span>"
    ]
  },
  {
    "objectID": "materials/edsa_case-studies.html",
    "href": "materials/edsa_case-studies.html",
    "title": "28  Case studies",
    "section": "",
    "text": "28.1 Case Study 1 - Plant growth & light colour\nIt is established that both the intensity and colour of light (balance of red, green and blue light) can affect plant growth. However, you’re interested in knowing how this relationship holds specifically in common varieties of British beans.\nTo investigate this, you have access to the following resources, plus other reasonable resources:",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Case studies</span>"
    ]
  },
  {
    "objectID": "materials/edsa_case-studies.html#case-study-1---plant-growth-light-colour",
    "href": "materials/edsa_case-studies.html#case-study-1---plant-growth-light-colour",
    "title": "28  Case studies",
    "section": "",
    "text": "Seedlings (unlimited supply) from 3 different common bean varieties.\nThree greenhouses to plant seedlings, with a capacity of 150 plants each.\nGrow lights (unlimited supply) that allow you to vary the ratio of green, red and blue light as desired. They come in one of three intensities: 500, 1000 or 2500 lumens per square foot.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Case studies</span>"
    ]
  },
  {
    "objectID": "materials/edsa_case-studies.html#case-study-2---teaching-experience-in-university-lecturers",
    "href": "materials/edsa_case-studies.html#case-study-2---teaching-experience-in-university-lecturers",
    "title": "28  Case studies",
    "section": "28.2 Case Study 2 - Teaching experience in university lecturers",
    "text": "28.2 Case Study 2 - Teaching experience in university lecturers\nA university is interested in testing whether a lecturer’s number of years of teaching experience can predict the quality of their teaching.\nTo investigate this, you have access to the following, plus other reasonable information or resources:\n\nAnonymised student grades from the last 5 years for 20 lecturers at the university. Eleven of these are from humanities, and nine from life sciences. Each year, each lecturer teaches between 50-100 students.\nThe ability to distribute surveys to current students for all 20 lecturers.\nDemographic information for the lecturers, such as gender, date of birth, the year they began teaching and the number of years spent in research.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Case studies</span>"
    ]
  },
  {
    "objectID": "materials/edsa_case-studies.html#case-study-3---caffeine-and-memory-in-rats",
    "href": "materials/edsa_case-studies.html#case-study-3---caffeine-and-memory-in-rats",
    "title": "28  Case studies",
    "section": "28.3 Case Study 3 - Caffeine and memory in rats",
    "text": "28.3 Case Study 3 - Caffeine and memory in rats\nAn animal psychologist is interested in determining the effects of caffeine consumption on rats’ ability to complete memory tasks.\nTo investigate this, you have access to the following, plus other reasonable resources:\n\nUp to 50 naïve rats that have never been trained on memory tasks before.\nThe ability to give the rats controlled doses of caffeine, and/or a placebo.\nThree common tasks for memory in rats: the Morris water maze, for spatial memory; the radial arm maze, for working memory; and an object recognition task.",
    "crumbs": [
      "Case studies",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Case studies</span>"
    ]
  }
]