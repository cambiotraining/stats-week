{
  "hash": "90615a90a07c8ecff575edbecd53901e",
  "result": {
    "markdown": "---\ntitle: \"Independence & replication\"\noutput: html_document\n---\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\nOne of the key assumptions of a linear model is that all the observations or datapoints in your sample are independent of one another. This is an assumption that can be easy to forget about, because isn't something that we can check with diagnostic plots. Determining whether your data meet this assumption can also be surprisingly complicated - separating a biological replicate from a technical replicate, and figuring out the correct value of *n*, can take a bit of thought.\n\nThere are a couple of example datasets here that you can work through, which will hopefully get you thinking about independence and natural clusters or groupings within datasets. Later in the course, we'll talk about an extension to the linear model that can be useful for dealing with this sort of structure, since it's very common in the biological sciences!\n\n## Libraries and functions\n\n::: {.callout-note collapse=\"true\"}\n## Click to expand\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n### Libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n```\n:::\n\n\n## Python\n\n### Libraries\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Simple yet exhaustive stats functions.\nimport pingouin as pg\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# Statistical models, conducting tests and statistical data exploration\nimport statsmodels.api as sm\n\n# Convenience interface for specifying models using formula strings and DataFrames\nimport statsmodels.formula.api as smf\n```\n:::\n\n:::\n:::\n\n## Exercise 1 - Flower petals\n\nThis dataset contains three variables: `shade`, which refers to the degree of shading that the plant received while growing; `petals`, the number of petals recorded on an individual flower of that plant; and `plant`, the numerical ID assigned to the plant.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflowers <- read_csv(\"data/flowers.csv\")\n```\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# load the data\nflowers_py = pd.read_csv(\"data/flowers.csv\")\n\n# and have a look\nflowers_py.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  shade  petals  plant\n0  none       3      1\n1  none       3      1\n2  none       6      1\n3  none       6      1\n4  none       4      1\n```\n:::\n:::\n\n\n:::\n\nHaving read in the dataset, we can start by doing some visualisation and analysis. Let's have a look at how the petal number differs across the shade conditions, and then run a one-way ANOVA to compare the groups statistically.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# construct a boxplot, grouped by shade\n\nflowers %>%\n  ggplot(aes(x = shade, y = petals)) +\n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](replication_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nNext, we run a one-way ANOVA:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a linear model and run an ANOVA\nlm_flowers <- lm(petals ~ shade, data = flowers)\nanova(lm_flowers)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: petals\n          Df Sum Sq Mean Sq F value    Pr(>F)    \nshade      2  50.70 25.3500   13.51 1.575e-05 ***\nResiduals 57 106.95  1.8763                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# visualise using a boxplot\n(ggplot(flowers_py,\n        aes(x = \"shade\",\n            y = \"petals\")) +\n     geom_boxplot())\n```\n\n::: {.cell-output-display}\n![](replication_files/figure-html/unnamed-chunk-9-1.png){width=614}\n:::\n:::\n\n\nNext, we run a one-way ANOVA:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# construct a one-way ANOVA\npg.anova(dv = \"petals\",\n         between = \"shade\",\n         data = flowers_py,\n         detailed = True).round(5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Source      SS  DF        MS         F    p-unc     np2\n0   shade   50.70   2  25.35000  13.51052  0.00002  0.3216\n1  Within  106.95  57   1.87632       NaN      NaN     NaN\n```\n:::\n:::\n\n\n:::\n\nThe plot and one-way ANOVA are both pretty convincing. It looks as if there are most petals on flowers in full sun, and the least petals on flowers in full shade, with partial shade somewhere in the middle.\n\nHowever, you may have noticed something about this dataset - namely, that multiple measurements of the `petals` variable have been made per plant. Or, to put it another way, though we have *biological* replicates by having measured from 12 different plants, our `petals` measurements appear to be *technical* replicates.\n\nThis dataset is a prime example of **pseudoreplication**.\n\nLet's adapt this dataset, by finding the mean petal count per plant.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean_flowers <- flowers %>%\n  group_by(plant, shade) %>%\n  summarise(petals = mean(petals))\n\nmean_flowers\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 12 Ã— 3\n# Groups:   plant [12]\n   plant shade   petals\n   <dbl> <chr>    <dbl>\n 1     1 none       4.4\n 2     2 none       7.2\n 3     3 none       6.6\n 4     4 none       5.8\n 5     5 partial    5.8\n 6     6 partial    5.2\n 7     7 partial    3.4\n 8     8 partial    4.8\n 9     9 full       5  \n10    10 full       4.2\n11    11 full       3.2\n12    12 full       2.6\n```\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmean_flowers_py = flowers_py.groupby(['plant', 'shade']).mean().reset_index()\n\nmean_flowers_py\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    plant    shade  petals\n0       1     none     4.4\n1       2     none     7.2\n2       3     none     6.6\n3       4     none     5.8\n4       5  partial     5.8\n5       6  partial     5.2\n6       7  partial     3.4\n7       8  partial     4.8\n8       9     full     5.0\n9      10     full     4.2\n10     11     full     3.2\n11     12     full     2.6\n```\n:::\n:::\n\n:::\n\nNow, we have a much clearer *n* = 12. What happens if we re-run our analyses, with these mean values?\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# construct a new boxplot\nmean_flowers %>%\n  ggplot(aes(x = shade, y = petals)) +\n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](replication_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nRun a new ANOVA:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ANOVA on the mean petal counts per plant\nlm_mean <- lm(petals ~ shade, data = mean_flowers)\nanova(lm_mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: petals\n          Df Sum Sq Mean Sq F value  Pr(>F)  \nshade      2  10.14  5.0700  4.1824 0.05195 .\nResiduals  9  10.91  1.2122                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# visualise using a boxplot\n(ggplot(mean_flowers_py,\n        aes(x = \"shade\",\n            y = \"petals\")) +\n     geom_boxplot())\n```\n\n::: {.cell-output-display}\n![](replication_files/figure-html/unnamed-chunk-15-1.png){width=614}\n:::\n:::\n\n\nNext, we run a one-way ANOVA:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# construct a one-way ANOVA\npg.anova(dv = \"petals\",\n         between = \"shade\",\n         data = mean_flowers_py,\n         detailed = True).round(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Source     SS  DF     MS      F  p-unc    np2\n0   shade  10.14   2  5.070  4.182  0.052  0.482\n1  Within  10.91   9  1.212    NaN    NaN    NaN\n```\n:::\n:::\n\n:::\n\nIf anything, the resulting boxplot looks *more* convincing than it did before. However, we don't get the same picture with the ANOVA. The p-value is far larger than before, to the point where this analysis is no longer significant. The reason for this is simple - previously, we ran an analysis with a false *n* = 60, which gave enough power to detect an effect. However, using the true *n* = 12, we discover that all that statistical power was an illusion or artefact, and with just 12 plants, we can see only the beginning of a trend.\n\n## Exercise 2 - Cabbages\n\nEach row in the cabbages dataset refers to an individual cabbage, harvested by a farmer who has been trying to find the optimum levels of fertiliser in his six fields. There are four variables: response variable `weight`, the weight of individual cabbages; `N_rate`, the rate of nitrogen fertiliser applied to the field in kilograms per metre; `fertiliser`, a categorical variable describing whether the fertiliser was liquid or granular; and `field`, the ID of the field that the cabbage was harvested from.\n\nStart by reading in the dataset. It's also important that we tell R to treat the `N_rate` variable as an ordinal variable, or factor, rather than as a continuous numerical variable.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncabbages <- read_csv(\"data/cabbages.csv\")\n\n# convert the N_rate column to factor\ncabbages <- cabbages %>%\n  mutate(N_rate = as.factor(N_rate))\n```\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# load the data\ncabbages_py = pd.read_csv(\"data/cabbages.csv\")\n\n# convert the N_rate column to factor\ncabbages_py['N_rate'] = cabbages_py['N_rate'].astype('category')\n```\n:::\n\n:::\n\nThe farmer is interested in knowing whether nitrogen rate and fertiliser type affects the weight of harvested cabbages in his fields.\n\nOn the face of it, you may therefore start by fitting a linear model with these two variables as predictors (since they're both categorical, that's a two-way ANOVA):\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_cabbage <- lm(weight ~ N_rate * fertiliser, data = cabbages)\nanova(lm_cabbage)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: weight\n                  Df  Sum Sq  Mean Sq F value  Pr(>F)  \nN_rate             2 0.23306 0.116531  4.6861 0.01176 *\nfertiliser         1 0.02891 0.028910  1.1626 0.28402  \nN_rate:fertiliser  2 0.23343 0.116715  4.6935 0.01169 *\nResiduals         84 2.08885 0.024867                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# create a linear model\nmodel = smf.ols(formula = \"weight ~ N_rate * fertiliser\",\n                data = cabbages_py)\n# and get the fitted parameters of the model\nlm_cabbages_py = model.fit()\n\n# look at the model output\nsm.stats.anova_lm(lm_cabbages_py)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                     df    sum_sq   mean_sq         F    PR(>F)\nN_rate              2.0  0.233062  0.116531  4.686123  0.011765\nfertiliser          1.0  0.028910  0.028910  1.162576  0.284018\nN_rate:fertiliser   2.0  0.233429  0.116715  4.693507  0.011687\nResidual           84.0  2.088850  0.024867       NaN       NaN\n```\n:::\n:::\n\n:::\n\nThis indicates that there is a significant interaction between `N_rate` and `fertiliser`. To help us visualise the direction of that effect, we can plot the data as follows:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncabbages %>%\n  ggplot(aes(x = N_rate, y = weight, fill = fertiliser)) +\n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](replication_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# visualise using a boxplot\n(ggplot(cabbages_py,\n        aes(x = \"N_rate\",\n            y = \"weight\",\n            fill = \"fertiliser\")) +\n     geom_boxplot())\n```\n\n::: {.cell-output-display}\n![](replication_files/figure-html/unnamed-chunk-22-1.png){width=614}\n:::\n:::\n\n\n:::\n\nTogether with the ANOVA table, you might be able to make some recommendations to the farmer about the optimum fertiliser programme for his cabbages.\n\nBut - is this a sensible approach? Do we trust the conclusions?\n\nTo help you answer that question, let's visualise the effect of the `field` variable, and its relationship to other variables, with a plot:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncabbages %>%\n  ggplot(aes(x = field, y = weight,\n             colour = fertiliser, size = N_rate)) +\n  geom_point()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Using size for a discrete variable is not advised.\n```\n:::\n\n::: {.cell-output-display}\n![](replication_files/figure-html/unnamed-chunk-23-3.png){width=672}\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# visualise using a boxplot\n(ggplot(cabbages_py,\n        aes(x = \"field\", y = \"weight\",\n            colour = \"fertiliser\",\n            size = \"N_rate\")) +\n     geom_point())\n```\n\n::: {.cell-output-display}\n![](replication_files/figure-html/unnamed-chunk-24-1.png){width=614}\n:::\n:::\n\n:::\n\nThis is rudimentary, but it hopefully helps to illustrate one of two problems with the approach taken here: our different treatments/conditions in the `fertiliser` and `N_rate` variables have been applied, wholesale, to entire fields. Which makes sense, practically speaking - it's hard to see how you would do any differently - but it does mean that there are issues with treating individual cabbages as independent observations, rather than technical replicates.\n\nHave a think about how you could actually investigate this question, using the dataset presented here. What is our actual value of *n*? (Or put another way: which are our biological replicates?) What kind of model might you fit instead of the linear model fitted above?\n\n## Criteria for true independent replication\n\nConfusing our biological and technical replicates leads to pseudoreplication, as discussed above. So, how do we make sure that we truly do have biological replicates?\n\nFor a replicate to qualify as a biological, rather than technical replicate, it needs to meet three criteria for independence. These are:\n\n**1) Independent randomisation to different treatment conditions**\n\nThere should be no systematic bias in how biological replicates are allocated to conditions. This means that allocations can't be made on the basis of sample characteristics. In the first example above, the flowers weren't randomly assigned to different shade conditions - they were assigned on the basis of which plant they were growing on, meaning that they weren't independent of one another.\n\n**2) The experimental intervention must be applied independently**\n\nThis is to ensure that any technical error is random. In the second example above, if the farmer incorrectly measures the nitrogren he's adding to one of his fields, this will affect more than just a single cabbage - it will likely affect a whole group of them, if not the entire field.\n\n**3) Data points/biological replicates must not influence each other**\n\nWhether they are from the same or different conditions, biological replicates shouldn't have an affect on one another (at least not *before* you've collected the data you need!). This may involve human participants conferring about the study, or in an experiment that involves cell culture, may involve organisms competing with one another for resources and affecting the rate of growth.\n\n## Summary\n\n::: {.callout-tip}\n#### Key points\n\n- Biological replicates increase *n*, while technical replicates do not\n- The value of *n* can have a meaningful impact on the results of significance tests\n- Pseudoreplication in a sample can lead to a researcher drawing the wrong conclusions\n:::",
    "supporting": [
      "replication_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}