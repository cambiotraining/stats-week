{
  "hash": "15190e21aff1c1efed4d564c00f1adee",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Operationalising variables\"\noutput: html_document\n---\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\nThis section of the course covers how we define and measure variables, and how that can affect our analyses. This is illustrated with an example dataset. If you want to do the exercises yourself, make sure to check if you have all the required libraries installed.\n\n## Libraries and functions\n\n::: {.callout-note collapse=\"true\"}\n## Click to expand\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n### Libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n```\n:::\n\n\n## Python\n\n### Libraries\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Simple yet exhaustive stats functions.\nimport pingouin as pg\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# Statistical models, conducting tests and statistical data exploration\nimport statsmodels.api as sm\n\n# Convenience interface for specifying models using formula strings and DataFrames\nimport statsmodels.formula.api as smf\n```\n:::\n\n:::\n:::\n\n## Exercise 1 - Cycling to work\n\nFor this example, we're interested in finding out whether cycling to work increases staff members' productivity.\n\nDownload the `productivity.csv` file.\n\nThis file contains a fictional dataset that explores the relationship between cycling to work and productivity at work. Each row corresponds to a different staff member at a small Cambridge-based company. There are four variables: `cycle` is a categorical variable denoting whether the individual cycles to work; `distance` is the distance in kilometres between the individual's house and the office; `projects` is the number of projects successfully completed by the individual within the last 6 months; and `mean_hrs` is the average number of hours worked per week in the last 6 months.\n\nAs you may have noticed, we have two variables here that could serve as measures of productivity, and two ways of looking at cycling - whether someone cycles, versus how far they cycle.\n\nFirst, let's start by reading in the data, and visualising it.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load the data\nproductivity <- read_csv(\"data/productivity.csv\")\n\n# and have a look\nhead(productivity)\n```\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# load the data\nproductivity_py = pd.read_csv(\"data/productivity.csv\")\n\n# and have a look\nproductivity_py.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  cycle  distance  projects  mean_hrs\n0    no      6.68         2      55.0\n1    no      4.32         3      48.0\n2    no      5.81         2      42.0\n3    no      8.49         2      37.5\n4    no      6.47         4      32.0\n```\n\n\n:::\n:::\n\n:::\n\nNow it's time to explore this data in a bit more detail. We can gain some insight by examining our two measures of \"cycling\" (our yes/no categorical variable, and the distance between home and office) and our two measures of \"productivity\" (mean hours worked per week, and projects completed in the last 6 months). \n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# visualise using a boxplot\n\nproductivity %>%\n  ggplot(aes(x = cycle, y = distance)) +\n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](operationalisation_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nNow, we'll use a t-test to compare `distance` between those who cycle, and those who don't.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(distance ~ cycle, data = productivity)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  distance by cycle\nt = 3.4417, df = 10.517, p-value = 0.005871\nalternative hypothesis: true difference in means between group no and group yes is not equal to 0\n95 percent confidence interval:\n 1.801439 8.293561\nsample estimates:\n mean in group no mean in group yes \n           7.3700            2.3225 \n```\n\n\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# visualise using a boxplot\n(ggplot(productivity_py,\n        aes(x = \"cycle\",\n            y = \"distance\")) +\n     geom_boxplot())\n```\n\n::: {.cell-output-display}\n![](operationalisation_files/figure-html/unnamed-chunk-9-1.png){width=614}\n:::\n:::\n\n\nNext, we compare the distance between those who cycle and those who do not. We use a t-test, since there are only two groups.\n\nHere we use the `ttest()` function from the `pingouin` library. This needs two vectors as input, so we split the data as follows and then run the test:\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndist_no_cycle = productivity_py.query('cycle == \"no\"')[\"distance\"]\ndist_yes_cycle = productivity_py.query('cycle == \"yes\"')[\"distance\"]\n\npg.ttest(dist_no_cycle, dist_yes_cycle).transpose()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                  T-test\nT               3.441734\ndof            10.516733\nalternative    two-sided\np-val           0.005871\nCI95%        [1.8, 8.29]\ncohen-d         1.683946\nBF10              15.278\npower           0.960302\n```\n\n\n:::\n:::\n\n\n:::\n\nLet's look at the second set of variables: the mean hours of worked per week and the number of projects completed in the past 6 months. When visualising this, we need to consider the `projects` as a categorical variable.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# visualise the data\nproductivity %>%\n  ggplot(aes(x = as.factor(projects), y = mean_hrs)) +\n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](operationalisation_files/figure-html/unnamed-chunk-11-3.png){width=672}\n:::\n\n```{.r .cell-code}\n# construct a one-way ANOVA, treating projects as a categorical variable\n\nlm_prod <- lm(mean_hrs ~ as.factor(projects), data = productivity)\nanova(lm_prod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: mean_hrs\n                    Df  Sum Sq Mean Sq F value Pr(>F)\nas.factor(projects)  7  936.02  133.72   1.598 0.2066\nResiduals           16 1338.88   83.68               \n```\n\n\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# visualise using a boxplot\n(ggplot(productivity_py,\n        aes(x = productivity_py['projects'].astype('category'),\n            y = \"mean_hrs\")) +\n     geom_boxplot())\n```\n\n::: {.cell-output-display}\n![](operationalisation_files/figure-html/unnamed-chunk-12-1.png){width=614}\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# construct a one-way ANOVA, treating projects as a categorical variable\npg.anova(dv = \"mean_hrs\",\n         between = \"projects\",\n         data = productivity_py,\n         detailed = True).round(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Source        SS  DF       MS      F  p-unc    np2\n0  projects   936.023   7  133.718  1.598  0.207  0.411\n1    Within  1338.883  16   83.680    NaN    NaN    NaN\n```\n\n\n:::\n:::\n\n\n:::\n\nWhat does this tell you about how these two sets of variables, which (in theory at least!) tap into the same underlying construct, relate to one another? Can you spot any problems, or have you got any concerns at this stage?\n\nIf so, hold that thought.\n\n#### Assessing the effect of cycling on productivity\n\nThe next step is to run some exploratory analyses. Since we're not going to reporting these data in any kind of paper or article, and the whole point is to look at different versions of the same analysis with different variables, we won't worry about multiple comparison correction here.\n\nWhen treating `mean_hrs` as our response variable, we can use standard linear models approach, since this variable is continuous.\n\n::: {.panel-tabset group=\"language\"}\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# visualise using ggplot\n\nproductivity %>%\n  ggplot(aes(x = cycle, y = mean_hrs)) +\n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](operationalisation_files/figure-html/unnamed-chunk-14-3.png){width=672}\n:::\n\n```{.r .cell-code}\n# run a t-test to compare mean_hrs for those who cycle vs those who don't\n\nt.test(mean_hrs ~ cycle, data = productivity)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  mean_hrs by cycle\nt = -0.51454, df = 11.553, p-value = 0.6166\nalternative hypothesis: true difference in means between group no and group yes is not equal to 0\n95 percent confidence interval:\n -12.803463   7.928463\nsample estimates:\n mean in group no mean in group yes \n          36.6875           39.1250 \n```\n\n\n:::\n:::\n\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# visualise using a boxplot\n(ggplot(productivity_py,\n        aes(x = \"cycle\",\n            y = \"mean_hrs\")) +\n     geom_boxplot())\n```\n\n::: {.cell-output-display}\n![](operationalisation_files/figure-html/unnamed-chunk-15-1.png){width=614}\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# run a t-test to compare mean_hrs for those who cycle vs those who don't\nhrs_no_cycle = productivity_py.query('cycle == \"no\"')[\"mean_hrs\"]\nhrs_yes_cycle = productivity_py.query('cycle == \"yes\"')[\"mean_hrs\"]\n\npg.ttest(hrs_no_cycle, hrs_yes_cycle).transpose()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                    T-test\nT                -0.514542\ndof              11.553188\nalternative      two-sided\np-val             0.616573\nCI95%        [-12.8, 7.93]\ncohen-d            0.24139\nBF10                 0.427\npower             0.083194\n```\n\n\n:::\n:::\n\n\n:::\n\nLet's also look at `mean_hrs` vs `distance`:\n\n::: {.panel-tabset group=\"language\"}\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nproductivity %>%\n  ggplot(aes(x = distance, y = mean_hrs)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](operationalisation_files/figure-html/unnamed-chunk-17-3.png){width=672}\n:::\n\n```{.r .cell-code}\n# run a simple linear regression analysis\n\nlm_hrs1 <- lm(mean_hrs ~ distance, data = productivity)\nanova(lm_hrs1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: mean_hrs\n          Df  Sum Sq Mean Sq F value  Pr(>F)  \ndistance   1  473.28  473.28  5.7793 0.02508 *\nResiduals 22 1801.63   81.89                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# visualise using a scatterplot\n(ggplot(productivity_py,\n        aes(x = \"distance\",\n            y = \"mean_hrs\")) +\n     geom_point())\n```\n\n::: {.cell-output-display}\n![](operationalisation_files/figure-html/unnamed-chunk-18-1.png){width=614}\n:::\n:::\n\n\nWe can perform a linear regression on these data:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# create a linear model\nmodel = smf.ols(formula = \"mean_hrs ~ distance\",\n                data = productivity_py)\n# and get the fitted parameters of the model\nlm_hrs1_py = model.fit()\n\n# look at the model output\nprint(lm_hrs1_py.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               mean_hrs   R-squared:                       0.208\nModel:                            OLS   Adj. R-squared:                  0.172\nMethod:                 Least Squares   F-statistic:                     5.779\nDate:                Thu, 11 Apr 2024   Prob (F-statistic):             0.0251\nTime:                        12:59:47   Log-Likelihood:                -85.875\nNo. Observations:                  24   AIC:                             175.8\nDf Residuals:                      22   BIC:                             178.1\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     43.0833      2.711     15.891      0.000      37.461      48.706\ndistance      -1.1912      0.496     -2.404      0.025      -2.219      -0.164\n==============================================================================\nOmnibus:                        1.267   Durbin-Watson:                   1.984\nProb(Omnibus):                  0.531   Jarque-Bera (JB):                0.300\nSkew:                          -0.163   Prob(JB):                        0.861\nKurtosis:                       3.441   Cond. No.                         8.18\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n\n\n:::\n:::\n\n\n:::\n\nThis shows us that while `cycle` does not significantly predict `mean_hrs`, `distance` does. (If you had some concerns about the `distance` variable earlier, continue to hold that thought.)\n\nSo, that's the picture for `mean_hrs`, the first of our two possible outcome variables. What about the predictive relationship(s) of cycling on our other candidate outcome variable, `projects`?\n\nLet's try fitting some models with `projects` as the outcome. We'll continue to use linear models for this for now, although technically, as `projects` is what we would refer to as a count variable, we should technically be fitting a different type of model called a generalised linear model. This topic will come up later in the week, and for this dataset the two types of model lead to similar outcomes, so we won't worry about the distinction for now.\n\nFirst, we look at `distance` vs `projects`.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nproductivity %>%\n  ggplot(aes(x = distance, y = projects)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](operationalisation_files/figure-html/unnamed-chunk-20-3.png){width=672}\n:::\n\n```{.r .cell-code}\nlm_proj1 <- lm(projects ~ distance, data = productivity)\nanova(lm_proj1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: projects\n          Df Sum Sq Mean Sq F value  Pr(>F)  \ndistance   1 12.644  12.644   3.667 0.06859 .\nResiduals 22 75.856   3.448                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# visualise using a scatterplot\n(ggplot(productivity_py,\n        aes(x = \"distance\",\n            y = \"projects\")) +\n     geom_point())\n```\n\n::: {.cell-output-display}\n![](operationalisation_files/figure-html/unnamed-chunk-21-1.png){width=614}\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# create a linear model\nmodel = smf.ols(formula = \"projects ~ distance\",\n                    data = productivity_py)\n# and get the fitted parameters of the model\nlm_proj1_py = model.fit()\n\n# look at the model output\nprint(lm_proj1_py.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               projects   R-squared:                       0.143\nModel:                            OLS   Adj. R-squared:                  0.104\nMethod:                 Least Squares   F-statistic:                     3.667\nDate:                Thu, 11 Apr 2024   Prob (F-statistic):             0.0686\nTime:                        12:59:48   Log-Likelihood:                -47.864\nNo. Observations:                  24   AIC:                             99.73\nDf Residuals:                      22   BIC:                             102.1\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      4.5298      0.556      8.143      0.000       3.376       5.683\ndistance      -0.1947      0.102     -1.915      0.069      -0.406       0.016\n==============================================================================\nOmnibus:                        1.216   Durbin-Watson:                   1.491\nProb(Omnibus):                  0.544   Jarque-Bera (JB):                1.048\nSkew:                           0.317   Prob(JB):                        0.592\nKurtosis:                       2.196   Cond. No.                         8.18\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n\n\n:::\n:::\n\n:::\n\nNext, we look at `cycle` vs `projects`.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nproductivity %>%\n  ggplot(aes(x = cycle, y = projects)) +\n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](operationalisation_files/figure-html/unnamed-chunk-23-3.png){width=672}\n:::\n\n```{.r .cell-code}\nlm_proj2 <- lm(projects ~ cycle, data = productivity)\nanova(lm_proj2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: projects\n          Df Sum Sq Mean Sq F value  Pr(>F)  \ncycle      1  18.75 18.7500   5.914 0.02362 *\nResiduals 22  69.75  3.1705                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# visualise using a scatterplot\n(ggplot(productivity_py,\n        aes(x = \"cycle\",\n            y = \"projects\")) +\n     geom_boxplot())\n```\n\n::: {.cell-output-display}\n![](operationalisation_files/figure-html/unnamed-chunk-24-1.png){width=614}\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# create a generalised linear model\nmodel = smf.ols(formula = \"projects ~ cycle\",\n                    data = productivity_py)\n# and get the fitted parameters of the model\nlm_proj2_py = model.fit()\n\n# look at the model output\nprint(lm_proj2_py.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               projects   R-squared:                       0.212\nModel:                            OLS   Adj. R-squared:                  0.176\nMethod:                 Least Squares   F-statistic:                     5.914\nDate:                Thu, 11 Apr 2024   Prob (F-statistic):             0.0236\nTime:                        12:59:49   Log-Likelihood:                -46.857\nNo. Observations:                  24   AIC:                             97.71\nDf Residuals:                      22   BIC:                             100.1\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n================================================================================\n                   coef    std err          t      P>|t|      [0.025      0.975]\n--------------------------------------------------------------------------------\nIntercept        2.5000      0.630      3.971      0.001       1.194       3.806\ncycle[T.yes]     1.8750      0.771      2.432      0.024       0.276       3.474\n==============================================================================\nOmnibus:                        0.057   Durbin-Watson:                   1.674\nProb(Omnibus):                  0.972   Jarque-Bera (JB):                0.221\nSkew:                           0.096   Prob(JB):                        0.895\nKurtosis:                       2.571   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n\n\n:::\n:::\n\n:::\n\nThis shows us that `cycle` significantly predicts `projects`, meaning the number of projects that get completed is not completely random, but some of the variance in that can be explained by whether a person cycles to work, or not. In contrast, `distance` does not appear to be a significant predictor of `projects` (although it's only marginally non-significant). This is the opposite pattern, more or less, to the one we had for `mean_hrs`.\n\n#### That thought you were holding...\n\nThose of you who are discerning may have noticed that the `distance` variable is problematic as a measure of \"cycling to work\" in this particular dataset - this is because the dataset includes all the distances to work for the staff members who *don't* cycle, as well as those who do.\n\nWhat happens if we remove those values, and look at the relationship between `distance` and our response variables again?\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# use the filter function to retain only the rows where the staff member cycles\n\nproductivity_cycle <- productivity %>%\n  filter(cycle == \"yes\")\n```\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nproductivity_cycle_py = productivity_py[productivity_py[\"cycle\"] == \"yes\"]\n```\n:::\n\n:::\n\nWe'll repeat earlier visualisations and analyses, this time with the colour aesthetic helping us to visualise how the `cycle` variable affects the relationships between `distance`, `mean_hrs` and `projects`.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nproductivity %>%\n  ggplot(aes(x = distance, y = mean_hrs, colour = cycle)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](operationalisation_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n\n```{.r .cell-code}\nlm_hrs2 <- lm(mean_hrs ~ distance, data = productivity_cycle)\nanova(lm_hrs2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: mean_hrs\n          Df  Sum Sq Mean Sq F value Pr(>F)\ndistance   1  202.77 202.766  2.6188 0.1279\nResiduals 14 1083.98  77.427               \n```\n\n\n:::\n\n```{.r .cell-code}\nproductivity %>%\n  ggplot(aes(x = distance, y = projects, colour = cycle)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](operationalisation_files/figure-html/unnamed-chunk-28-2.png){width=672}\n:::\n\n```{.r .cell-code}\nlm_proj3 <- lm(projects ~ distance, data = productivity_cycle)\nsummary(lm_proj3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = projects ~ distance, data = productivity_cycle)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.2086 -1.0679 -0.1724  1.6874  3.4674 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   4.6899     0.7163   6.547  1.3e-05 ***\ndistance     -0.1356     0.2095  -0.647    0.528    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.103 on 14 degrees of freedom\nMultiple R-squared:  0.02904,\tAdjusted R-squared:  -0.04031 \nF-statistic: 0.4187 on 1 and 14 DF,  p-value: 0.528\n```\n\n\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# visualise using a scatterplot\n(ggplot(productivity_py,\n        aes(x = \"distance\",\n            y = \"mean_hrs\",\n            colour = \"cycle\")) +\n     geom_point())\n```\n\n::: {.cell-output-display}\n![](operationalisation_files/figure-html/unnamed-chunk-29-1.png){width=614}\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# create a linear model\nmodel = smf.ols(formula = \"mean_hrs ~ distance\",\n                data = productivity_cycle_py)\n# and get the fitted parameters of the model\nlm_hrs2_py = model.fit()\n\n# look at the model output\nprint(lm_hrs2_py.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               mean_hrs   R-squared:                       0.158\nModel:                            OLS   Adj. R-squared:                  0.097\nMethod:                 Least Squares   F-statistic:                     2.619\nDate:                Thu, 11 Apr 2024   Prob (F-statistic):              0.128\nTime:                        12:59:51   Log-Likelihood:                -56.429\nNo. Observations:                  16   AIC:                             116.9\nDf Residuals:                      14   BIC:                             118.4\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     42.4204      2.998     14.151      0.000      35.991      48.850\ndistance      -1.4189      0.877     -1.618      0.128      -3.299       0.462\n==============================================================================\nOmnibus:                        4.148   Durbin-Watson:                   2.906\nProb(Omnibus):                  0.126   Jarque-Bera (JB):                2.008\nSkew:                          -0.820   Prob(JB):                        0.366\nKurtosis:                       3.565   Cond. No.                         4.85\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# visualise using a scatterplot\n(ggplot(productivity_py,\n        aes(x = \"distance\",\n            y = \"projects\",\n            colour = \"cycle\")) +\n     geom_point())\n```\n\n::: {.cell-output-display}\n![](operationalisation_files/figure-html/unnamed-chunk-31-3.png){width=614}\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# create a linear model\nmodel = smf.ols(formula = \"projects ~ distance\",\n                    data = productivity_cycle_py)\n# and get the fitted parameters of the model\nlm_proj3_py = model.fit()\n\n# look at the model output\nprint(lm_proj3_py.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               projects   R-squared:                       0.029\nModel:                            OLS   Adj. R-squared:                 -0.040\nMethod:                 Least Squares   F-statistic:                    0.4187\nDate:                Thu, 11 Apr 2024   Prob (F-statistic):              0.528\nTime:                        12:59:51   Log-Likelihood:                -33.526\nNo. Observations:                  16   AIC:                             71.05\nDf Residuals:                      14   BIC:                             72.60\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      4.6899      0.716      6.547      0.000       3.154       6.226\ndistance      -0.1356      0.210     -0.647      0.528      -0.585       0.314\n==============================================================================\nOmnibus:                        0.919   Durbin-Watson:                   1.657\nProb(Omnibus):                  0.632   Jarque-Bera (JB):                0.694\nSkew:                           0.016   Prob(JB):                        0.707\nKurtosis:                       1.980   Cond. No.                         4.85\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n\n\n:::\n:::\n\n:::\n\nAh. Turns out we were right to be concerned; when staff members who don't cycle are removed from the dataset, the significant relationship that we saw earlier between `distance` and `mean_hrs` disappears. And the marginally non-significant relationship we observed between `distance` and `projects` becomes much less significant.\n\nThis leaves us with just one significant result: `projects ~ cycle`. But if we really were trying to report on these data, in a paper or report of some kind, we'd need to think very carefully about how much we *trust* this result, or whether perhaps we've stumbled on a false positive by virtue of running so many tests. We may also want to think carefully about whether or not we're happy with these definitions of the variables; for instance, is the number of projects completed really the best metric for productivity at work?\n\n## Summary\n\n::: {.callout-tip}\n#### Key points\n\n- There are multiple ways to operationalise a variable, which may affect whether the variable is categorical or continuous\n- The nature of the response variable will alter what type of model can be fitted to the dataset\n- Some operationalisations may better capture your variable of interest than others\n- If you do not effectively operationalise your variable in advance, you may find yourself \"cherry-picking\" your dataset\n:::\n",
    "supporting": [
      "operationalisation_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}