{
  "hash": "9e5db772e7cf7ccc45608144120fefe7",
  "result": {
    "markdown": "---\ntitle: \"Mixed effects models\"\noutput: html_document\n---\n\n::: {.cell}\n\n:::\n\n\nMixed effects models are particularly useful in biological and clinical sciences, where we commonly have innate clusters or groups within our datasets. This is because mixed effects models contain **random effects** in addition to **fixed effects** (hence the name, \"mixed\"). \n\nRather than incorrectly assuming independence between observations, random effects allow us to take into account the natural clusters or structures within datasets, without requiring us to calculate separate coefficients for each group. In other words, this solves the problem of pseudoreplication, without sacrificing as much statistical power.\n\nGetting your head around the difference between a fixed effect and a random effect can be tricky, but the good news is that once you've got that broader understanding, actually fitting a mixed effects model is very easy. \n\n## Libraries and functions\n\n::: {.callout-note collapse=\"true\"}\n## Click to expand\n\nWe'll be using the `lme4` package in R, which is by far the most common and best choice of package for this type of model. (It's an update of the older package `nlme`, which you might also see people using.) The syntax is nice and simple and extends what we've been doing so far with the `lm()` function in (hopefully!) a very intuitive way. The package also contains functions for fitting non-linear mixed effects and generalised mixed effects models - though we won't be focusing on those here, it's nice to know that the package can handle them in case you ever choose to explore them in future!\n\nFor Python users, the `pymer4` package in Python allows you to \"borrow\" most of the functionality of R's `lme4`, though it still has many bugs that make it difficult to run on any system except Linux. There is also some functionality for fitting mixed models using `statsmodels` in Python. We won't be using those packages here, but you may wish to explore them if you are a die-hard Python user!\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n### Libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# you'll need this for plotting\nlibrary(tidyverse)\n\n# install and load lme4 for fitting mixed effects models\ninstall.packages(\"lme4\")\nlibrary(lme4)\n```\n:::\n\n:::\n:::\n\n## The sleep study data\n\nAs an example, we're going to use the internal `sleepstudy` dataset from the `lme4` package in R (this dataset is also provided as a `.csv` file, if you'd prefer to read it in or are using Python). This is a simple dataset taken from a real study that investigated the effects of sleep deprivation on reaction times in 18 subjects, and has just three variables: `Reaction`, reaction time in milliseconds; `Days`, number of days of sleep deprivation; and `Subject`, subject ID.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"sleepstudy\")\n\nhead(sleepstudy)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Reaction Days Subject\n1 249.5600    0     308\n2 258.7047    1     308\n3 250.8006    2     308\n4 321.4398    3     308\n5 356.8519    4     308\n6 414.6901    5     308\n```\n:::\n:::\n\n\nHave a look at the data more closely. You'll notice that for each subject, we've got 10 measurements, one for each day of sleep deprivation. This repeated measurement means that our data are not independent of one another; for each subject in the study we would expect measurements of reaction times to be more similar to one another than they are to reaction times of another subject.\n\nLet's start by doing something that we know is wrong, and ignoring this dependence for now. We'll begin by visualising the data with a simple scatterplot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(sleepstudy, aes(x = Days, y = Reaction)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n```\n\n::: {.cell-output-display}\n![](mixed-effects-models_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nThis gives the overall impression that we might expect - reaction time does seem to slow as people become more sleep deprived.\n\nBut, as we've already pointed out, ignoring the fact that subjects' own reaction times will be more similar to themselves than to another subject's, we should make a point of accounting for this.\n\n## Adding a random effect\n\nIn this dataset, we want to treat `Subject` as a random effect, which means fitting a mixed effects model. Why `Subject`? There are two things at play here that make us what to treat this as a random effect:\n\n1. `Subject` is a *grouping* variable within our dataset, and is causing us problems with independence.\n2. It's not these specific 18 subjects that we're interested in - they instead represent 18 random selections from a broader distribution/population of subjects that we could have tested. We would like to generalise our findings to this broader population.\n\nTo fit the model, we use a different function to what we've used so far, but the syntax looks very similar. The difference is the addition of a new term `(1|Subject)`, which represents our random effect.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# construct a linear mixed effects model with Subject\n# as a random effect\nlme_sleep1 <- lmer(Reaction ~ Days + (1|Subject),\n                   data = sleepstudy)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# summarise the model\nsummary(lme_sleep1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML ['lmerMod']\nFormula: Reaction ~ Days + (1 | Subject)\n   Data: sleepstudy\n\nREML criterion at convergence: 1786.5\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.2257 -0.5529  0.0109  0.5188  4.2506 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n Subject  (Intercept) 1378.2   37.12   \n Residual              960.5   30.99   \nNumber of obs: 180, groups:  Subject, 18\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 251.4051     9.7467   25.79\nDays         10.4673     0.8042   13.02\n\nCorrelation of Fixed Effects:\n     (Intr)\nDays -0.371\n```\n:::\n:::\n\n\nOkay. The syntax might have looked similar, but the output does not.\n\nMixed effects models are much easier to get your head around them if you visualise - so let's give that a go.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a linear model - we'll use this in our graph\nlm_sleep <- lm(Reaction ~ Days,\n               data = sleepstudy)\n\n# set up our basic plot\nggplot(sleepstudy, aes(x = Days, y = Reaction)) +\n  \n  # create separate plots for each subject in the sample\n  # and add the data points\n  facet_wrap(facets = vars(Subject), nrow = 3) +\n  geom_point() +\n  \n  # this adds the line of best fit for the whole sample\n  # (without the random effect), using coefficients\n  # from our simple linear model object\n  geom_line(data = cbind(sleepstudy, pred = predict(lm_sleep)),\n            aes(y = pred)) + \n  \n  # and finally, this will add different lines of best fit\n  # for each subject as calculated in our mixed model object\n  geom_line(data = cbind(sleepstudy, pred = predict(lme_sleep1)),\n            aes(y = pred), colour = \"blue\")\n```\n\n::: {.cell-output-display}\n![](mixed-effects-models_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nEach plot represents a different subject's data. On each plot, we've added the following:\n\n* in black we have the same overall line of best fit from our original (incorrect) linear model.\n* in blue are the individual lines of best fit for each subject. These lines move up and down the plot relative to the global line of best fit. This reflects the fact that, though all subjects are declining as they become more sleep deprived, some of them started with slower baseline reaction times, with different y-intercepts to match. Subject 310, for instance, seems to have pretty good reflexes relative to everyone else, while subject 337 isn't quite as quick on the trigger.\n\nThe eagle-eyed among you, though, might have spotted that the *gradient* of each of these blue lines is still the same as the overall line of best fit. This is because we've added a random intercept in our model, but have **kept the same slope**. This reflects an underlying assumption that the relationship between sleep deprivation and reaction time is the same - i.e. that people get worse at the same rate - even if their starting baselines differ.\n\nWe might not think that this assumption is a good one, however. And that's where random slopes come in.\n\n## Adding random slopes and random intercepts\n\nTo add a random slope as well as a random intercept, we need to alter the syntax slightly for our random effect. Now, instead of `(1|Subject)`, we'll instead use `(1 + Days|Subject)`. This allows the relationship between `Days` and `Reaction` to vary between subjects.\n\nLet's fit that new model and summarise it.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlme_sleep2 <- lmer(Reaction ~ Days + (1 + Days|Subject),\n                   data = sleepstudy)\n\nsummary(lme_sleep2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML ['lmerMod']\nFormula: Reaction ~ Days + (1 + Days | Subject)\n   Data: sleepstudy\n\nREML criterion at convergence: 1743.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.9536 -0.4634  0.0231  0.4634  5.1793 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n Subject  (Intercept) 612.10   24.741       \n          Days         35.07    5.922   0.07\n Residual             654.94   25.592       \nNumber of obs: 180, groups:  Subject, 18\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  251.405      6.825  36.838\nDays          10.467      1.546   6.771\n\nCorrelation of Fixed Effects:\n     (Intr)\nDays -0.138\n```\n:::\n:::\n\n\nWe can go ahead and add our new lines (in red) to our earlier facet plot:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(sleepstudy, aes(x = Days, y = Reaction)) +\n  facet_wrap(facets = vars(Subject), nrow = 3) +\n  geom_point() +\n  \n  # the global line of best fit\n  geom_line(data = cbind(sleepstudy, pred = predict(lm_sleep)),\n            aes(y = pred)) + \n  \n  # our previous lines of best fit, with random intercepts\n  # but constant slope\n  geom_line(data = cbind(sleepstudy, pred = predict(lme_sleep1)),\n            aes(y = pred), colour = \"blue\") +\n  \n  # our lines of best with random intercepts and random slopes\n  geom_line(data = cbind(sleepstudy, pred = predict(lme_sleep2)),\n            aes(y = pred), colour = \"red\") \n```\n\n::: {.cell-output-display}\n![](mixed-effects-models_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nWhile for some of our subjects, the red, blue and black lines look quite similar, for others they diverge a fair amount. Subjects 309 and 335, for instance, are displaying a remarkably flat trend that suggests they're not really suffering delays in reaction time from their sleep deprivation very much at all, while subject 308 definitely seems to struggle without their eight hours. \n\nAs an extra observation, let's use `geom_smooth` to add the lines of best fit that we would see if we fitted each subject with their own individual regression:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(sleepstudy, aes(x = Days, y = Reaction)) +\n  facet_wrap(facets = vars(Subject), nrow = 3) +\n  geom_point() +\n  \n  # the global line of best fit\n  geom_line(data = cbind(sleepstudy, pred = predict(lm_sleep)),\n            aes(y = pred)) + \n  \n  # random slopes only\n  geom_line(data = cbind(sleepstudy, pred = predict(lme_sleep1)),\n            aes(y = pred), colour = \"blue\") +\n  \n  # random intercepts and random slopes\n  geom_line(data = cbind(sleepstudy, pred = predict(lme_sleep2)),\n            aes(y = pred), colour = \"red\") +\n  \n  # individual regression lines for each individual\n  geom_smooth(method = \"lm\", se = FALSE,\n              colour = \"green\", linewidth = 0.5)\n```\n\n::: {.cell-output-display}\n![](mixed-effects-models_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nHere, the black line (which is the same on every plot) represents a global line of best fit - this is what we would see using **complete pooling**.\n\nThe blue and red lines represent our mixed effects models - the difference between the two is whether we allowed the slope to vary randomly, as well as the random intercept. In both cases, we are using **partial pooling**.\n\nThe green line, meanwhile, represents what happens when we allow **no pooling**. In other words, we've fit individual regressions between `Reaction` and `Days` for each subject. Comparing this to the red lines allows us to see the phenomenon of \"shrinkage\". The green lines are all slightly closer to the black line than the red line is; in other words, there's some shrinkage towards the global line. (Subjects 330, 335 and 370 perhaps show this best.) It's subtle, but it's a nice demonstration of what happens when we share information between levels of a random effect.\n\n## Evaluating models and assessing significance\n\nYou may have noticed the lack of p-values in any of our model outputs, and my avoidance of discussing significance. There's a very good reason for this: it's a fair bit more complicated than for standard linear models.\n\nOne of the main reasons is that when you are using partial pooling, there is no way to *precisely* figure out how many degrees of freedom you have, like we can do for fixed effects. This matters, because you need to know the degrees of freedom in order to be able to calculate p-values from the test statistics - an F-value alone is meaningless without associated degrees of freedom!\n\nThis is why the authors of the `lme4` package have deliberately excluded p-values from the model summaries, and also why we won't go any further for this particular course - talking about model comparison and significance testing when we have random effects included is a topic that deserves a course all to itself. \n\n::: {.callout-note collapse=\"true\"}\n## But, if you're really keen to know a bit more about significance...\n\nSo, I know I said we weren't going to go any further on this topic, but for those who are really interested, I figured I might as well include a short summary here of some of the approaches that are taken for significance testing of mixed effects models and their parameters, as a starting point for further reading.\n\n- Use **approximations for the degrees of freedom**, to yield estimated p-values. There is a companion package called `lmerTest` that allows for p-values to be calculated using two common approximations, the Satterthwaite and Kenward-Roger approximations.\n- **Likelihood ratio tests**. LRTs involve making comparisons between models, to determine whether or not a particular parameter should be included. In other words, if you compare the model with versus without the parameter, you can see how that changes the fit; this is typically used for random effects, but sometimes for fixed effects as well. When using LRTs, however, you sometimes have to refit the model using maximum likelihood estimation instead of restricted maximum likelihood estimation (`lme4` uses the latter by default) so it's not always straightforward.\n- **Markov chain Monte Carlo (MCMC) sampling**, to determine the probability distribution associated with model parameters without a requirement for degrees of freedom. Unfortunately this technique cannot be used when the model contains random slopes, and therefore is not used very often.\n- Make **z-to-t approximations**. There are Wald *t*-values reported as standard in the `lme4` outputs; by treating them instead as *z*-values, you can find associated p-values. This approach relies on the fact that the *z* and *t* distributions are identical when degrees of freedom are infinite (so if the degrees of freedom are large enough, i.e., you have a lot of data, the approximation is decent).\n:::\n\n## Summary and additional resources\n\nFor our purposes this week, the main takeaway from this section is to be aware of what a random effect is, and how you might identify when a mixed effects model would be appropriate for your data.\n\nThese sorts of data are very common in the biological sciences, though, so there might be several of you who are thinking about trying to use a mixed effects model already. I really recommend [this blog post](https://ourcodingclub.github.io/tutorials/mixed-models/) for further reading - it's one of my favourites for introducing mixed effects models. In addition to talking about random slopes and intercepts, it also touches on how to cope with nested and crossed factors.\n\nHere is [some more discussion](https://bookdown.org/steve_midway/DAR/random-effects.html) that might be useful to you if you're still wrapping your head around the fixed vs random distinction.\n\n**A final note on some terminology**: when searching for information on mixed effects models, you may also wish to use the term \"multilevel\" model, or even \"hierarchical\" model. The name \"mixed model\" refers specifically to the fact that there are both fixed and random effects within the same model; but you'll also see the same sort of model referred to as \"multilevel/hierarchical\", which references the fact that there are grouping variables that give the dataset a structure with different levels in it.\n\n## Key points\n\n::: callout-note\n\n- A model with both fixed and random effects is referred to as a mixed effects model\n- These models can be fitted in the `lme4` package in R, using specialised syntax for random effects\n- For random intercepts, we use `(1|B)`, while random intercepts with random slopes can be fitted using `(1 + A|B)`\n- Random effects are fitted using partial pooling, which results in the phenomenon of \"shrinkage\"\n:::",
    "supporting": [
      "mixed-effects-models_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}